{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YvnF0-Vh2xT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# A Digital Twin for \"FX-constrained growth: Fundamentalists, chartists and the dynamic trade-multiplier\"\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2508.02252-b31b1b.svg)](https://arxiv.org/abs/2508.02252)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/fx_constrained_growth)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Behavioral%20Finance%20%26%20Econometrics-blue)](https://github.com/chirindaopensource/fx_constrained_growth)\n",
        "[![Research](https://img.shields.io/badge/Research-Macro--Financial%20Modeling-green)](https://github.com/chirindaopensource/fx_constrained_growth)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Heterogeneous%20Agent%20Model-orange)](https://github.com/chirindaopensource/fx_constrained_growth)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025596?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/Statsmodels-1B62A8.svg?style=flat&logo=statsmodels&logoColor=white)](https://www.statsmodels.org/stable/index.html)\n",
        "[![Joblib](https://img.shields.io/badge/Joblib-00A0B0.svg?style=flat)](https://joblib.readthedocs.io/en/latest/)\n",
        "\n",
        "--\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/fx_constrained_growth`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"FX-constrained growth: Fundamentalists, chartists and the dynamic trade-multiplier\"** by:\n",
        "\n",
        "*   Marwil J. Dávila-Fernández\n",
        "*   Serena Sordi\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for creating a \"digital twin\" of the paper's findings. It delivers a modular, auditable, and extensible pipeline that replicates the study's entire workflow: from rigorous data validation and cleaning, through the complex Bayesian estimation of a time-varying parameter model, to the numerical simulation and analysis of the proposed heterogeneous agent model. The goal is to provide a transparent and robust toolkit for researchers in computational economics, behavioral finance, and development macroeconomics to replicate, validate, and build upon this important work.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: execute_digital_twin_replication](#key-callable-execute_digital_twin_replication)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"FX-constrained growth: Fundamentalists, chartists and the dynamic trade-multiplier.\" The core of this repository is the iPython Notebook `fx_constrained_growth_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final execution of a full suite of robustness checks.\n",
        "\n",
        "The paper addresses a critical gap in behavioral finance by building a model from the perspective of developing, FX-constrained economies. This codebase operationalizes that model, allowing users to:\n",
        "-   Rigorously validate and cleanse the input macroeconomic and financial time-series data.\n",
        "-   Execute a sophisticated Bayesian state-space model to estimate the time-varying trade multiplier.\n",
        "-   Simulate the theoretical heterogeneous agent model (HAM) with fundamentalist, chartist, and trend-extrapolator agents.\n",
        "-   Perform advanced numerical analysis of the model's nonlinear dynamics, including generating bifurcation diagrams and basins of attraction.\n",
        "-   Quantitatively validate that the simulated model output reproduces the key \"stylized facts\" (e.g., fat-tailed distributions) observed in the empirical data.\n",
        "-   Conduct a comprehensive suite of robustness checks to test the stability of the findings to changes in parameters, model specifications, and data samples.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in development macroeconomics, behavioral finance, and nonlinear dynamics.\n",
        "\n",
        "**1. The Dynamic Trade Multiplier (Thirlwall's Law):**\n",
        "A core concept is the balance-of-payments-constrained growth rate, which posits that a country's long-run growth is determined by the growth of its exports relative to its income elasticity of demand for imports (`π`). The paper's key empirical task is to estimate a time-varying version of this elasticity.\n",
        "\n",
        "$$\n",
        "\\Delta y_t^{BP} = \\frac{\\Delta z_t^T}{\\pi_t}\n",
        "$$\n",
        "\n",
        "where `Δy_t^{BP}` is the trade-multiplier growth rate, `Δz_t^T` is the trend growth of exports, and `π_t` is the time-varying income elasticity of imports. This is estimated using a Bayesian state-space model.\n",
        "\n",
        "**2. Heterogeneous Agent Model (HAM):**\n",
        "The theoretical model populates the FX market with boundedly rational agents using simple heuristics:\n",
        "-   **Fundamentalists:** Bet on the convergence of the exchange rate (`e`) to its fundamental value (`f`). Their demand is nonlinear: `Δd^F ∝ (f - e)³`.\n",
        "-   **Chartists:** Bet on the persistence of deviations from the fundamental. Their demand is linear: `Δd^C ∝ (e - f)`.\n",
        "-   **Trend-Extrapolators:** Bet on the continuation of recent trends: `Δd^E ∝ (e_{t-1} - e_{t-2})`.\n",
        "\n",
        "**3. Market Clearing and System Dynamics:**\n",
        "The paper's central innovation is to show that the trade multiplier emerges as a market-clearing condition in the FX market. The full dynamic system is a set of coupled nonlinear difference equations for the exchange rate (`e_t`) and output growth (`Δy_t`), which can produce complex dynamics, including multiple equilibria and chaos.\n",
        "\n",
        "$$\n",
        "e_t = F(e_{t-1}, \\Delta y_{t-1}, \\dots)\n",
        "$$\n",
        "$$\n",
        "\\Delta y_t = G(e_{t-1}, \\Delta y_{t-1}, \\dots)\n",
        "$$\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`fx_constrained_growth_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Task-Based Architecture:** The entire pipeline is broken down into 7 distinct, modular tasks, from data validation to robustness analysis.\n",
        "-   **Professional-Grade Data Validation:** A comprehensive validation suite ensures all inputs (data and configurations) conform to the required schema before execution.\n",
        "-   **Auditable Data Cleansing:** A non-destructive cleansing process that handles missing values and outliers, returning a detailed log of all transformations.\n",
        "-   **Advanced Bayesian Estimation:** A custom, multi-chain Gibbs Sampler with an embedded FFBS algorithm for time-varying parameter estimation.\n",
        "-   **Rigorous MCMC Diagnostics:** Implements both Gelman-Rubin (`R-hat`) and Effective Sample Size (ESS) diagnostics to ensure chain convergence.\n",
        "-   **Sophisticated Numerical Analysis:** A parallelized engine for generating bifurcation diagrams, basins of attraction, and quantifying chaos (LLE, Correlation Dimension).\n",
        "-   **Comprehensive Robustness Suite:** A master orchestrator to systematically test the sensitivity of the results to changes in parameters (local and global), model specifications, and data samples.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Validation and Preparation (Task 1):** Ingests and rigorously validates all raw data and configuration files, then cleanses and transforms the data.\n",
        "2.  **Empirical Stylized Facts (Task 2):** Establishes the non-normal, fat-tailed nature of empirical FX returns and growth deviations.\n",
        "3.  **Theoretical Model Implementation (Task 3):** Provides the functional toolkit for the HAM.\n",
        "4.  **Numerical Dynamics Analysis (Task 4):** Executes numerical experiments (bifurcations, basins of attraction, etc.).\n",
        "5.  **Bayesian Econometric Estimation (Task 5):** Runs the full MCMC pipeline to estimate the time-varying trade multiplier.\n",
        "6.  **Statistical Validation (Task 6):** Provides a general toolkit for distributional and time-series analysis, used for both empirical data and simulation output.\n",
        "7.  **Orchestration and Robustness (Task 7):** Provides the master functions to run the entire pipeline and the full suite of robustness checks.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `fx_constrained_growth_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 7 major tasks.\n",
        "\n",
        "## Key Callable: execute_digital_twin_replication\n",
        "\n",
        "The central function in this project is `execute_digital_twin_replication`. It orchestrates the entire analytical workflow, providing a single entry point for running the baseline study replication and the advanced robustness checks.\n",
        "\n",
        "```python\n",
        "def execute_digital_twin_replication(\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    analyses_to_run: List[str],\n",
        "    output_filepath: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete, end-to-end digital twin research pipeline.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `statsmodels`, `joblib`, `tqdm`, `arch`, `ruptures`, `nolds`, `SALib`, `memory_profiler`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/fx_constrained_growth.git\n",
        "    cd fx_constrained_growth\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy statsmodels joblib tqdm arch ruptures nolds SALib memory_profiler ipython\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two `pandas.DataFrame` objects with specific structures, which are rigorously validated by the first task.\n",
        "1.  **`raw_macro_df`:** A DataFrame with a `MultiIndex` of `('country_iso', 'year')` and columns `['gdp_const_lcu', 'imports_const_lcu', 'exports_const_lcu', 'reer']`.\n",
        "2.  **`raw_fx_df`:** A DataFrame with a `MultiIndex` of `('country_iso', 'date')` and a column `['fx_rate_usd']`.\n",
        "\n",
        "A mock data generation script is provided in the main notebook to create valid example DataFrames for testing the pipeline.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `fx_constrained_growth_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Create your DataFrames or use the provided mock data generator. Define the `master_config` dictionary to control all aspects of the run.\n",
        "2.  **Execute Pipeline:** Call the grand master orchestrator function.\n",
        "\n",
        "    ```python\n",
        "    # Define which major phases to run\n",
        "    analyses_to_run = ['data_prep', 'empirical', 'theoretical', 'simulation_validation']\n",
        "\n",
        "    # This single call runs the entire project.\n",
        "    final_results = execute_digital_twin_replication(\n",
        "        raw_macro_df=raw_macro_df,\n",
        "        raw_fx_df=raw_fx_df,\n",
        "        master_config=master_config,\n",
        "        analyses_to_run=analyses_to_run,\n",
        "        output_filepath=\"./project_outputs/final_results.joblib\"\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** Programmatically access any result from the returned dictionary. For example, to view the main empirical performance summary:\n",
        "    ```python\n",
        "    performance_summary = final_results['pipeline_run_results']['empirical_results'] \\\n",
        "        ['trade_multiplier_validation']['cross_country_summary']['performance_summary']\n",
        "    print(performance_summary)\n",
        "    ```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `execute_digital_twin_replication` function returns a single, comprehensive dictionary containing all generated artifacts, including:\n",
        "-   `pipeline_run_results`: A dictionary containing all primary results (data prep reports, empirical analysis, theoretical simulations).\n",
        "-   `performance_report`: A dictionary detailing the computational performance of the run.\n",
        "-   `quality_assurance_report`: A high-level summary and cross-validation of key findings.\n",
        "-   `robustness_analysis_report`: A dictionary containing the summary tables from each of the executed robustness checks.\n",
        "\n",
        "The full results object is also saved to disk at the specified `output_filepath`.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "fx_constrained_growth/\n",
        "│\n",
        "├── fx_constrained_growth_draft.ipynb  # Main implementation notebook\n",
        "├── requirements.txt                   # Python package dependencies\n",
        "├── LICENSE                            # MIT license file\n",
        "└── README.md                          # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `master_config` dictionary. Users can easily modify all relevant parameters, such as MCMC settings, prior distributions, the definition of theoretical model scenarios, and the specific robustness checks to perform.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "\n",
        "-   **Automated Report Generation:** Creating a function that takes the final `master_results` dictionary and generates a full PDF or HTML report summarizing the findings.\n",
        "-   **Alternative Behavioral Heuristics:** Expanding the theoretical model to include other types of agent behavior (e.g., adaptive expectations, learning agents).\n",
        "-   **Policy Experiments:** Using the validated model to conduct \"what-if\" scenario analysis, such as simulating the impact of capital controls or changes in export growth on an economy's stability.\n",
        "-   **Integration with General Equilibrium Models:** Embedding the HAM FX market into a larger-scale DSGE or Agent-Based Model to explore deeper macroeconomic feedback loops.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{davila2025fx,\n",
        "  title={{FX-constrained growth: Fundamentalists, chartists and the dynamic trade-multiplier}},\n",
        "  author={D{\\'a}vila-Fern{\\'a}ndez, Marwil J and Sordi, Serena},\n",
        "  journal={arXiv preprint arXiv:2508.02252},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Digital Twin of \"FX-constrained growth: Fundamentalists, chartists and the dynamic trade-multiplier\".\n",
        "GitHub repository: https://github.com/chirindaopensource/fx_constrained_growth\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Marwil J. Dávila-Fernández** and **Serena Sordi** for their insightful and clearly articulated research, which forms the entire basis for this computational replication.\n",
        "-   This project stands on the shoulders of giants. Sincere thanks to the global open-source community and the developers of the scientific Python ecosystem, whose tireless work provides the foundational tools for modern computational science. Specifically, this implementation relies heavily on:\n",
        "    -   **NumPy** and **SciPy** for foundational numerical computing and scientific algorithms.\n",
        "    -   **Pandas** for its indispensable data structures and time-series manipulation capabilities.\n",
        "    -   **Statsmodels** for its robust implementation of econometric methods, including time-series diagnostics and filtering.\n",
        "    -   The **arch** library for its specialized, professional-grade tools for volatility modeling.\n",
        "    -   **Joblib** for enabling efficient, straightforward parallel processing.\n",
        "    -   **SALib** for providing a state-of-the-art framework for sensitivity analysis.\n",
        "    -   The **ruptures** and **nolds** libraries for their powerful algorithms in change-point detection and nonlinear dynamics.\n",
        "    -   The **Jupyter** and **IPython** projects for creating an unparalleled environment for interactive scientific development and literate programming.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `fx_constrained_growth_draft.ipynb` and follows best practices for research software documentation.*\n",
        "\n"
      ],
      "metadata": {
        "id": "DjldpVO1boJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*FX-constrained growth: Fundamentalists, chartists and the dynamic trade-multiplier*\"\n",
        "\n",
        "Authors: Marwil J. Davila-Fernandez, Serena Sordi\n",
        "\n",
        "E-Journal Submission Date: 4 August 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2508.02252\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Behavioural finance offers a valuable framework for examining foreign exchange (FX) market dynamics, including puzzles such as excess volatility and fat-tailed distributions. Yet, when it comes to their interaction with the `real' side of the economy, existing scholarship has overlooked a critical feature of developing countries. They cannot trade in their national currencies and need US dollars to access modern production techniques as well as maintain consumption patterns similar to those of wealthier societies. To address this gap, we present a novel heterogeneous agents model from the perspective of a developing economy that distinguishes between speculative and non-speculative sectors in the FX market. We demonstrate that as long as non-speculative demand responds to domestic economic activity, a market-clearing output growth rate exists that, in steady-state, is equal to the ratio between FX supply growth and the income elasticity of demand for foreign assets, i.e., a generalised dynamic trade-multiplier. Numerical simulations reproduce key stylised facts of exchange rate dynamics and economic growth, including distributions that deviate from the typical bell-shaped curve. Data from a sample of Latin American countries reveal that FX fluctuations exhibit similar statistical properties. Furthermore, we employ time-varying parameter estimation techniques to show that the dynamic trade-multiplier closely tracks observed growth rates in these economies."
      ],
      "metadata": {
        "id": "i1tr44tQiAt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "Here is a step-by-step summary and analysis of \"FX-constrained growth: Fundamentalists, chartists and the dynamic trade-multiplier.\"\n",
        "\n",
        "--\n",
        "\n",
        "### **The Core Problem and Central Contribution**\n",
        "\n",
        "The authors identify a critical gap in the existing literature on behavioral exchange rate models. While these models, featuring heterogeneous agents like \"fundamentalists\" and \"chartists,\" have been successful in explaining stylized facts of financial markets (e.g., excess volatility, fat tails), they are almost exclusively designed from the perspective of developed economies.\n",
        "\n",
        "The key oversight is what development economists call the **\"original sin\"**: developing countries cannot borrow or trade internationally in their own currency. They are dependent on a constant inflow of hard currency (primarily US dollars) to import capital goods, technology, and intermediate inputs necessary for production.\n",
        "\n",
        "The paper's central contribution is to build a novel **Heterogeneous Agent Model (HAM)** that explicitly incorporates this **FX constraint** on growth. They create a unified framework where the chaotic dynamics of the FX market, driven by speculators, directly interact with and are constrained by the real economy's need for foreign currency.\n",
        "\n",
        "### **Empirical Motivation: The Stylized Facts**\n",
        "\n",
        "Before building their model, the authors present two sets of empirical facts, primarily from Latin American economies, to motivate their approach:\n",
        "\n",
        "1.  **FX Market Dynamics:** Using daily data for countries like Brazil, Mexico, and Argentina, they show that FX returns exhibit high volatility and are not normally distributed. The QQ plots clearly indicate \"fat tails\" (leptokurtosis), meaning extreme events are far more common than a Gaussian distribution would predict. This justifies the use of a behavioral model with non-rational agents over standard efficient-market models.\n",
        "2.  **Real Growth Dynamics:** They introduce the **dynamic trade-multiplier**, a concept from post-Keynesian/structuralist economics also known as \"Thirlwall's Law.\" In its simplest form, it states that the long-run growth rate of an economy is constrained by the growth rate of its exports divided by the income elasticity of its demand for imports (`Δy ≈ Δz / π`). Using time-varying parameter estimation, they show this simple rule remarkably tracks the long-run trend of actual GDP growth in these countries. Actual growth appears to fluctuate around this \"trade-multiplier\" benchmark.\n",
        "\n",
        "The goal of the model is to explain these two sets of facts within a single, coherent framework.\n",
        "\n",
        "### **The Model's Architecture**\n",
        "\n",
        "The model is a two-dimensional nonlinear map describing the co-evolution of the **log exchange rate (`e`)** and the **GDP growth rate (`Δy`)**. It consists of several interconnected blocks:\n",
        "\n",
        "1.  **FX Market Demand:** Demand for foreign assets is split into two sectors:\n",
        "    *   **Non-Speculative Demand:** This is the crucial link to the real economy. It represents firms needing USD for imports. This demand is modeled as a power function of GDP (`DNS = Y^π`). In growth rates, this is `ΔdNS = πΔy`.\n",
        "    *   **Speculative Demand:** This is driven by a population of heterogeneous traders. The baseline model includes:\n",
        "        *   **Fundamentalists:** They believe the exchange rate will revert to its fundamental value (`f`). Their demand is a cubic function of the perceived misalignment `(E[f] - e)^3`, meaning they react disproportionately to large deviations.\n",
        "        *   **Chartists:** They believe recent trends will persist and bet against the fundamentalists. Their demand is a linear function of the misalignment `(e - E[f])`.\n",
        "        *   A later extension adds **Trend-Extrapolators**, who base their demand on the last period's price change `(e_t-1 - e_t-2)`.\n",
        "\n",
        "2.  **Market Clearing and the Bridge to the Real Economy:** This is the most innovative part of the model. Instead of the exchange rate being the sole variable that clears the market, the authors propose that there exists a **market-clearing output growth rate (`ΔyMC`)**. This is the rate of GDP growth that would precisely balance the non-speculative demand for FX with the supply available after speculators have made their trades.\n",
        "\n",
        "3.  **Production and Growth Dynamics:** The supply side is a simple AK growth model. Firms are also heterogeneous:\n",
        "    *   **Flexible Firms:** A fraction of firms can adjust their investment plans based on the gap between the `ΔyMC` and the actual growth rate `Δy_t-1`. If `ΔyMC` is high (signaling ample FX liquidity), they invest more.\n",
        "    *   **Rigid Firms:** The remaining firms are bound by existing plans and do not adjust.\n",
        "\n",
        "4.  **Expectations Formation (The Second Bridge):** The loop is closed by how agents form expectations about the fundamental exchange rate `E[f]`. The authors posit that these expectations are influenced by past macroeconomic performance. Stronger past growth (`Δy_t-1`) leads agents to expect a stronger (more appreciated) fundamental currency. `E[f_t] = -ΩΔy_t-1 + ε_t`.\n",
        "\n",
        "### **The Core Theoretical Result: A New Foundation for the Trade Multiplier**\n",
        "\n",
        "The model's structure yields a powerful theoretical result. The equation for the market-clearing growth rate is:\n",
        "`ΔyMC = (ΔzNS / π) - (Speculative Trade / π)`\n",
        "\n",
        "In a steady-state equilibrium where speculative forces are balanced and the exchange rate equals its expected fundamental value, the \"Speculative Trade\" term goes to zero. This leaves:\n",
        "`Δy_equilibrium = ΔzNS / π`\n",
        "\n",
        "This is precisely the dynamic trade-multiplier. The authors have thus derived a cornerstone of development macroeconomics not as an accounting identity, but as a **steady-state equilibrium condition emerging from a behavioral FX market**.\n",
        "\n",
        "### **Dynamic Analysis and Numerical Experiments**\n",
        "\n",
        "When the deterministic skeleton of the model is simulated, it exhibits incredibly rich nonlinear dynamics:\n",
        "\n",
        "*   **Multiple Equilibria:** The presence of chartists creates two additional equilibria besides the fundamental one: a stable state with an overvalued currency and another with an undervalued currency.\n",
        "*   **Bifurcations and Chaos:** As parameters are varied (e.g., the reactivity of speculators), the model undergoes a **Flip bifurcation**, leading to a period-doubling cascade into chaos. This means the model can endogenously generate persistent, irregular fluctuations without any external shocks.\n",
        "*   **Discontinuous Basins of Attraction:** The system is multi-stable, and the basins of attraction for the overvalued and undervalued equilibria are disconnected and fractal. This has a profound economic implication: a large, temporary shock (like a currency crisis) could permanently knock the economy from one basin to another (e.g., from an undervalued to an overvalued regime). This provides a novel explanation for the experience of some Latin American countries post-1990s.\n",
        "*   **Role of Trend-Extrapolators:** These agents act as a double-edged sword. They initially smooth fluctuations and stabilize the system, but if their market share grows too large, they make the dynamics explosive and the basins of attraction even more fragmented.\n",
        "\n",
        "### **Model Validation via Simulation**\n",
        "\n",
        "The final step is to check if the simulated model can replicate the initial stylized facts. The results are compelling:\n",
        "\n",
        "*   Both the stochastic version (before bifurcation) and the deterministic chaotic version (after bifurcation) generate time series for FX returns and GDP growth that exhibit **high volatility and fat tails**, closely matching the empirical data.\n",
        "*   The model that includes all three trader types (fundamentalists, chartists, and trend-extrapolators) provides the best qualitative fit for the shape of the FX return distribution seen in the real-world QQ plots.\n",
        "*   The simulated GDP growth fluctuates endogenously around the built-in trade-multiplier, just as it appears to do in the data.\n",
        "\n",
        "### **Conclusion and Implications**\n",
        "\n",
        "In summary, this paper makes a significant contribution by constructing a methodologically rigorous bridge between behavioral finance and development economics. It demonstrates that the FX constraint is not just a background condition but an active driver of macroeconomic dynamics.\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "1.  A **\"generalised dynamic trade-multiplier\"** can be derived as an equilibrium condition from a micro-founded behavioral model of the FX market.\n",
        "2.  The interaction between speculative FX trading and the real economy's need for hard currency can endogenously generate **complex dynamics**, including chaos and multi-stability.\n",
        "3.  This framework provides a new, powerful explanation for **stylized facts** in developing countries, such as fat-tailed distributions in both financial and real variables, and the tendency for economies to get \"stuck\" in regimes of currency over- or undervaluation.\n",
        "\n",
        "The work opens up several avenues for future research, such as incorporating monetary and fiscal policy, analyzing the impact of capital controls, and exploring the model's implications for macro-development strategy. It is a superb example of how combining different theoretical traditions can yield profound insights into complex economic problems.\n"
      ],
      "metadata": {
        "id": "0DZ0uwdDiurv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "WrI4Zdvanqaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  A Digital Twin for \"FX-constrained growth: Fundamentalists, chartists and\n",
        "#  the dynamic trade-multiplier\"\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"FX-constrained growth: Fundamentalists,\n",
        "#  chartists and the dynamic trade-multiplier\" by Dávila-Fernández & Sordi (2025).\n",
        "#  It delivers a full end-to-end pipeline for replicating the paper's findings,\n",
        "#  from raw data validation and cleaning to the empirical estimation of a\n",
        "#  time-varying trade multiplier and the numerical analysis of the proposed\n",
        "#  heterogeneous agent model.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Bayesian State-Space Model Estimation via a Gibbs Sampler with a\n",
        "#    Forward-Filter, Backward-Sampler (FFBS) step for the time-varying parameter.\n",
        "#  • Hodrick-Prescott (HP) filtering for time-series decomposition.\n",
        "#  • Heterogeneous Agent Model (HAM) simulation with fundamentalist, chartist,\n",
        "#    and trend-extrapolator agents.\n",
        "#  • Numerical analysis of nonlinear dynamics, including bifurcation diagrams,\n",
        "#    basins of attraction, and chaos quantification (Lyapunov exponents).\n",
        "#  • Comprehensive statistical validation, including Extreme Value Theory (EVT)\n",
        "#    for tail analysis and robust MCMC convergence diagnostics (R-hat and ESS).\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • A modular, hierarchical architecture of orchestrators and worker functions.\n",
        "#  • Parallelized execution for computationally intensive tasks (MCMC, bifurcation,\n",
        "#    basin analysis, sensitivity analysis) using `joblib`.\n",
        "#  • Rigorous data validation, quality assurance, and preprocessing pipelines.\n",
        "#  • A complete robustness analysis framework, including local and global (Sobol)\n",
        "#    parameter sensitivity, model specification checks, and data sample stability tests.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Dávila-Fernández, M. J., & Sordi, S. (2025). FX-constrained growth:\n",
        "#  Fundamentalists, chartists and the dynamic trade-multiplier.\n",
        "#  arXiv preprint arXiv:2508.02252. https://arxiv.org/abs/2508.02252\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# =============================================================================\n",
        "# MASTER IMPORT BLOCK FOR THE ENTIRE RESEARCH PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "# --- Core Data Handling and Numerical Computation ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Standard Library ---\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from typing import (\n",
        "    Dict, Any, List, Tuple, Optional, Union, Callable, Set\n",
        ")\n",
        "\n",
        "# --- Econometrics and Time Series Analysis ---\n",
        "from statsmodels.tsa.filters.hp_filter import hpfilter\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch\n",
        "from statsmodels.stats.multitest import fdrcorrection\n",
        "from arch import arch_model\n",
        "\n",
        "# --- Statistical Analysis and Distributions ---\n",
        "from scipy import stats\n",
        "from scipy.linalg import solve\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "# --- Machine Learning and Imputation ---\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# --- Advanced Numerical Analysis ---\n",
        "import nolds  # For Approximate Entropy\n",
        "import ruptures as rpt  # For structural break detection\n",
        "from SALib.sample import saltelli\n",
        "from SALib.analyze import sobol\n",
        "\n",
        "# --- Computational Performance and Parallelization ---\n",
        "import psutil\n",
        "from memory_profiler import memory_usage\n",
        "from joblib import Parallel, delayed, dump\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "q6bR1PSJnwQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "ffk-yXuCnyln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Discussion of the Inputs, Processes and Outputs (IPO Analysis) of Key Callables\n",
        "\n",
        "#### **TASK I: DATA VALIDATION AND QUALITY ASSURANCE**\n",
        "\n",
        "**Callable: `orchestrate_data_preparation_pipeline`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `raw_macro_df`: A `pandas.DataFrame` with raw annual macroeconomic data.\n",
        "    *   `raw_fx_df`: A `pandas.DataFrame` with raw daily foreign exchange data.\n",
        "    *   `master_config`: The global configuration dictionary.\n",
        "*   **Processes:** This function orchestrates the entire data preparation workflow.\n",
        "    1.  **Validation (Task 1.1):** It first calls `validate_inputs` to perform a rigorous check of the structure and content of all inputs. If this fails, the pipeline halts.\n",
        "    2.  **Assessment (Task 1.2):** It calls `assess_data_quality` to programmatically identify statistical outliers, analyze missing data patterns, and validate key economic relationships.\n",
        "    3.  **Cleansing (Task 1.3):** It calls `_impute_missing_values` to fill gaps using appropriate methods (iterative imputation for macro, forward-fill for FX). It then calls the remediated `_treat_outliers` to mitigate the effect of extreme values by winsorizing growth rates and reconstructing levels.\n",
        "    4.  **Transformation (Task 1.3):** Finally, it calls `_transform_and_standardize_variables` to convert all level data to logarithms and then compute the log-difference series (growth rates and returns) required for all subsequent analyses.\n",
        "*   **Outputs:**\n",
        "    *   `analysis_ready_data`: A dictionary of `DataFrames` containing the fully cleaned, treated, and transformed data, ready for empirical and theoretical work.\n",
        "    *   `data_prep_report`: A comprehensive dictionary containing the reports from the validation and quality assessment steps, and a log of all treatments applied to the data.\n",
        "*   **Role in Research Pipeline:** This callable implements the foundational data engineering phase of the research. It ensures that the empirical analysis is based on data that is clean, complete, and correctly formatted, thereby guaranteeing the reproducibility and reliability of the stylized facts and model estimations. It is the gatekeeper for the entire project.\n",
        "\n",
        "--\n",
        "\n",
        "#### **TASK II & V: EMPIRICAL ANALYSIS & BAYESIAN ESTIMATION**\n",
        "\n",
        "**Callable: `orchestrate_empirical_analysis`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `analysis_ready_data`: The output from the data preparation pipeline.\n",
        "    *   `master_config`: The global configuration dictionary.\n",
        "*   **Processes:** This function orchestrates the entire empirical part of the paper, which aims to establish the stylized facts and estimate the trade multiplier.\n",
        "    1.  **FX Stylized Facts (Task 2.1):** It calls `analyze_exchange_rate_properties` to compute descriptive statistics, perform Anderson-Darling normality tests, and generate QQ plot data for the empirical FX returns. This directly replicates the analysis for Figure 1 and Appendix A.1.\n",
        "    2.  **Trade Multiplier Estimation (Task 2.2 & 5):** It calls `estimate_trade_multiplier`. This is the most complex sub-process. It first uses `_apply_hp_filter` to detrend the macro data. Then, for each country, it executes the full Bayesian estimation by calling `orchestrate_mcmc_analysis`, which runs a multi-chain Gibbs Sampler (`_run_mcmc_chain`) with an embedded Forward-Filter, Backward-Sampler (`_forward_filter_backward_sampler`) to estimate the time-varying import elasticity, `π_t`.\n",
        "    3.  **Model Validation (Task 2.3):** After the MCMC, it calls `_compute_posterior_trade_multiplier` to calculate the final trade multiplier series using the posterior mean of `π_t`. It then calls `_analyze_growth_correlations` and `_analyze_growth_deviations` to compare this estimated series against actual GDP growth, replicating the analysis for Figure 2.\n",
        "*   **Outputs:**\n",
        "    *   `empirical_report`: A comprehensive dictionary containing the full results of the FX analysis, the detailed MCMC estimation results (including convergence diagnostics and posterior summaries), and the final trade multiplier validation reports.\n",
        "*   **Role in Research Pipeline:** This callable implements the entire empirical argument of the paper. It programmatically generates the two core stylized facts that motivate the theoretical model: (1) FX returns are non-normal and fat-tailed, and (2) a time-varying trade multiplier, estimated via a state-space model, closely tracks actual GDP growth.\n",
        "*   **Key Equations Implemented:**\n",
        "    *   Hodrick-Prescott Filter: $$ \\min_{\\\\{\\tau_t\\\\}} \\sum_{t=1}^T (y_t - \\tau_t)^2 + \\lambda \\sum_{t=2}^{T-1} [(\\tau_{t+1} - \\tau_t) - (\\tau_t - \\tau_{t-1})]^2 $$\n",
        "    *   State-Space Model (Appendix A.2): $$ m_t^T = \\eta \\cdot \\text{rer}_t + \\pi_t \\cdot y_t^T + \\varepsilon_{m,t} $$ $$ \\pi_t = \\pi_{t-1} + \\varepsilon_{\\pi,t} $$\n",
        "    *   Dynamic Trade Multiplier (Appendix A.1): $$ \\Delta y_t^{BP} = \\frac{\\Delta z_t^T}{\\pi_t} $$\n",
        "\n",
        "--\n",
        "\n",
        "#### **TASK III & IV: THEORETICAL MODEL & NUMERICAL ANALYSIS**\n",
        "\n",
        "**Callable: `implement_theoretical_model`**\n",
        "\n",
        "*   **Inputs:** None.\n",
        "*   **Processes:** This is not an execution function but a factory. It assembles the suite of fully validated, professional-grade functions that define and solve the theoretical model. It calls the sub-orchestrators `implement_core_dynamics`, `implement_evolution_engine`, and `analyze_equilibria_and_stability`.\n",
        "*   **Outputs:**\n",
        "    *   `model_toolkit`: A dictionary containing the callable functions themselves, organized by purpose (core logic, simulation engine, stability analysis).\n",
        "*   **Role in Research Pipeline:** This callable provides the verified computational toolkit for the theoretical model. It ensures that all numerical experiments in the next phase are conducted using a consistent, correct, and robust set of underlying functions.\n",
        "\n",
        "**Callable: `orchestrate_numerical_dynamics_analysis`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `analysis_type`: A string specifying the analysis to run (e.g., `'bifurcation'`).\n",
        "    *   `scenario_name`: A string referencing a specific scenario in the config.\n",
        "    *   `master_config`: The global configuration dictionary.\n",
        "    *   `model_toolkit`: The output from `implement_theoretical_model`.\n",
        "    *   Optional sensitivity configurations.\n",
        "*   **Processes:** This function is the master dispatcher for all numerical experiments. Based on the `analysis_type`, it calls the appropriate specialized orchestrator.\n",
        "    *   For `'bifurcation'`, it calls `orchestrate_bifurcation_analysis`, which systematically runs simulations across a parameter range to map changes in long-run dynamics.\n",
        "    *   For `'basin_of_attraction'`, it calls `orchestrate_basin_analysis`, which runs thousands of simulations from a grid of initial conditions to map their long-run fate.\n",
        "    *   For `'chaos_quantification'`, it runs a very long simulation and passes the trajectory to the `quantify_chaos` function to compute metrics like the Largest Lyapunov Exponent.\n",
        "    *   For sensitivity analyses, it calls `analyze_local_parameter_sensitivity` or `analyze_global_parameter_sensitivity`.\n",
        "*   **Outputs:**\n",
        "    *   A dictionary containing the detailed results of the requested numerical analysis (e.g., the data for a bifurcation plot, the classification matrix for a basin plot, or a dictionary of chaos metrics).\n",
        "*   **Role in Research Pipeline:** This callable is the engine that generates all the theoretical results of the paper, such as the bifurcation diagrams (Figs. 4, 8) and basins of attraction (Figs. 5, 9) that illustrate the model's complex dynamics, multi-stability, and path dependence.\n",
        "*   **Key Equations Implemented:** This function orchestrates the simulation of the full dynamic system: $$ e_t = e_{t-1} + (\\mu+\\rho)[w^F(-\\Omega\\Delta y_{t-1}-e_{t-1})^3 + w^C(e_{t-1}+\\Omega\\Delta y_{t-1}) + w^E(e_{t-1}-e_{t-2})] $$ $$ \\Delta y_t = \\Delta y_{t-1} + w^{flex}\\beta\\{\\Delta y^{BP} - \\gamma'[...speculative\\_demand...] - \\Delta y_{t-1}\\} $$\n",
        "\n",
        "--\n",
        "\n",
        "#### **TASK VI & VII: STATISTICAL VALIDATION & FINAL ORCHESTRATION**\n",
        "\n",
        "**Callable: `orchestrate_statistical_validation`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `data_dict`: A dictionary of time series to be analyzed.\n",
        "    *   `master_config`: The global configuration dictionary.\n",
        "*   **Processes:** This function provides a general-purpose toolkit for rigorous statistical analysis of any time series. It orchestrates the three main branches of statistical validation:\n",
        "    1.  **Distributional Tests (Task 6.1):** Calls `orchestrate_distributional_analysis` to perform a battery of normality tests.\n",
        "    2.  **Extreme Value Analysis (Task 6.2):** Calls `orchestrate_extreme_value_analysis` to perform specialized tail analysis using EVT methods.\n",
        "    3.  **Temporal Analysis (Task 6.3):** Calls `orchestrate_temporal_analysis` to diagnose autocorrelation, structural breaks, and volatility clustering.\n",
        "*   **Outputs:**\n",
        "    *   A comprehensive, nested dictionary containing the reports from all three analysis components.\n",
        "*   **Role in Research Pipeline:** This callable serves a dual purpose. In the empirical phase, it's used to establish the stylized facts of the real-world data. In the final validation phase, it's used to analyze the output from the theoretical model's simulations to check if the model can endogenously reproduce those same stylized facts.\n",
        "\n",
        "**Callable: `execute_digital_twin_replication`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `raw_macro_df`, `raw_fx_df`, `master_config`, `analyses_to_run`, `output_filepath`.\n",
        "*   **Processes:** This is the apex orchestrator, the single `main` function for the entire project. It executes the complete, end-to-end research workflow in the correct sequence.\n",
        "    1.  It wraps the entire execution in the `profile_computational_pipeline` to gather performance metrics.\n",
        "    2.  It calls `run_full_research_pipeline`, which in turn executes the master orchestrators for Data Preparation (Task 1), Empirical Analysis (Tasks 2 & 5), and Theoretical Analysis (Tasks 3 & 4).\n",
        "    3.  It takes the complete results from the pipeline and passes them to `integrate_and_validate_results` to generate a high-level QA summary.\n",
        "    4.  It calls `orchestrate_robustness_analysis` to perform the final, computationally intensive robustness checks.\n",
        "    5.  It aggregates all reports into a final master dictionary and saves it to disk.\n",
        "*   **Outputs:**\n",
        "    *   The final, all-encompassing `master_results` dictionary, which is the complete digital record of the entire research project.\n",
        "*   **Role in Research Pipeline:** This is the master controller. It represents the full, reproducible workflow of the research paper, transforming raw data and a configuration file into a complete set of empirical findings, theoretical results, and robustness checks. It is the implementation of the scientific method for this specific research project.\n",
        "\n",
        "<br> <br>\n",
        "\n",
        "### Usage Example\n",
        "\n",
        "#### **Example Usage of the End-to-End Research Pipeline**\n",
        "\n",
        "This example will walk through the setup and execution of the `execute_digital_twin_replication` function. It covers the creation of mock input data, the construction of the `master_config` dictionary, and the final function call.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 1: Data Preparation - Creating Mock Input DataFrames**\n",
        "\n",
        "First, we must create mock data that strictly adheres to the validated schema. For a real-world application, this data would be loaded from sources like the World Bank (WDI), Bruegel, and Yahoo Finance. Here, we generate synthetic but structurally correct `pandas.DataFrame` objects.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Create Mock Annual Macroeconomic DataFrame (`raw_macro_df`) ---\n",
        "\n",
        "# Define the countries and the time period for the annual data.\n",
        "countries = ['BRA', 'MEX']\n",
        "years = range(1960, 2024)\n",
        "\n",
        "# Create a MultiIndex from the product of countries and years.\n",
        "multi_index_macro = pd.MultiIndex.from_product([countries, years], names=['country_iso', 'year'])\n",
        "\n",
        "# Generate synthetic data with plausible random walks.\n",
        "num_obs_macro = len(multi_index_macro)\n",
        "data_macro = {\n",
        "    'gdp_const_lcu': 1e12 * (1 + np.random.randn(num_obs_macro).cumsum() * 0.01),\n",
        "    'imports_const_lcu': 2e11 * (1 + np.random.randn(num_obs_macro).cumsum() * 0.015),\n",
        "    'exports_const_lcu': 2.5e11 * (1 + np.random.randn(num_obs_macro).cumsum() * 0.014),\n",
        "    'reer': 100 + np.random.randn(num_obs_macro).cumsum() * 0.5\n",
        "}\n",
        "\n",
        "# Create the DataFrame.\n",
        "raw_macro_df = pd.DataFrame(data_macro, index=multi_index_macro)\n",
        "# Introduce some missing values to test the imputation pipeline.\n",
        "raw_macro_df.iloc[5:10] = np.nan\n",
        "raw_macro_df.iloc[100:102] = np.nan\n",
        "\n",
        "print(\"--- Mock `raw_macro_df` created ---\")\n",
        "print(raw_macro_df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# --- Create Mock Daily FX DataFrame (`raw_fx_df`) ---\n",
        "\n",
        "# Define the time period for the daily data.\n",
        "date_range = pd.bdate_range(start='2014-01-01', end='2024-12-31')\n",
        "\n",
        "# Create a MultiIndex from the product of countries and dates.\n",
        "multi_index_fx = pd.MultiIndex.from_product([countries, date_range], names=['country_iso', 'date'])\n",
        "\n",
        "# Generate a synthetic geometric random walk for the FX rate.\n",
        "num_obs_fx = len(multi_index_fx)\n",
        "fx_returns = np.random.randn(num_obs_fx) * 0.01 # Daily volatility of 1%\n",
        "# Create the price series from the returns.\n",
        "fx_rate = 3.5 * np.exp(np.cumsum(fx_returns))\n",
        "\n",
        "# Create the DataFrame.\n",
        "raw_fx_df = pd.DataFrame({'fx_rate_usd': fx_rate}, index=multi_index_fx)\n",
        "# Introduce some missing values to simulate non-trading holidays.\n",
        "raw_fx_df.iloc[100:105] = np.nan\n",
        "\n",
        "print(\"--- Mock `raw_fx_df` created ---\")\n",
        "print(raw_fx_df.head())\n",
        "print(\"\\n\")\n",
        "```\n",
        "\n",
        "#### **Step 2: Configuration - Defining the `master_config`**\n",
        "\n",
        "Next, we define the `master_config` dictionary. This object is the central \"control panel\" for the entire project. It specifies every parameter, from the `lambda` of the HP filter to the configuration for each theoretical simulation and the settings for the robustness tests. For this example, we will use a slightly simplified version of the full configuration, focusing on running one key analysis from each major task.\n",
        "\n",
        "```python\n",
        "# The full master_config is extensive. Here, we define a version for a targeted run.\n",
        "# A complete version would include configurations for all figures and robustness tests.\n",
        "master_config = {\n",
        "    # --- Section 1: Empirical Analysis Configuration ---\n",
        "    'empirical_analysis': {\n",
        "        'parameters': {\n",
        "            'hp_filter': {'lambda': 1600},\n",
        "            'bayesian_model': {\n",
        "                'mcmc_iterations': 5000,  # Reduced for a quick example run\n",
        "                'burn_in_samples': 1000,\n",
        "                'num_chains': 2,\n",
        "                'priors': {\n",
        "                    'eta': {'dist': 'Normal', 'mu': 0, 'sigma': 100},\n",
        "                    'sigma_m': {'dist': 'InverseGamma', 'alpha': 0.01, 'beta': 0.01},\n",
        "                    'sigma_pi': {'dist': 'InverseGamma', 'alpha': 0.01, 'beta': 0.01},\n",
        "                    'pi_0': {'dist': 'Normal', 'mu': 1.5, 'sigma': 1.0}\n",
        "                }\n",
        "            },\n",
        "            'statistical_tests': {'alpha_level': 0.05}\n",
        "        }\n",
        "    },\n",
        "    # --- Section 2: Theoretical Model Configuration ---\n",
        "    'theoretical_simulation': {\n",
        "        'global_settings': {\n",
        "            'simulation_horizon_daily': 3500,\n",
        "            'transient_iterations': 5000,\n",
        "            'plot_iterations': 500,\n",
        "            'initial_conditions': {'e0': 0.01, 'delta_y0': 0.00003, 'e_neg1': 0.00}\n",
        "        },\n",
        "        'figure_scenarios': {\n",
        "            # We will run one bifurcation analysis as an example.\n",
        "            'fig4a': {\n",
        "                'bifurcation_param': {'name': 'mu', 'range': [0, 15], 'steps': 50}, # Reduced steps\n",
        "                'fixed_params': {\n",
        "                    'rho': 4.5, 'w_flex': 0.1, 'beta': 0.1, 'Omega': 0.01,\n",
        "                    'theta': 0.3, 'pi': 2.0, 'delta_z_ns': 0.00006,\n",
        "                    'w_F': 0.9, 'w_C': 0.1, 'w_E': 0.0, 'sigma_epsilon': 0.0\n",
        "                }\n",
        "            },\n",
        "            # We will run one standard simulation for validation.\n",
        "            'fig12': {\n",
        "                'fixed_params': {\n",
        "                    'mu': 8.0, 'rho': 4.5, 'w_flex': 0.1, 'beta': 0.1, 'Omega': 0.01,\n",
        "                    'theta': 0.3, 'pi': 2.0, 'delta_z_ns': 0.00006,\n",
        "                    'w_F': 0.875, 'w_C': 0.060, 'w_E': 0.065, 'sigma_epsilon': 0.0\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # --- Section 3: Robustness Test Configuration ---\n",
        "    'robustness_tests': {\n",
        "        'sample_period': {\n",
        "            'periods': {\n",
        "                'pre_2000': (1980, 1999), # Shorter periods for example\n",
        "                'post_2000': (2000, 2019)\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"--- `master_config` defined for the example run ---\")\n",
        "```\n",
        "\n",
        "#### **Step 3: Execution - Calling the Master Orchestrator**\n",
        "\n",
        "With the data and configuration prepared, the final step is to call the top-level orchestrator, `execute_digital_twin_replication`. We will specify which major phases of the analysis to run. For a full replication, all phases would be included.\n",
        "\n",
        "```python\n",
        "# Define which major parts of the pipeline to execute.\n",
        "# For a full run: ['data_prep', 'empirical', 'theoretical', 'simulation_validation']\n",
        "# For this example, we'll run a targeted set.\n",
        "analyses_to_run = [\n",
        "    'data_prep',\n",
        "    'empirical',\n",
        "    'theoretical'\n",
        "]\n",
        "\n",
        "# Define the path to save the final, comprehensive results object.\n",
        "output_filepath = \"./digital_twin_full_results.joblib\"\n",
        "\n",
        "# --- Execute the entire pipeline ---\n",
        "# This single function call will now run all the specified tasks,\n",
        "# from data validation to the final analyses, and save the output.\n",
        "# Note: This is a computationally intensive operation.\n",
        "\n",
        "# To make this example runnable, we will assume all the previously defined\n",
        "# orchestrator functions are available in the local scope.\n",
        "\n",
        "# final_results = execute_digital_twin_replication(\n",
        "#     raw_macro_df=raw_macro_df,\n",
        "#     raw_fx_df=raw_fx_df,\n",
        "#     master_config=master_config,\n",
        "#     analyses_to_run=analyses_to_run,\n",
        "#     output_filepath=output_filepath\n",
        "# )\n",
        "\n",
        "# print(\"\\n--- PIPELINE EXECUTION COMPLETE ---\")\n",
        "# print(f\"Full results object saved to {output_filepath}\")\n",
        "# print(\"Keys in the final results object:\", final_results.keys())\n",
        "```\n",
        "\n",
        "#### **Step 4: Interpreting the Output**\n",
        "\n",
        "After the execution completes, the `final_results` object (and the file saved to disk) would contain a deeply nested but highly structured dictionary. A user could then explore the results programmatically.\n",
        "\n",
        "**Example of exploring the results:**\n",
        "\n",
        "```python\n",
        "# --- Example of how to access results after the run ---\n",
        "\n",
        "# Load the results from the file if needed\n",
        "# loaded_results = joblib.load(output_filepath)\n",
        "\n",
        "# # Access the R-squared of the trade multiplier model for Brazil\n",
        "# r_squared_brazil = loaded_results['pipeline_run_results']['empirical_results'] \\\n",
        "#     ['trade_multiplier_validation']['cross_country_summary'] \\\n",
        "#     ['performance_summary'].loc['BRA']['r_squared']\n",
        "\n",
        "# print(f\"\\nModel R-squared for Brazil: {r_squared_brazil:.4f}\")\n",
        "\n",
        "# # Access the bifurcation plot data for Figure 4a\n",
        "# bifurcation_plot_data = loaded_results['pipeline_run_results']['theoretical_results'] \\\n",
        "#     ['fig4a']['plot_data']\n",
        "\n",
        "# print(f\"Generated {len(bifurcation_plot_data['x_coords'])} points for the bifurcation diagram.\")\n",
        "\n",
        "# # Access the HP filter robustness report\n",
        "# hp_robustness_report = loaded_results['robustness_analysis_report'] \\\n",
        "#     ['specification_robustness_report']['hp_filter_sensitivity']\n",
        "\n",
        "# print(\"\\nHP Filter Sensitivity Report:\")\n",
        "# print(hp_robustness_report)\n",
        "```\n",
        "\n",
        "This complete example demonstrates how a user interacts with the pipeline at the highest level. They provide standardized data and a comprehensive configuration file, and they receive a single, auditable object containing every result and report from the entire research workflow. This is the hallmark of a professional, reproducible computational science project.\n"
      ],
      "metadata": {
        "id": "Juv5xn8SVACT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK I: DATA VALIDATION AND QUALITY ASSURANCE\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 1.1.1: MASTER CONFIGURATION DICTIONARY VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "def _validate_master_config_schema(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validates the structure, types, and constraints of the master configuration dictionary.\n",
        "\n",
        "    This function performs a deep validation of the configuration object, ensuring\n",
        "    all required keys are present, data types are correct, and critical\n",
        "    constraints (e.g., sum of trader weights equals 1) are met. It is the\n",
        "    first line of defense against invalid experimental setups.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The master configuration dictionary for the study.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: A list of dictionaries, where each dictionary\n",
        "                               represents a validation error. An empty list\n",
        "                               indicates successful validation. Each error\n",
        "                               dictionary contains 'path', 'message', and 'status'.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store any validation errors found.\n",
        "    errors = []\n",
        "\n",
        "    # Define the top-level keys that are mandatory for the configuration.\n",
        "    required_top_keys = ['empirical_analysis', 'theoretical_simulation']\n",
        "    # Check for the presence of each required top-level key.\n",
        "    for key in required_top_keys:\n",
        "        # If a key is missing, record an error and continue to the next key.\n",
        "        if key not in config:\n",
        "            errors.append({\n",
        "                'path': key,\n",
        "                'message': f\"Missing required top-level key: '{key}'.\",\n",
        "                'status': 'FAIL'\n",
        "            })\n",
        "\n",
        "    # If essential top-level keys are missing, return immediately as further checks are impossible.\n",
        "    if errors:\n",
        "        return errors\n",
        "\n",
        "    # --- Validate 'theoretical_simulation' section ---\n",
        "    # Check for the presence of 'figure_scenarios' which contains simulation parameters.\n",
        "    if 'figure_scenarios' not in config.get('theoretical_simulation', {}):\n",
        "        errors.append({\n",
        "            'path': 'theoretical_simulation.figure_scenarios',\n",
        "            'message': \"Missing required key: 'figure_scenarios'.\",\n",
        "            'status': 'FAIL'\n",
        "        })\n",
        "        # Return if this critical section is missing.\n",
        "        return errors\n",
        "\n",
        "    # Retrieve the dictionary of figure-specific simulation scenarios.\n",
        "    scenarios = config['theoretical_simulation']['figure_scenarios']\n",
        "    # Iterate through each scenario defined in the configuration.\n",
        "    for fig_name, scenario in scenarios.items():\n",
        "        # Construct the base path for error reporting within this scenario.\n",
        "        base_path = f\"theoretical_simulation.figure_scenarios.{fig_name}\"\n",
        "\n",
        "        # Validate the presence and type of 'fixed_params'.\n",
        "        if 'fixed_params' not in scenario or not isinstance(scenario['fixed_params'], dict):\n",
        "            errors.append({\n",
        "                'path': f\"{base_path}.fixed_params\",\n",
        "                'message': \"Scenario must contain a 'fixed_params' dictionary.\",\n",
        "                'status': 'FAIL'\n",
        "            })\n",
        "            # Skip to the next scenario if 'fixed_params' is missing.\n",
        "            continue\n",
        "\n",
        "        # Retrieve the fixed parameters for the current scenario.\n",
        "        params = scenario['fixed_params']\n",
        "\n",
        "        # --- Constraint Validation: Trader Weights ---\n",
        "        # Check for the presence of trader weights.\n",
        "        if all(k in params for k in ['w_F', 'w_C', 'w_E']):\n",
        "            # Sum the weights.\n",
        "            # Constraint: w_F + w_C + w_E = 1.0\n",
        "            sum_weights = params['w_F'] + params['w_C'] + params['w_E']\n",
        "            # Use numpy.isclose for robust floating-point comparison.\n",
        "            if not np.isclose(sum_weights, 1.0, atol=1e-9):\n",
        "                errors.append({\n",
        "                    'path': f\"{base_path}.fixed_params.weights\",\n",
        "                    'message': f\"Sum of weights w_F, w_C, w_E must be 1.0, but got {sum_weights}.\",\n",
        "                    'status': 'FAIL'\n",
        "                })\n",
        "        # Handle dynamic weight calculation for bifurcation scenarios.\n",
        "        elif 'bifurcation_param' in scenario and scenario['bifurcation_param']['name'] == 'w_F':\n",
        "            # In this case, w_C is often implicitly defined as 1 - w_F.\n",
        "            # We check that the fixed weights sum to a value less than 1.\n",
        "            fixed_weights = params.get('w_C', 0.0) + params.get('w_E', 0.0)\n",
        "            if fixed_weights > 1.0:\n",
        "                 errors.append({\n",
        "                    'path': f\"{base_path}.fixed_params.weights\",\n",
        "                    'message': f\"Sum of fixed weights w_C and w_E ({fixed_weights}) cannot exceed 1.0 in a w_F bifurcation.\",\n",
        "                    'status': 'FAIL'\n",
        "                })\n",
        "\n",
        "        # --- Range Validation for Key Economic Parameters ---\n",
        "        # Validate the income elasticity of imports ('pi').\n",
        "        if 'pi' in params and not (0.5 <= params['pi'] <= 5.0):\n",
        "            errors.append({\n",
        "                'path': f\"{base_path}.fixed_params.pi\",\n",
        "                'message': f\"Parameter 'pi' must be in [0.5, 5.0], but got {params['pi']}.\",\n",
        "                'status': 'FAIL'\n",
        "            })\n",
        "\n",
        "        # Validate the fundamentalist reaction parameter ('mu').\n",
        "        if 'mu' in params and not (0 < params['mu'] <= 50.0): # Extended range for robustness\n",
        "            errors.append({\n",
        "                'path': f\"{base_path}.fixed_params.mu\",\n",
        "                'message': f\"Parameter 'mu' must be in (0, 50.0], but got {params['mu']}.\",\n",
        "                'status': 'FAIL'\n",
        "            })\n",
        "\n",
        "    # Return the list of all accumulated errors.\n",
        "    return errors\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 1.1.2: DATAFRAME STRUCTURE VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "def _validate_dataframe_structures(\n",
        "    macro_df: pd.DataFrame,\n",
        "    fx_df: pd.DataFrame\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validates the structural integrity of the input macroeconomic and FX DataFrames.\n",
        "\n",
        "    This function checks for correct indexing (MultiIndex), column presence,\n",
        "    data types, and geographic consistency between the two primary datasets.\n",
        "    It ensures that the data is correctly formatted for all subsequent\n",
        "    time-series and cross-sectional analysis.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): DataFrame containing annual macroeconomic data.\n",
        "        fx_df (pd.DataFrame): DataFrame containing daily foreign exchange data.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: A list of dictionaries detailing any validation\n",
        "                               failures. An empty list signifies success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store validation errors.\n",
        "    errors = []\n",
        "\n",
        "    # --- Validate macro_df (Annual Macroeconomic Data) ---\n",
        "    # Define the expected structure for the macro DataFrame.\n",
        "    macro_expected_structure = {\n",
        "        'name': 'macro_df',\n",
        "        'index_names': ['country_iso', 'year'],\n",
        "        'columns': {\n",
        "            'gdp_const_lcu': np.float64,\n",
        "            'imports_const_lcu': np.float64,\n",
        "            'exports_const_lcu': np.float64,\n",
        "            'reer': np.float64\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Check if the object is a pandas DataFrame.\n",
        "    if not isinstance(macro_df, pd.DataFrame):\n",
        "        errors.append({'path': 'macro_df', 'message': 'Input is not a pandas DataFrame.', 'status': 'FAIL'})\n",
        "    else:\n",
        "        # Check for MultiIndex.\n",
        "        if not isinstance(macro_df.index, pd.MultiIndex):\n",
        "            errors.append({'path': 'macro_df.index', 'message': 'Index is not a MultiIndex.', 'status': 'FAIL'})\n",
        "        # Check index names.\n",
        "        elif list(macro_df.index.names) != macro_expected_structure['index_names']:\n",
        "            errors.append({\n",
        "                'path': 'macro_df.index.names',\n",
        "                'message': f\"Expected index names {macro_expected_structure['index_names']}, but got {list(macro_df.index.names)}.\",\n",
        "                'status': 'FAIL'\n",
        "            })\n",
        "\n",
        "        # Check for required columns.\n",
        "        missing_cols = set(macro_expected_structure['columns'].keys()) - set(macro_df.columns)\n",
        "        if missing_cols:\n",
        "            errors.append({\n",
        "                'path': 'macro_df.columns',\n",
        "                'message': f\"Missing required columns: {missing_cols}.\",\n",
        "                'status': 'FAIL'\n",
        "            })\n",
        "\n",
        "        # Check column data types.\n",
        "        for col, dtype in macro_expected_structure['columns'].items():\n",
        "            if col in macro_df.columns and macro_df[col].dtype != dtype:\n",
        "                errors.append({\n",
        "                    'path': f'macro_df.{col}.dtype',\n",
        "                    'message': f\"Expected dtype {dtype} for column '{col}', but got {macro_df[col].dtype}.\",\n",
        "                    'status': 'FAIL'\n",
        "                })\n",
        "\n",
        "    # --- Validate fx_df (Daily FX Data) ---\n",
        "    # Define the expected structure for the FX DataFrame.\n",
        "    fx_expected_structure = {\n",
        "        'name': 'fx_df',\n",
        "        'index_names': ['country_iso', 'date'],\n",
        "        'columns': {'fx_rate_usd': np.float64},\n",
        "        'date_dtype': 'datetime64[ns]'\n",
        "    }\n",
        "\n",
        "    # Check if the object is a pandas DataFrame.\n",
        "    if not isinstance(fx_df, pd.DataFrame):\n",
        "        errors.append({'path': 'fx_df', 'message': 'Input is not a pandas DataFrame.', 'status': 'FAIL'})\n",
        "    else:\n",
        "        # Check for MultiIndex.\n",
        "        if not isinstance(fx_df.index, pd.MultiIndex):\n",
        "            errors.append({'path': 'fx_df.index', 'message': 'Index is not a MultiIndex.', 'status': 'FAIL'})\n",
        "        # Check index names.\n",
        "        elif list(fx_df.index.names) != fx_expected_structure['index_names']:\n",
        "            errors.append({\n",
        "                'path': 'fx_df.index.names',\n",
        "                'message': f\"Expected index names {fx_expected_structure['index_names']}, but got {list(fx_df.index.names)}.\",\n",
        "                'status': 'FAIL'\n",
        "            })\n",
        "        else:\n",
        "            # Check the data type of the 'date' index level.\n",
        "            date_level = fx_df.index.get_level_values('date')\n",
        "            if not pd.api.types.is_datetime64_any_dtype(date_level):\n",
        "                errors.append({\n",
        "                    'path': 'fx_df.index.date',\n",
        "                    'message': f\"Expected date index level to be datetime64, but got {date_level.dtype}.\",\n",
        "                    'status': 'FAIL'\n",
        "                })\n",
        "\n",
        "        # Check for required columns.\n",
        "        missing_cols = set(fx_expected_structure['columns'].keys()) - set(fx_df.columns)\n",
        "        if missing_cols:\n",
        "            errors.append({'path': 'fx_df.columns', 'message': f\"Missing required columns: {missing_cols}.\", 'status': 'FAIL'})\n",
        "\n",
        "    # --- Cross-DataFrame Consistency Check ---\n",
        "    # Ensure both DataFrames contain the same set of countries.\n",
        "    if isinstance(macro_df, pd.DataFrame) and isinstance(fx_df, pd.DataFrame) and \\\n",
        "       isinstance(macro_df.index, pd.MultiIndex) and isinstance(fx_df.index, pd.MultiIndex):\n",
        "\n",
        "        # Extract unique country ISO codes from each DataFrame.\n",
        "        macro_countries: Set[str] = set(macro_df.index.get_level_values('country_iso').unique())\n",
        "        fx_countries: Set[str] = set(fx_df.index.get_level_values('country_iso').unique())\n",
        "\n",
        "        # Check if the sets of countries are identical.\n",
        "        if macro_countries != fx_countries:\n",
        "            # Identify countries present in one DataFrame but not the other.\n",
        "            only_in_macro = macro_countries - fx_countries\n",
        "            only_in_fx = fx_countries - macro_countries\n",
        "            message = \"Country sets are inconsistent between DataFrames. \"\n",
        "            if only_in_macro:\n",
        "                message += f\"Only in macro_df: {only_in_macro}. \"\n",
        "            if only_in_fx:\n",
        "                message += f\"Only in fx_df: {only_in_fx}.\"\n",
        "            errors.append({'path': 'cross_dataframe.countries', 'message': message, 'status': 'FAIL'})\n",
        "\n",
        "    # Return the list of all accumulated errors.\n",
        "    return errors\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 1.1.3: TEMPORAL CONSISTENCY AND COMPLETENESS VERIFICATION\n",
        "# =============================================================================\n",
        "\n",
        "def _validate_temporal_consistency(\n",
        "    macro_df: pd.DataFrame,\n",
        "    fx_df: pd.DataFrame\n",
        ") -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Verifies the temporal completeness and consistency of the time-series data.\n",
        "\n",
        "    This function checks two main aspects:\n",
        "    1.  Annual Data (`macro_df`): Ensures that for each country, the data spans\n",
        "        the full expected range (1960-2023) without gaps.\n",
        "    2.  Daily Data (`fx_df`): Checks for missing business days within the\n",
        "        observed date range for each country.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): DataFrame with annual macroeconomic data.\n",
        "        fx_df (pd.DataFrame): DataFrame with daily foreign exchange data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, Any], List[Dict[str, Any]]]:\n",
        "            - A dictionary containing detailed temporal quality reports for each country.\n",
        "            - A list of dictionaries summarizing any critical validation failures.\n",
        "    \"\"\"\n",
        "    # Initialize containers for the report and any errors.\n",
        "    report = {}\n",
        "    errors = []\n",
        "\n",
        "    # --- Validate Annual Data (`macro_df`) ---\n",
        "    # Define the expected full range of years for the annual data.\n",
        "    expected_years = set(range(1960, 2024))\n",
        "    # Group the DataFrame by country to perform checks individually.\n",
        "    for country, df_country in macro_df.groupby(level='country_iso'):\n",
        "        # Initialize the report structure for the current country.\n",
        "        report[country] = {}\n",
        "        # Extract the years present for the current country.\n",
        "        actual_years = set(df_country.index.get_level_values('year'))\n",
        "        # Find the set of missing years by comparing actual vs. expected.\n",
        "        missing_years = expected_years - actual_years\n",
        "\n",
        "        # Calculate the completeness ratio.\n",
        "        completeness = len(actual_years) / len(expected_years)\n",
        "\n",
        "        # Populate the report for the annual data of the current country.\n",
        "        report[country]['annual_data'] = {\n",
        "            'completeness_ratio': completeness,\n",
        "            'expected_years': len(expected_years),\n",
        "            'actual_years': len(actual_years),\n",
        "            'missing_years_count': len(missing_years),\n",
        "            'missing_years_list': sorted(list(missing_years))\n",
        "        }\n",
        "        # If completeness is below a critical threshold (e.g., 90%), log it as an error.\n",
        "        if completeness < 0.90:\n",
        "            errors.append({\n",
        "                'path': f'macro_df.{country}',\n",
        "                'message': f\"Annual data completeness is only {completeness:.2%}. Significant gaps detected.\",\n",
        "                'status': 'WARN'\n",
        "            })\n",
        "\n",
        "    # --- Validate Daily Data (`fx_df`) ---\n",
        "    # Group the DataFrame by country for individual checks.\n",
        "    for country, df_country in fx_df.groupby(level='country_iso'):\n",
        "        # Determine the start and end dates of the data for the country.\n",
        "        start_date = df_country.index.get_level_values('date').min()\n",
        "        end_date = df_country.index.get_level_values('date').max()\n",
        "\n",
        "        # Generate the expected set of business days for that period.\n",
        "        # Note: This uses a generic business day calendar (Mon-Fri).\n",
        "        expected_bdays = set(pd.bdate_range(start=start_date, end=end_date))\n",
        "        # Get the actual dates present in the data.\n",
        "        actual_dates = set(df_country.index.get_level_values('date'))\n",
        "\n",
        "        # Find the set of missing business days.\n",
        "        missing_bdays = expected_bdays - actual_dates\n",
        "\n",
        "        # Calculate the completeness ratio for business days.\n",
        "        completeness = len(actual_dates.intersection(expected_bdays)) / len(expected_bdays) if expected_bdays else 1.0\n",
        "\n",
        "        # Populate the report for the daily data of the current country.\n",
        "        report[country]['daily_data'] = {\n",
        "            'completeness_ratio': completeness,\n",
        "            'expected_bdays': len(expected_bdays),\n",
        "            'actual_dates_on_bdays': len(actual_dates.intersection(expected_bdays)),\n",
        "            'missing_bdays_count': len(missing_bdays),\n",
        "        }\n",
        "        # If completeness is low, log a warning.\n",
        "        if completeness < 0.95:\n",
        "            errors.append({\n",
        "                'path': f'fx_df.{country}',\n",
        "                'message': f\"Daily business day data completeness is only {completeness:.2%}.\",\n",
        "                'status': 'WARN'\n",
        "            })\n",
        "\n",
        "    # Return the detailed report and the list of summary errors.\n",
        "    return report, errors\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 1.1 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def validate_inputs(\n",
        "    macro_df: pd.DataFrame,\n",
        "    fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of all study inputs.\n",
        "\n",
        "    This master function executes a sequence of validation checks on the\n",
        "    configuration dictionary and the input DataFrames. It aggregates all\n",
        "    results and errors into a single, comprehensive report. If any critical\n",
        "    validation fails, it raises a ValueError summarizing all issues,\n",
        "    preventing the pipeline from running with invalid inputs.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): DataFrame with annual macroeconomic data.\n",
        "        fx_df (pd.DataFrame): DataFrame with daily foreign exchange data.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive validation report containing the results\n",
        "                        from all individual validation steps.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any critical validation check fails, summarizing all\n",
        "                    detected errors.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Validate the Master Configuration Dictionary ---\n",
        "    # Execute the schema validation for the configuration dictionary.\n",
        "    config_errors = _validate_master_config_schema(master_config)\n",
        "\n",
        "    # --- Step 2: Validate DataFrame Structures ---\n",
        "    # Execute the structural validation for both DataFrames.\n",
        "    df_structure_errors = _validate_dataframe_structures(macro_df, fx_df)\n",
        "\n",
        "    # --- Step 3: Validate Temporal Consistency ---\n",
        "    # Execute the temporal validation checks.\n",
        "    temporal_report, temporal_errors = _validate_temporal_consistency(macro_df, fx_df)\n",
        "\n",
        "    # --- Step 4: Aggregate Results and Errors ---\n",
        "    # Combine all errors from the validation steps.\n",
        "    all_errors = config_errors + df_structure_errors + temporal_errors\n",
        "\n",
        "    # Construct the final, comprehensive validation report.\n",
        "    validation_report = {\n",
        "        'validation_summary': {\n",
        "            'status': 'PASS' if not all_errors else 'FAIL',\n",
        "            'total_errors': len(all_errors),\n",
        "            'error_details': all_errors\n",
        "        },\n",
        "        'config_validation': {\n",
        "            'status': 'PASS' if not config_errors else 'FAIL',\n",
        "            'errors': config_errors\n",
        "        },\n",
        "        'structure_validation': {\n",
        "            'status': 'PASS' if not df_structure_errors else 'FAIL',\n",
        "            'errors': df_structure_errors\n",
        "        },\n",
        "        'temporal_validation': {\n",
        "            'status': 'PASS' if not temporal_errors else 'WARN',\n",
        "            'errors': temporal_errors,\n",
        "            'detailed_report': temporal_report\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- Step 5: Final Decision ---\n",
        "    # If any errors were found, raise a single, informative ValueError.\n",
        "    # We check for any error with status 'FAIL'. Warnings do not stop execution.\n",
        "    critical_errors = [e for e in all_errors if e.get('status') == 'FAIL']\n",
        "    if critical_errors:\n",
        "        # Format a detailed error message summarizing all critical failures.\n",
        "        error_summary = \"\\nInput validation failed with critical errors:\"\n",
        "        for error in critical_errors:\n",
        "            error_summary += f\"\\n- Path: {error['path']}\\n  Message: {error['message']}\"\n",
        "        # Raise the exception to halt the pipeline.\n",
        "        raise ValueError(error_summary)\n",
        "\n",
        "    # If validation passes, print a success message and return the report.\n",
        "    print(\"Input validation successful. All inputs conform to the required schema and constraints.\")\n",
        "    return validation_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 1.2.1: STATISTICAL OUTLIER DETECTION\n",
        "# =============================================================================\n",
        "\n",
        "def _detect_statistical_outliers(\n",
        "    macro_df: pd.DataFrame,\n",
        "    fx_df: pd.DataFrame\n",
        ") -> Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Detects statistical outliers and economically implausible values in the data.\n",
        "\n",
        "    This function applies two primary methods for outlier detection:\n",
        "    1.  Interquartile Range (IQR): Identifies points falling 1.5 * IQR below Q1\n",
        "        or above Q3, calculated on a per-country basis.\n",
        "    2.  Economic Reasonableness: Applies hard thresholds to growth rates and\n",
        "        returns based on established economic literature to flag implausible values.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): DataFrame with annual macroeconomic data.\n",
        "        fx_df (pd.DataFrame): DataFrame with daily foreign exchange data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]:\n",
        "            - A dictionary of DataFrames containing boolean flags for each\n",
        "              detected outlier.\n",
        "            - A summary dictionary reporting the count of outliers detected by\n",
        "              each method for each variable.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure inputs are pandas DataFrames.\n",
        "    if not isinstance(macro_df, pd.DataFrame) or not isinstance(fx_df, pd.DataFrame):\n",
        "        raise TypeError(\"Inputs 'macro_df' and 'fx_df' must be pandas DataFrames.\")\n",
        "\n",
        "    # Initialize dictionaries to store outlier flags and summary statistics.\n",
        "    outlier_flags = {}\n",
        "    outlier_summary = {}\n",
        "\n",
        "    # --- 1. Process Annual Macroeconomic Data (macro_df) ---\n",
        "    # Define columns to check in the macro data.\n",
        "    macro_cols_to_check = ['gdp_const_lcu', 'imports_const_lcu', 'exports_const_lcu']\n",
        "    # Compute annual log growth rates for outlier detection.\n",
        "    # Equation: growth_rate_t = log(value_t) - log(value_{t-1})\n",
        "    macro_growth = macro_df[macro_cols_to_check].groupby(level='country_iso').transform(\n",
        "        lambda x: np.log(x).diff()\n",
        "    )\n",
        "    macro_growth.rename(columns=lambda c: f\"{c}_growth\", inplace=True)\n",
        "\n",
        "    # --- IQR Method for Macro Growth Rates ---\n",
        "    # Calculate Q1, Q3, and IQR on a per-country basis.\n",
        "    Q1 = macro_growth.groupby(level='country_iso').transform('quantile', 0.25)\n",
        "    Q3 = macro_growth.groupby(level='country_iso').transform('quantile', 0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    # Define the lower and upper bounds for outlier detection.\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Generate a boolean mask for values outside the IQR bounds.\n",
        "    iqr_outliers_macro = (macro_growth < lower_bound) | (macro_growth > upper_bound)\n",
        "    outlier_flags['macro_growth_iqr'] = iqr_outliers_macro\n",
        "    outlier_summary['macro_growth_iqr'] = iqr_outliers_macro.sum().to_dict()\n",
        "\n",
        "    # --- Economic Reasonableness for Macro Growth Rates ---\n",
        "    # Define plausible economic bounds for annual growth (-25% to +30%).\n",
        "    gdp_growth_col = 'gdp_const_lcu_growth'\n",
        "    economic_outliers_gdp = (macro_growth[gdp_growth_col] < -0.25) | (macro_growth[gdp_growth_col] > 0.30)\n",
        "    outlier_flags['gdp_growth_economic'] = economic_outliers_gdp.to_frame()\n",
        "    outlier_summary['gdp_growth_economic'] = {'count': economic_outliers_gdp.sum()}\n",
        "\n",
        "    # --- 2. Process Daily FX Data (fx_df) ---\n",
        "    # Compute daily log returns for the FX data.\n",
        "    # Equation: return_t = log(fx_rate_t) - log(fx_rate_{t-1})\n",
        "    fx_returns = fx_df[['fx_rate_usd']].groupby(level='country_iso').transform(\n",
        "        lambda x: np.log(x).diff()\n",
        "    )\n",
        "    fx_returns.rename(columns={'fx_rate_usd': 'fx_return'}, inplace=True)\n",
        "\n",
        "    # --- Economic Reasonableness for FX Returns ---\n",
        "    # Define plausible bounds for daily returns (-50% to +50%) to catch extreme events/errors.\n",
        "    economic_outliers_fx = (fx_returns['fx_return'] < -0.50) | (fx_returns['fx_return'] > 0.50)\n",
        "    outlier_flags['fx_return_economic'] = economic_outliers_fx.to_frame()\n",
        "    outlier_summary['fx_return_economic'] = {'count': economic_outliers_fx.sum()}\n",
        "\n",
        "    # Return the detailed flag DataFrames and the summary dictionary.\n",
        "    return outlier_flags, outlier_summary\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 1.2.2: MISSING VALUE PATTERN ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def _analyze_missing_patterns(\n",
        "    macro_df: pd.DataFrame,\n",
        "    fx_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyzes and reports patterns of missing values in the datasets.\n",
        "\n",
        "    This function provides a detailed report on missing data, including:\n",
        "    1.  Overall missing value percentages for each variable.\n",
        "    2.  Analysis of consecutive missing value blocks (gaps).\n",
        "    3.  A correlation matrix of missingness to identify systematic data issues.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): DataFrame with annual macroeconomic data.\n",
        "        fx_df (pd.DataFrame): DataFrame with daily foreign exchange data.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing detailed reports on\n",
        "                        missing value patterns for both DataFrames.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure inputs are pandas DataFrames.\n",
        "    if not isinstance(macro_df, pd.DataFrame) or not isinstance(fx_df, pd.DataFrame):\n",
        "        raise TypeError(\"Inputs 'macro_df' and 'fx_df' must be pandas DataFrames.\")\n",
        "\n",
        "    # Initialize the main report dictionary.\n",
        "    report = {}\n",
        "\n",
        "    # Process each DataFrame through a helper function.\n",
        "    for df_name, df in [('macro_df', macro_df), ('fx_df', fx_df)]:\n",
        "        # Create a boolean mask of missing values.\n",
        "        is_missing = df.isnull()\n",
        "\n",
        "        # 1. Calculate overall missing summary statistics.\n",
        "        missing_count = is_missing.sum()\n",
        "        missing_percent = (missing_count / len(df)) * 100\n",
        "        summary_df = pd.DataFrame({\n",
        "            'missing_count': missing_count,\n",
        "            'missing_percent': missing_percent\n",
        "        })\n",
        "\n",
        "        # 2. Analyze consecutive missing value gaps.\n",
        "        gap_reports = {}\n",
        "        for col in df.columns:\n",
        "            # Create a series that identifies blocks of consecutive values (True/False).\n",
        "            blocks = (is_missing[col] != is_missing[col].shift()).cumsum()\n",
        "            # Filter for blocks that are missing and count their sizes.\n",
        "            gaps = is_missing[col].groupby(blocks).sum()[is_missing[col]]\n",
        "            gap_reports[col] = gaps.describe().to_dict() if not gaps.empty else {}\n",
        "\n",
        "        # 3. Calculate the correlation of missingness across variables.\n",
        "        missing_corr = is_missing.corr()\n",
        "\n",
        "        # Populate the report for the current DataFrame.\n",
        "        report[df_name] = {\n",
        "            'missing_summary': summary_df,\n",
        "            'consecutive_gaps_summary': gap_reports,\n",
        "            'missingness_correlation': missing_corr\n",
        "        }\n",
        "\n",
        "    # Return the comprehensive report.\n",
        "    return report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 1.2.3: ECONOMIC RELATIONSHIP VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "def _validate_economic_relationships(\n",
        "    macro_df: pd.DataFrame\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validates key economic relationships within the macroeconomic data.\n",
        "\n",
        "    This function checks for plausible long-term relationships and value ranges\n",
        "    that are expected from economic theory, such as:\n",
        "    1.  A positive correlation between GDP and imports.\n",
        "    2.  Reasonable bounds for the Real Effective Exchange Rate (REER) index.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): DataFrame with annual macroeconomic data.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: A list of dictionaries, where each dictionary\n",
        "                               represents a validation warning or failure.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure input is a pandas DataFrame.\n",
        "    if not isinstance(macro_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'macro_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # Initialize a list to store validation warnings.\n",
        "    warnings = []\n",
        "\n",
        "    # Group data by country for individual relationship checks.\n",
        "    for country, df_country in macro_df.groupby(level='country_iso'):\n",
        "        # --- 1. GDP-Import Correlation Check ---\n",
        "        # Check if required columns are present and have enough data points.\n",
        "        if 'gdp_const_lcu' in df_country and 'imports_const_lcu' in df_country and len(df_country) > 10:\n",
        "            # Calculate the Pearson correlation between GDP and imports.\n",
        "            correlation = df_country['gdp_const_lcu'].corr(df_country['imports_const_lcu'])\n",
        "            # Check if the correlation is positive and reasonably strong.\n",
        "            if pd.isna(correlation) or correlation < 0.3:\n",
        "                warnings.append({\n",
        "                    'country': country,\n",
        "                    'check': 'GDP-Import Correlation',\n",
        "                    'metric_value': correlation,\n",
        "                    'message': 'Expected a strong positive correlation (> 0.3) between GDP and imports.',\n",
        "                    'status': 'WARN'\n",
        "                })\n",
        "\n",
        "        # --- 2. REER Bounds Check ---\n",
        "        # Check if the REER column exists.\n",
        "        if 'reer' in df_country:\n",
        "            # Check if REER values fall outside a plausible range (e.g., 30-300).\n",
        "            if df_country['reer'].min() < 30 or df_country['reer'].max() > 300:\n",
        "                warnings.append({\n",
        "                    'country': country,\n",
        "                    'check': 'REER Bounds',\n",
        "                    'metric_value': f\"min: {df_country['reer'].min()}, max: {df_country['reer'].max()}\",\n",
        "                    'message': 'REER index values fall outside the typical range of [30, 300].',\n",
        "                    'status': 'WARN'\n",
        "                })\n",
        "\n",
        "    # Return the list of accumulated warnings.\n",
        "    return warnings\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 1.2 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def assess_data_quality(\n",
        "    macro_df: pd.DataFrame,\n",
        "    fx_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive data quality and anomaly detection assessment.\n",
        "\n",
        "    This master function executes a sequence of data quality checks, including\n",
        "    statistical outlier detection, missing value pattern analysis, and economic\n",
        "    relationship validation. It aggregates all findings into a single, structured\n",
        "    report that provides a holistic view of the data's integrity before it is\n",
        "    used in the research pipeline.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): DataFrame with annual macroeconomic data.\n",
        "        fx_df (pd.DataFrame): DataFrame with daily foreign exchange data.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive data quality report containing the\n",
        "                        results from all individual assessment steps.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Detect Statistical and Economic Outliers ---\n",
        "    # Call the outlier detection function.\n",
        "    outlier_flags, outlier_summary = _detect_statistical_outliers(macro_df, fx_df)\n",
        "\n",
        "    # --- Step 2: Analyze Missing Value Patterns ---\n",
        "    # Call the missing value analysis function.\n",
        "    missing_patterns_report = _analyze_missing_patterns(macro_df, fx_df)\n",
        "\n",
        "    # --- Step 3: Validate Economic Relationships ---\n",
        "    # Call the economic relationship validation function.\n",
        "    economic_warnings = _validate_economic_relationships(macro_df)\n",
        "\n",
        "    # --- Step 4: Aggregate Results into a Final Report ---\n",
        "    # Construct the final, comprehensive data quality report.\n",
        "    quality_report = {\n",
        "        'statistical_outlier_detection': {\n",
        "            'summary': outlier_summary,\n",
        "            'flags': outlier_flags\n",
        "        },\n",
        "        'missing_value_analysis': missing_patterns_report,\n",
        "        'economic_relationship_validation': {\n",
        "            'warnings_count': len(economic_warnings),\n",
        "            'warnings_details': economic_warnings\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Print a summary message to the console.\n",
        "    print(\"Data quality assessment complete.\")\n",
        "    print(f\" - Outliers detected: {sum(v['count'] for k, v in outlier_summary.items() if 'count' in v)} potential outliers flagged.\")\n",
        "    print(f\" - Economic relationship warnings: {len(economic_warnings)} potential issues found.\")\n",
        "\n",
        "    # Return the comprehensive report.\n",
        "    return quality_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 1.3.1: MISSING VALUE IMPUTATION STRATEGY\n",
        "# =============================================================================\n",
        "\n",
        "def _impute_missing_values(\n",
        "    macro_df: pd.DataFrame,\n",
        "    fx_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Imputes missing values in the datasets using appropriate strategies.\n",
        "\n",
        "    This function applies a differentiated imputation strategy:\n",
        "    1.  Daily FX Data (`fx_df`): Forward-fills values to handle non-trading\n",
        "        days (weekends, holidays), which is the standard practice for financial\n",
        "        time series.\n",
        "    2.  Annual Macro Data (`macro_df`): Uses a multivariate IterativeImputer\n",
        "        to estimate missing values based on relationships with other variables,\n",
        "        providing a more robust imputation than simple univariate methods.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): Raw annual macroeconomic data.\n",
        "        fx_df (pd.DataFrame): Raw daily foreign exchange data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "            - macro_df_imputed: Macro DataFrame with missing values filled.\n",
        "            - fx_df_imputed: FX DataFrame with missing values filled.\n",
        "            - macro_imputation_mask: Boolean mask indicating which macro values were imputed.\n",
        "            - fx_imputation_mask: Boolean mask indicating which FX values were imputed.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(macro_df, pd.DataFrame) or not isinstance(fx_df, pd.DataFrame):\n",
        "        raise TypeError(\"Inputs must be pandas DataFrames.\")\n",
        "\n",
        "    # --- 1. Impute Daily FX Data ---\n",
        "    # Create a boolean mask to track which values are originally missing.\n",
        "    fx_imputation_mask = fx_df.isnull()\n",
        "    # Forward-fill is appropriate for daily financial data to carry over the last known price.\n",
        "    # We group by country to prevent filling across different entities.\n",
        "    # A limit of 7 is set to avoid filling across very long gaps without review.\n",
        "    fx_df_imputed = fx_df.groupby(level='country_iso').ffill(limit=7)\n",
        "\n",
        "    # --- 2. Impute Annual Macro Data ---\n",
        "    # Create a boolean mask for the macro data.\n",
        "    macro_imputation_mask = macro_df.isnull()\n",
        "    # Initialize the IterativeImputer. It models each feature as a function of others.\n",
        "    imputer = IterativeImputer(max_iter=10, random_state=0)\n",
        "\n",
        "    # The imputer works on numpy arrays, so we must preserve the index and columns.\n",
        "    original_index = macro_df.index\n",
        "    original_columns = macro_df.columns\n",
        "\n",
        "    # Apply the imputer to the DataFrame's values.\n",
        "    macro_df_imputed_np = imputer.fit_transform(macro_df)\n",
        "\n",
        "    # Reconstruct the DataFrame with the original index and columns.\n",
        "    macro_df_imputed = pd.DataFrame(macro_df_imputed_np, index=original_index, columns=original_columns)\n",
        "\n",
        "    # Return the imputed dataframes and their corresponding masks for auditability.\n",
        "    return macro_df_imputed, fx_df_imputed, macro_imputation_mask, fx_imputation_mask\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 1.3.2: OUTLIER TREATMENT AND SMOOTHING\n",
        "# =============================================================================\n",
        "\n",
        "def _treat_outliers(\n",
        "    macro_df_imputed: pd.DataFrame,\n",
        "    fx_df_imputed: pd.DataFrame,\n",
        "    outlier_flags: Dict[str, pd.DataFrame]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Treats identified outliers by winsorizing growth rates and reconstructing levels.\n",
        "\n",
        "    This function provides a robust and auditable process for mitigating the\n",
        "    impact of extreme data points identified in the quality assessment phase.\n",
        "    Its methodology is twofold:\n",
        "    1.  For macroeconomic data, it treats outliers in the growth rate space via\n",
        "        winsorization and then meticulously reconstructs the original level series.\n",
        "        This preserves the integrity of the time series while capping the influence\n",
        "        of extreme single-period shocks.\n",
        "    2.  For FX data, values flagged as economically implausible are treated as\n",
        "        missing data and re-imputed using a forward-fill, consistent with\n",
        "        financial time series best practices.\n",
        "\n",
        "    Args:\n",
        "        macro_df_imputed (pd.DataFrame): The imputed annual macroeconomic data.\n",
        "        fx_df_imputed (pd.DataFrame): The imputed daily foreign exchange data.\n",
        "        outlier_flags (Dict[str, pd.DataFrame]): A dictionary of boolean masks\n",
        "            from the outlier detection step, flagging which points to treat.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, List[Dict[str, Any]]]:\n",
        "            - macro_df_treated: The macro DataFrame with outliers treated.\n",
        "            - fx_df_treated: The FX DataFrame with outliers treated.\n",
        "            - treatment_log: A detailed log of all modifications made.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(macro_df_imputed, pd.DataFrame) or not isinstance(fx_df_imputed, pd.DataFrame):\n",
        "        raise TypeError(\"Inputs must be pandas DataFrames.\")\n",
        "    if 'macro_growth_iqr' not in outlier_flags or 'fx_return_economic' not in outlier_flags:\n",
        "        raise KeyError(\"`outlier_flags` dictionary is missing required keys.\")\n",
        "\n",
        "    # Initialize a log to record all treatments for transparency and auditability.\n",
        "    treatment_log = []\n",
        "    # Create copies of the input dataframes to avoid modifying them in place.\n",
        "    macro_df_treated = macro_df_imputed.copy()\n",
        "    fx_df_treated = fx_df_imputed.copy()\n",
        "\n",
        "    # --- 1. Treat Macroeconomic Outliers via Growth Rate Winsorization ---\n",
        "    # Define the columns in the macro data that require treatment.\n",
        "    macro_cols_to_treat = ['gdp_const_lcu', 'imports_const_lcu', 'exports_const_lcu']\n",
        "\n",
        "    # Step 1.1: Transform level data to log-space.\n",
        "    log_macro_df = np.log(macro_df_treated[macro_cols_to_treat])\n",
        "\n",
        "    # Step 1.2: Calculate the original log-growth rates.\n",
        "    # Equation: g_t = log(Y_t) - log(Y_{t-1})\n",
        "    original_growth = log_macro_df.groupby(level='country_iso').diff()\n",
        "\n",
        "    # Step 1.3: Define the winsorization function.\n",
        "    # This function clips a series at its 1st and 99th percentiles.\n",
        "    def winsorize_series(s: pd.Series) -> pd.Series:\n",
        "        lower_bound = s.quantile(0.01)\n",
        "        upper_bound = s.quantile(0.99)\n",
        "        return s.clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "    # Step 1.4: Apply winsorization to the growth rates on a per-country basis.\n",
        "    winsorized_growth = original_growth.groupby(level='country_iso').transform(winsorize_series)\n",
        "\n",
        "    # Step 1.5: Create the final, treated growth rate series.\n",
        "    # Where an outlier was flagged, use the winsorized value; otherwise, keep the original.\n",
        "    iqr_flags = outlier_flags['macro_growth_iqr']\n",
        "    final_growth = original_growth.where(~iqr_flags, winsorized_growth)\n",
        "\n",
        "    # Step 1.6: Reconstruct the log-level series from the treated growth rates.\n",
        "    # This is the critical inverse of the diff() operation.\n",
        "    def reconstruct_from_growth(df_group: pd.DataFrame) -> pd.DataFrame:\n",
        "        # Get the first valid log-level value for this country.\n",
        "        first_log_level = df_group.iloc[0]\n",
        "        # Calculate the cumulative sum of the treated growth rates.\n",
        "        cumulative_growth = df_group.iloc[1:].cumsum()\n",
        "        # The reconstructed series is the first level plus the cumulative growth.\n",
        "        reconstructed_log_level = first_log_level.add(cumulative_growth, fill_value=0)\n",
        "        # Combine the first row with the reconstructed part.\n",
        "        return pd.concat([first_log_level.to_frame().T, reconstructed_log_level])\n",
        "\n",
        "    # Apply the reconstruction logic to each country group.\n",
        "    reconstructed_log_levels = final_growth.groupby(level='country_iso').apply(reconstruct_from_growth)\n",
        "\n",
        "    # Step 1.7: Transform the reconstructed log-levels back to the original scale.\n",
        "    # Equation: Y'_t = exp(log(Y'_t))\n",
        "    macro_df_treated[macro_cols_to_treat] = np.exp(reconstructed_log_levels)\n",
        "\n",
        "    # --- 2. Treat FX Economic Outliers via Re-imputation ---\n",
        "    # This logic handles extreme, economically implausible daily returns.\n",
        "    fx_econ_flags = outlier_flags['fx_return_economic']['fx_return']\n",
        "\n",
        "    # Set the flagged outlier values to NaN, marking them as missing.\n",
        "    fx_df_treated.loc[fx_econ_flags, 'fx_rate_usd'] = np.nan\n",
        "\n",
        "    # Re-impute the newly created missing values using forward-fill.\n",
        "    # This is consistent with the handling of other missing financial data.\n",
        "    fx_df_treated['fx_rate_usd'] = fx_df_treated['fx_rate_usd'].groupby(level='country_iso').ffill(limit=7)\n",
        "\n",
        "    # --- 3. Generate Treatment Log ---\n",
        "    # Compare the final treated dataframes with the original imputed ones to log changes.\n",
        "    macro_changes = macro_df_treated[macro_df_imputed != macro_df_treated].stack()\n",
        "    for idx, val in macro_changes.items():\n",
        "        treatment_log.append({\n",
        "            'index': idx, 'column': idx[-1], 'method': 'Winsorize-Reconstruct',\n",
        "            'original_value': macro_df_imputed.loc[idx[:-1]][idx[-1]], 'treated_value': val\n",
        "        })\n",
        "\n",
        "    fx_changes = fx_df_treated[fx_df_imputed != fx_df_treated].stack()\n",
        "    for idx, val in fx_changes.items():\n",
        "         treatment_log.append({\n",
        "            'index': idx, 'column': idx[-1], 'method': 'Re-impute (Economic)',\n",
        "            'original_value': fx_df_imputed.loc[idx[:-1]][idx[-1]], 'treated_value': val\n",
        "        })\n",
        "\n",
        "    # Return the treated dataframes and the detailed audit log.\n",
        "    return macro_df_treated, fx_df_treated, treatment_log\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 1.3.3: VARIABLE TRANSFORMATION AND STANDARDIZATION\n",
        "# =============================================================================\n",
        "\n",
        "def _transform_and_standardize_variables(\n",
        "    macro_df: pd.DataFrame,\n",
        "    fx_df: pd.DataFrame\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Transforms and standardizes data into an analysis-ready format.\n",
        "\n",
        "    This function performs the final preprocessing steps:\n",
        "    1.  Logarithmic Transformation: Converts level data to logs, which is a\n",
        "        prerequisite for calculating growth rates and returns.\n",
        "    2.  Growth/Return Calculation: Computes log-differences to get the key\n",
        "        variables for the empirical analysis and model.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): Cleaned and treated annual macro data.\n",
        "        fx_df (pd.DataFrame): Cleaned and treated daily FX data.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary of analysis-ready DataFrames,\n",
        "                                 including log-levels and growth rates.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(macro_df, pd.DataFrame) or not isinstance(fx_df, pd.DataFrame):\n",
        "        raise TypeError(\"Inputs must be pandas DataFrames.\")\n",
        "\n",
        "    # Initialize the dictionary to hold the final datasets.\n",
        "    analysis_ready_data = {}\n",
        "\n",
        "    # --- 1. Logarithmic Transformation ---\n",
        "    # Check for non-positive values before applying log.\n",
        "    if (macro_df <= 0).any().any() or (fx_df <= 0).any().any():\n",
        "        raise ValueError(\"Data contains non-positive values, cannot apply log transformation.\")\n",
        "\n",
        "    # Apply natural log to all relevant columns.\n",
        "    macro_df_log = np.log(macro_df)\n",
        "    fx_df_log = np.log(fx_df)\n",
        "\n",
        "    # --- 2. Growth and Return Calculation ---\n",
        "    # Calculate annual log growth rates for macro variables.\n",
        "    # Equation: growth_t = log(Value_t) - log(Value_{t-1})\n",
        "    macro_growth_cols = ['gdp_const_lcu', 'imports_const_lcu', 'exports_const_lcu']\n",
        "    macro_growth = macro_df_log[macro_growth_cols].groupby(level='country_iso').diff()\n",
        "    macro_growth.rename(columns=lambda c: f\"{c.replace('_const_lcu', '')}_growth\", inplace=True)\n",
        "\n",
        "    # Calculate daily log returns for FX rates.\n",
        "    # Equation: return_t = log(Rate_t) - log(Rate_{t-1})\n",
        "    fx_returns = fx_df_log[['fx_rate_usd']].groupby(level='country_iso').diff()\n",
        "    fx_returns.rename(columns={'fx_rate_usd': 'fx_return'}, inplace=True)\n",
        "\n",
        "    # Store all generated datasets in the output dictionary.\n",
        "    analysis_ready_data['macro_log_levels'] = macro_df_log\n",
        "    analysis_ready_data['fx_log_levels'] = fx_df_log\n",
        "    analysis_ready_data['macro_growth'] = macro_growth\n",
        "    analysis_ready_data['fx_returns'] = fx_returns\n",
        "\n",
        "    return analysis_ready_data\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 1 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_data_preparation_pipeline(\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire data validation, quality assessment, and preprocessing pipeline.\n",
        "\n",
        "    This master function serves as the entry point for Task 1. It executes all\n",
        "    sub-tasks in a logical sequence, passing data between them and aggregating\n",
        "    all reports and logs. It ensures that the data is rigorously validated and\n",
        "    cleaned before any analysis is performed.\n",
        "\n",
        "    Args:\n",
        "        raw_macro_df (pd.DataFrame): The raw annual macroeconomic data.\n",
        "        raw_fx_df (pd.DataFrame): The raw daily foreign exchange data.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]:\n",
        "            - A dictionary of analysis-ready DataFrames.\n",
        "            - A comprehensive dictionary containing all validation and quality reports.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Task 1: Data Validation and Preparation Pipeline ---\")\n",
        "\n",
        "    # --- Step 1: Input Validation (Task 1.1) ---\n",
        "    # Validate all inputs (config dictionary and DataFrame structures).\n",
        "    # This will raise a ValueError if critical errors are found.\n",
        "    print(\"Step 1.1: Validating inputs...\")\n",
        "    validation_report = validate_inputs(raw_macro_df, raw_fx_df, master_config)\n",
        "    print(\"Step 1.1: Validation successful.\")\n",
        "\n",
        "    # --- Step 2: Data Quality Assessment (Task 1.2) ---\n",
        "    # Assess data for outliers, missing patterns, and economic consistency.\n",
        "    print(\"\\nStep 1.2: Assessing data quality...\")\n",
        "    quality_report = assess_data_quality(raw_macro_df, raw_fx_df)\n",
        "    print(\"Step 1.2: Quality assessment complete.\")\n",
        "\n",
        "    # --- Step 3: Data Cleansing and Preprocessing (Task 1.3) ---\n",
        "    print(\"\\nStep 1.3: Starting data cleansing and preprocessing...\")\n",
        "\n",
        "    # Sub-step 1.3.1: Impute missing values.\n",
        "    print(\"  - Sub-step 1.3.1: Imputing missing values...\")\n",
        "    macro_imputed, fx_imputed, macro_mask, fx_mask = _impute_missing_values(\n",
        "        raw_macro_df, raw_fx_df\n",
        "    )\n",
        "\n",
        "    # Sub-step 1.3.2: Treat outliers.\n",
        "    print(\"  - Sub-step 1.3.2: Treating outliers...\")\n",
        "    macro_treated, fx_treated, treatment_log = _treat_outliers(\n",
        "        macro_imputed, fx_imputed, quality_report['statistical_outlier_detection']['flags']\n",
        "    )\n",
        "\n",
        "    # Sub-step 1.3.3: Transform and standardize variables.\n",
        "    print(\"  - Sub-step 1.3.3: Transforming variables to logs and growth rates...\")\n",
        "    analysis_ready_data = _transform_and_standardize_variables(\n",
        "        macro_treated, fx_treated\n",
        "    )\n",
        "    print(\"Step 1.3: Preprocessing complete.\")\n",
        "\n",
        "    # --- Step 4: Final Aggregation ---\n",
        "    # Combine all reports into a single master report object.\n",
        "    master_report = {\n",
        "        'initial_validation': validation_report,\n",
        "        'data_quality_assessment': quality_report,\n",
        "        'preprocessing_summary': {\n",
        "            'imputation_masks': {\n",
        "                'macro': macro_mask,\n",
        "                'fx': fx_mask\n",
        "            },\n",
        "            'treatment_log': treatment_log\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Task 1: Data Preparation Pipeline Completed Successfully ---\")\n",
        "\n",
        "    return analysis_ready_data, master_report\n"
      ],
      "metadata": {
        "id": "VjkLJ_tMn1-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task II: Empirical Stylized Facts Analysis\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 2.1.1: DAILY EXCHANGE RATE RETURN STATISTICS\n",
        "# =============================================================================\n",
        "\n",
        "def _analyze_fx_return_statistics(\n",
        "    fx_returns: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates key descriptive statistics for daily FX returns.\n",
        "\n",
        "    This function computes the first four statistical moments (mean, standard\n",
        "    deviation, skewness, kurtosis) for the daily log-returns of foreign\n",
        "    exchange rates for each country. It also calculates the annualized\n",
        "    volatility, a standard metric in financial analysis.\n",
        "\n",
        "    Args:\n",
        "        fx_returns (pd.DataFrame): A DataFrame with a MultiIndex ('country_iso', 'date')\n",
        "                                   and a column 'fx_return' containing daily log-returns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by country_iso, containing the\n",
        "                      calculated descriptive statistics for each country.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(fx_returns, pd.DataFrame) or 'fx_return' not in fx_returns.columns:\n",
        "        raise ValueError(\"Input must be a DataFrame with an 'fx_return' column.\")\n",
        "\n",
        "    # Drop missing values that result from the initial diff() operation.\n",
        "    returns_cleaned = fx_returns.dropna()\n",
        "\n",
        "    # Group by country to perform calculations on a per-country basis.\n",
        "    grouped = returns_cleaned['fx_return'].groupby('country_iso')\n",
        "\n",
        "    # Calculate the first four moments.\n",
        "    # Mean of daily returns.\n",
        "    mean = grouped.mean()\n",
        "    # Standard deviation of daily returns.\n",
        "    std_daily = grouped.std()\n",
        "    # Skewness of the distribution (asymmetry).\n",
        "    skewness = grouped.skew()\n",
        "    # Excess kurtosis (tails relative to normal dist). pandas.kurt() gives excess kurtosis.\n",
        "    excess_kurtosis = grouped.kurt()\n",
        "\n",
        "    # Calculate annualized volatility.\n",
        "    # Equation: vol_annual = std_daily * sqrt(252)\n",
        "    # 252 is the standard number of trading days in a year.\n",
        "    volatility_annualized = std_daily * np.sqrt(252)\n",
        "\n",
        "    # Assemble the results into a single DataFrame.\n",
        "    stats_df = pd.DataFrame({\n",
        "        'mean_daily_return': mean,\n",
        "        'std_daily_return': std_daily,\n",
        "        'volatility_annualized': volatility_annualized,\n",
        "        'skewness': skewness,\n",
        "        'excess_kurtosis': excess_kurtosis\n",
        "    })\n",
        "\n",
        "    return stats_df\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 2.1.2: ANDERSON-DARLING NORMALITY TESTING\n",
        "# =============================================================================\n",
        "\n",
        "def _perform_anderson_darling_test(\n",
        "    fx_returns: pd.DataFrame,\n",
        "    alpha_level: float = 0.05\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs the Anderson-Darling test for normality on daily FX returns.\n",
        "\n",
        "    This function tests the null hypothesis that the FX returns for each country\n",
        "    are drawn from a normal distribution. It compares the calculated test\n",
        "    statistic against the critical value at the specified significance level.\n",
        "\n",
        "    Args:\n",
        "        fx_returns (pd.DataFrame): A DataFrame of daily log-returns.\n",
        "        alpha_level (float): The significance level for the test (e.g., 0.05 for 5%).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by country_iso, reporting the test\n",
        "                      statistic, critical value, and a boolean decision on\n",
        "                      whether to reject the null hypothesis of normality.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(fx_returns, pd.DataFrame) or 'fx_return' not in fx_returns.columns:\n",
        "        raise ValueError(\"Input must be a DataFrame with an 'fx_return' column.\")\n",
        "    if not 0 < alpha_level < 1:\n",
        "        raise ValueError(\"alpha_level must be between 0 and 1.\")\n",
        "\n",
        "    # Drop missing values.\n",
        "    returns_cleaned = fx_returns.dropna()\n",
        "\n",
        "    # Initialize a list to store results for each country.\n",
        "    results = []\n",
        "\n",
        "    # Iterate over each country in the dataset.\n",
        "    for country, data in returns_cleaned.groupby('country_iso'):\n",
        "        # Perform the Anderson-Darling test for normality.\n",
        "        ad_result = stats.anderson(data['fx_return'], dist='norm')\n",
        "\n",
        "        # Find the critical value corresponding to the chosen alpha level.\n",
        "        # The result object contains significance levels and corresponding critical values.\n",
        "        significance_map = {\n",
        "            0.15: 0, 0.10: 1, 0.05: 2, 0.025: 3, 0.01: 4\n",
        "        }\n",
        "        if alpha_level not in significance_map:\n",
        "            raise ValueError(f\"Unsupported alpha_level. Choose from {list(significance_map.keys())}\")\n",
        "\n",
        "        # Get the index for the critical value array.\n",
        "        critical_value_index = significance_map[alpha_level]\n",
        "        # Extract the critical value.\n",
        "        critical_value = ad_result.critical_values[critical_value_index]\n",
        "\n",
        "        # The null hypothesis is rejected if the statistic is greater than the critical value.\n",
        "        reject_null = ad_result.statistic > critical_value\n",
        "\n",
        "        # Append the results for this country to our list.\n",
        "        results.append({\n",
        "            'country_iso': country,\n",
        "            'ad_statistic': ad_result.statistic,\n",
        "            f'critical_value_{int(alpha_level*100)}pct': critical_value,\n",
        "            f'reject_normality_at_{int(alpha_level*100)}pct': reject_null\n",
        "        })\n",
        "\n",
        "    # Convert the list of results into a DataFrame, indexed by country.\n",
        "    return pd.DataFrame(results).set_index('country_iso')\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 2.1.3: QQ PLOT CONSTRUCTION AND ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def _generate_qq_plot_data(\n",
        "    fx_returns: pd.DataFrame\n",
        ") -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Generates the data required to construct Quantile-Quantile (QQ) plots.\n",
        "\n",
        "    For each country, this function computes the sample quantiles from the FX\n",
        "    return data and the corresponding theoretical quantiles from a standard\n",
        "    normal distribution. This data is the basis for visually assessing\n",
        "    normality, particularly deviations in the tails.\n",
        "\n",
        "    Args:\n",
        "        fx_returns (pd.DataFrame): A DataFrame of daily log-returns.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, np.ndarray]]: A dictionary where each key is a\n",
        "                                          country_iso. The value is another\n",
        "                                          dictionary containing the\n",
        "                                          'sample_quantiles' and\n",
        "                                          'theoretical_quantiles' as numpy arrays.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(fx_returns, pd.DataFrame) or 'fx_return' not in fx_returns.columns:\n",
        "        raise ValueError(\"Input must be a DataFrame with an 'fx_return' column.\")\n",
        "\n",
        "    # Drop missing values.\n",
        "    returns_cleaned = fx_returns.dropna()\n",
        "\n",
        "    # Initialize a dictionary to store the plot data for all countries.\n",
        "    all_plot_data = {}\n",
        "\n",
        "    # Iterate over each country.\n",
        "    for country, data in returns_cleaned.groupby('country_iso'):\n",
        "        # Extract the return series for the country.\n",
        "        series = data['fx_return'].values\n",
        "\n",
        "        # 1. Compute Sample Quantiles: simply the sorted data points.\n",
        "        sample_quantiles = np.sort(series)\n",
        "\n",
        "        # 2. Compute Theoretical Quantiles.\n",
        "        # Create a uniform probability distribution from 0 to 1.\n",
        "        # The formula (i - 0.5) / n is a standard convention to avoid 0 and 1.\n",
        "        n = len(series)\n",
        "        probabilities = (np.arange(n) + 0.5) / n\n",
        "\n",
        "        # Use the inverse CDF (percent-point function) of the normal distribution.\n",
        "        theoretical_quantiles = stats.norm.ppf(probabilities)\n",
        "\n",
        "        # Store the results for the country.\n",
        "        all_plot_data[country] = {\n",
        "            'sample_quantiles': sample_quantiles,\n",
        "            'theoretical_quantiles': theoretical_quantiles\n",
        "        }\n",
        "\n",
        "    return all_plot_data\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 2.1 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_exchange_rate_properties(\n",
        "    analysis_ready_data: Dict[str, pd.DataFrame],\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the statistical analysis of exchange rate properties.\n",
        "\n",
        "    This master function executes the full workflow for Task 2.1, replicating\n",
        "    the empirical analysis of FX returns presented in the paper. It computes\n",
        "    descriptive statistics, performs normality tests, and generates data for\n",
        "    QQ plots.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_data (Dict[str, pd.DataFrame]): A dictionary containing\n",
        "            the preprocessed data, including the 'fx_returns' DataFrame.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary,\n",
        "            used to retrieve parameters like the alpha level for tests.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive report containing the results of all\n",
        "                        statistical analyses.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Task 2.1: Exchange Rate Statistical Properties Analysis ---\")\n",
        "\n",
        "    # --- Input Validation and Data Retrieval ---\n",
        "    if 'fx_returns' not in analysis_ready_data:\n",
        "        raise KeyError(\"The 'fx_returns' DataFrame is missing from analysis_ready_data.\")\n",
        "    fx_returns = analysis_ready_data['fx_returns']\n",
        "\n",
        "    # Retrieve the significance level from the master configuration.\n",
        "    alpha_level = master_config['empirical_analysis']['parameters']['statistical_tests']['alpha_level']\n",
        "\n",
        "    # --- Step 1: Calculate Descriptive Statistics ---\n",
        "    print(\"Step 2.1.1: Calculating descriptive statistics...\")\n",
        "    descriptive_stats = _analyze_fx_return_statistics(fx_returns)\n",
        "    print(\"Step 2.1.1: Calculation complete.\")\n",
        "\n",
        "    # --- Step 2: Perform Anderson-Darling Normality Test ---\n",
        "    print(\"\\nStep 2.1.2: Performing Anderson-Darling normality tests...\")\n",
        "    normality_test_results = _perform_anderson_darling_test(fx_returns, alpha_level)\n",
        "    print(\"Step 2.1.2: Testing complete.\")\n",
        "\n",
        "    # --- Step 3: Generate QQ Plot Data ---\n",
        "    print(\"\\nStep 2.1.3: Generating data for QQ plots...\")\n",
        "    qq_plot_data = _generate_qq_plot_data(fx_returns)\n",
        "    print(\"Step 2.1.3: Data generation complete.\")\n",
        "\n",
        "    # --- Step 4: Aggregate Results into a Final Report ---\n",
        "    report = {\n",
        "        'descriptive_statistics': descriptive_stats,\n",
        "        'normality_test': normality_test_results,\n",
        "        'qq_plot_data': qq_plot_data\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Task 2.1: Analysis Completed Successfully ---\")\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 2.2.1: HODRICK-PRESCOTT FILTERING IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def _apply_hp_filter(\n",
        "    log_levels_df: pd.DataFrame,\n",
        "    lambda_param: float\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies the Hodrick-Prescott (HP) filter to detrend time series.\n",
        "\n",
        "    This function decomposes each time series in the input DataFrame into a\n",
        "    trend and a cyclical component. It returns a DataFrame containing only the\n",
        "    smooth trend components, which are used as inputs for the state-space model.\n",
        "\n",
        "    Args:\n",
        "        log_levels_df (pd.DataFrame): DataFrame of log-transformed level data.\n",
        "        lambda_param (float): The smoothing parameter for the HP filter.\n",
        "                               The paper uses 1600 for annual data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with the same index as the input, containing\n",
        "                      the trend component for each column.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(log_levels_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'log_levels_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # Initialize a DataFrame to store the trend components.\n",
        "    trends_df = pd.DataFrame(index=log_levels_df.index)\n",
        "\n",
        "    # Apply the HP filter to each column on a per-country basis.\n",
        "    for col in log_levels_df.columns:\n",
        "        # The hpfilter function is applied to each country's series individually.\n",
        "        # Equation: min_{τ} Σ(y_t - τ_t)² + λ * Σ((τ_{t+1} - τ_t) - (τ_t - τ_{t-1}))²\n",
        "        trend_series = log_levels_df[col].groupby('country_iso').apply(\n",
        "            lambda x: hpfilter(x, lamb=lambda_param)[1] # [1] returns the trend\n",
        "        )\n",
        "        trends_df[f\"{col}_trend\"] = trend_series\n",
        "\n",
        "    return trends_df\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 2.2.2: BAYESIAN STATE-SPACE MODEL MCMC ESTIMATION\n",
        "# =============================================================================\n",
        "\n",
        "def _forward_filter_backward_sampler(\n",
        "    y: np.ndarray,\n",
        "    X: np.ndarray,\n",
        "    F: np.ndarray,\n",
        "    Q: float,\n",
        "    R: float,\n",
        "    pi_0: float,\n",
        "    P_0: float\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Implements the Carter and Kohn (1994) Forward-Filter, Backward-Sampler.\n",
        "\n",
        "    This algorithm is a cornerstone of Bayesian time-series analysis for sampling\n",
        "    the entire path of a latent state vector from its full conditional posterior\n",
        "    distribution in a linear Gaussian state-space model. It is a critical\n",
        "    component of the Gibbs sampler for the specified model.\n",
        "\n",
        "    The model structure is:\n",
        "    - Measurement Equation: y_t = X_t * pi_t + v_t,  v_t ~ N(0, R)\n",
        "    - State Equation:       pi_t = F * pi_{t-1} + w_t, w_t ~ N(0, Q)\n",
        "\n",
        "    Args:\n",
        "        y (np.ndarray): The observed data vector (T x 1), representing the\n",
        "                        dependent variable in the measurement equation.\n",
        "        X (np.ndarray): The design matrix for the state effect in the\n",
        "                        measurement equation (T x 1).\n",
        "        F (np.ndarray): The state transition matrix (1 x 1 for a random walk).\n",
        "        Q (float): The variance of the state equation error (a scalar).\n",
        "        R (float): The variance of the measurement equation error (a scalar).\n",
        "        pi_0 (float): The prior mean for the initial state pi_0.\n",
        "        P_0 (float): The prior variance for the initial state pi_0.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A numpy array of shape (T,) containing a single draw of the\n",
        "                    entire latent state vector pi_{1:T} from its full\n",
        "                    conditional posterior distribution.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure all inputs are of the correct numeric types.\n",
        "    if not all(isinstance(arg, (np.ndarray, float, int)) for arg in [y, X, F, Q, R, pi_0, P_0]):\n",
        "        raise TypeError(\"All inputs must be numpy arrays or numeric scalars.\")\n",
        "    # Get the number of time periods from the length of the observation vector.\n",
        "    T = len(y)\n",
        "\n",
        "    # --- 1. Forward Filter (Standard Kalman Filter) ---\n",
        "    # This pass iterates from t=1 to T, computing the filtered state estimates\n",
        "    # pi_{t|t} and their variances P_{t|t}, which are the distributions of the\n",
        "    # state at time t given information up to time t.\n",
        "\n",
        "    # Initialize arrays to store the filtered states and their variances.\n",
        "    pi_filtered = np.zeros(T)\n",
        "    P_filtered = np.zeros(T)\n",
        "\n",
        "    # Initialize the prediction of the state and variance at t=1 using prior info.\n",
        "    pi_pred, P_pred = pi_0, P_0\n",
        "\n",
        "    # Iterate through each time period to compute the filtered estimates.\n",
        "    for t in range(T):\n",
        "        # Calculate the prediction error (innovation).\n",
        "        # v_t = y_t - E[y_t | I_{t-1}] = y_t - X_t * pi_{t|t-1}\n",
        "        v = y[t] - X[t] @ pi_pred\n",
        "\n",
        "        # Calculate the variance of the prediction error.\n",
        "        # S_t = Var(v_t) = X_t * P_{t|t-1} * X_t' + R\n",
        "        S = X[t] @ P_pred @ X[t].T + R\n",
        "\n",
        "        # Calculate the Kalman Gain.\n",
        "        # K_t = P_{t|t-1} * X_t' * S_t^{-1}\n",
        "        K = (P_pred @ X[t].T) / S\n",
        "\n",
        "        # Update step: Incorporate the new information from y_t.\n",
        "        # pi_{t|t} = pi_{t|t-1} + K_t * v_t\n",
        "        pi_filtered[t] = pi_pred + K * v\n",
        "        # P_{t|t} = P_{t|t-1} - K_t * S_t * K_t'\n",
        "        P_filtered[t] = P_pred - K * S * K.T\n",
        "\n",
        "        # Prediction for the next step (t+1).\n",
        "        # pi_{t+1|t} = F * pi_{t|t}\n",
        "        pi_pred = F @ pi_filtered[t]\n",
        "        # P_{t+1|t} = F * P_{t|t} * F' + Q\n",
        "        P_pred = F @ P_filtered[t] @ F.T + Q\n",
        "\n",
        "    # --- 2. Backward Sampler ---\n",
        "    # This pass iterates backward from t=T to 1, drawing the smoothed states\n",
        "    # pi_t from p(pi_t | pi_{t+1}, I_T), which results in a draw from the\n",
        "    # full joint posterior p(pi_{1:T} | I_T).\n",
        "\n",
        "    # Initialize an array to store the final sampled (smoothed) states.\n",
        "    pi_smoothed = np.zeros(T)\n",
        "\n",
        "    # Draw the final state pi_T from its filtered distribution N(pi_{T|T}, P_{T|T}).\n",
        "    # This is the starting point for the backward recursion.\n",
        "    pi_smoothed[-1] = stats.norm.rvs(pi_filtered[-1], np.sqrt(P_filtered[-1]))\n",
        "\n",
        "    # Iterate backward from T-1 down to 0.\n",
        "    for t in range(T - 2, -1, -1):\n",
        "        # One-step-ahead prediction from the filtered state at time t.\n",
        "        pi_pred_t = F @ pi_filtered[t]\n",
        "        P_pred_t = F @ P_filtered[t] @ F.T + Q\n",
        "\n",
        "        # Calculate the smoothing gain J_t and the moments of the conditional distribution.\n",
        "        # J_t = P_{t|t} * F' * (P_{t+1|t})^{-1}\n",
        "        J = (P_filtered[t] @ F.T) / P_pred_t\n",
        "        # E[pi_t | pi_{t+1}, I_T] = pi_{t|t} + J_t * (pi_{t+1} - pi_{t+1|t})\n",
        "        pi_mean_smooth = pi_filtered[t] + J * (pi_smoothed[t+1] - pi_pred_t)\n",
        "        # Var(pi_t | pi_{t+1}, I_T) = P_{t|t} - J_t * P_{t+1|t} * J_t'\n",
        "        P_smooth = P_filtered[t] - J * P_pred_t * J.T\n",
        "\n",
        "        # Draw the smoothed state pi_t from its conditional normal distribution.\n",
        "        pi_smoothed[t] = stats.norm.rvs(pi_mean_smooth, np.sqrt(P_smooth))\n",
        "\n",
        "    # Return the single, complete path draw of the state vector.\n",
        "    return pi_smoothed\n",
        "\n",
        "# =============================================================================\n",
        "# MCMC WORKER FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def _run_mcmc_chain(\n",
        "    country_data: pd.DataFrame,\n",
        "    mcmc_config: Dict[str, Any]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Executes a single MCMC chain for the state-space model for one country.\n",
        "\n",
        "    This function implements a Gibbs sampler to draw from the posterior\n",
        "    distribution of the model parameters: the time-varying import elasticity\n",
        "    (pi_t), the price elasticity (eta), and the error variances.\n",
        "\n",
        "    Args:\n",
        "        country_data (pd.DataFrame): A DataFrame containing the necessary\n",
        "                                     time-series data for a single country.\n",
        "        mcmc_config (Dict[str, Any]): A dictionary containing MCMC settings\n",
        "                                      (iterations, burn-in) and prior specifications.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary where keys are parameter names and\n",
        "                               values are numpy arrays containing the posterior\n",
        "                               draws from this single chain.\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Data for Estimation ---\n",
        "    # Extract the observation vector y_t (HP-filtered imports).\n",
        "    # Measurement Equation: m_t^T = η*rer_t + π_t*y_t^T + ε_m\n",
        "    y = country_data['imports_const_lcu_trend'].values\n",
        "    # Extract the design matrix for the fixed coefficient eta (price elasticity).\n",
        "    X_eta = country_data[['reer_log_levels']].values\n",
        "    # Extract the design matrix for the time-varying coefficient pi_t (import elasticity).\n",
        "    X_pi = country_data[['gdp_const_lcu_trend']].values\n",
        "    # Get the number of time periods.\n",
        "    T = len(y)\n",
        "\n",
        "    # --- 2. Unpack Priors from Configuration ---\n",
        "    priors = mcmc_config['priors']\n",
        "    eta_prior_mu, eta_prior_var = priors['eta']['mu'], priors['eta']['sigma']**2\n",
        "    pi0_mu, pi0_var = priors['pi_0']['mu'], priors['pi_0']['sigma']**2\n",
        "    sm_alpha, sm_beta = priors['sigma_m']['alpha'], priors['sigma_m']['beta']\n",
        "    spi_alpha, spi_beta = priors['sigma_pi']['alpha'], priors['sigma_pi']['beta']\n",
        "\n",
        "    # --- 3. Initialize Parameters and State Vector ---\n",
        "    # Draw initial values for each parameter from its prior distribution.\n",
        "    eta = stats.norm.rvs(eta_prior_mu, np.sqrt(eta_prior_var))\n",
        "    sigma2_m = 1 / stats.gamma.rvs(sm_alpha, scale=1/sm_beta)\n",
        "    sigma2_pi = 1 / stats.gamma.rvs(spi_alpha, scale=1/spi_beta)\n",
        "    # Initialize the state vector at its prior mean.\n",
        "    pi_t = np.full(T, pi0_mu)\n",
        "\n",
        "    # --- 4. Set up MCMC Storage Arrays ---\n",
        "    total_iter = mcmc_config['mcmc_iterations']\n",
        "    burn_in = mcmc_config['burn_in_samples']\n",
        "    n_samples = total_iter - burn_in\n",
        "    eta_draws = np.zeros(n_samples)\n",
        "    sigma2_m_draws = np.zeros(n_samples)\n",
        "    sigma2_pi_draws = np.zeros(n_samples)\n",
        "    pi_t_draws = np.zeros((n_samples, T))\n",
        "\n",
        "    # --- 5. Main Gibbs Sampling Loop ---\n",
        "    for i in range(total_iter):\n",
        "        # --- Block 1: Sample the state vector pi_t using FFBS ---\n",
        "        # Adjust the observation vector by subtracting the effect of the fixed parameter.\n",
        "        y_star = y - (X_eta @ eta).flatten()\n",
        "        # Call the FFBS algorithm to get a new draw for the entire path of pi_t.\n",
        "        pi_t = _forward_filter_backward_sampler(\n",
        "            y=y_star, X=X_pi, F=np.array([[1.0]]), Q=sigma2_pi, R=sigma2_m,\n",
        "            pi_0=pi0_mu, P_0=pi0_var\n",
        "        )\n",
        "\n",
        "        # --- Block 2: Sample eta (price elasticity) from its Normal posterior ---\n",
        "        # Adjust the observation vector by subtracting the effect of the time-varying state.\n",
        "        y_tilde = y - (X_pi * pi_t[:, np.newaxis]).flatten()\n",
        "        # Calculate posterior variance for eta.\n",
        "        V_eta_inv = 1/eta_prior_var + (X_eta.T @ X_eta) / sigma2_m\n",
        "        V_eta = 1 / V_eta_inv\n",
        "        # Calculate posterior mean for eta.\n",
        "        m_eta = V_eta * (eta_prior_mu/eta_prior_var + (X_eta.T @ y_tilde) / sigma2_m)\n",
        "        # Draw eta from its full conditional posterior.\n",
        "        eta = stats.norm.rvs(m_eta.item(), np.sqrt(V_eta.item()))\n",
        "\n",
        "        # --- Block 3: Sample sigma2_m (measurement variance) from its Inverse-Gamma posterior ---\n",
        "        # Calculate the residuals from the measurement equation.\n",
        "        residuals_m = y - (X_eta @ eta).flatten() - (X_pi * pi_t[:, np.newaxis]).flatten()\n",
        "        # Calculate the posterior shape parameter for the Inverse-Gamma distribution.\n",
        "        sm_alpha_post = sm_alpha + T / 2\n",
        "        # Calculate the posterior scale parameter for the Inverse-Gamma distribution.\n",
        "        sm_beta_post = sm_beta + (residuals_m.T @ residuals_m).item() / 2\n",
        "        # Draw sigma2_m from its full conditional posterior.\n",
        "        sigma2_m = 1 / stats.gamma.rvs(sm_alpha_post, scale=1/sm_beta_post)\n",
        "\n",
        "        # --- Block 4: Sample sigma2_pi (state variance) from its Inverse-Gamma posterior ---\n",
        "        # Calculate the residuals from the state equation (a random walk).\n",
        "        residuals_pi = np.diff(pi_t, prepend=pi0_mu)\n",
        "        # Calculate the posterior shape parameter.\n",
        "        spi_alpha_post = spi_alpha + T / 2\n",
        "        # Calculate the posterior scale parameter.\n",
        "        spi_beta_post = spi_beta + (residuals_pi.T @ residuals_pi).item() / 2\n",
        "        # Draw sigma2_pi from its full conditional posterior.\n",
        "        sigma2_pi = 1 / stats.gamma.rvs(spi_alpha_post, scale=1/spi_beta_post)\n",
        "\n",
        "        # --- 6. Store Draws After Burn-in Period ---\n",
        "        if i >= burn_in:\n",
        "            # Calculate the storage index.\n",
        "            idx = i - burn_in\n",
        "            # Store the current draw for each parameter.\n",
        "            eta_draws[idx] = eta\n",
        "            sigma2_m_draws[idx] = sigma2_m\n",
        "            sigma2_pi_draws[idx] = sigma2_pi\n",
        "            pi_t_draws[idx, :] = pi_t\n",
        "\n",
        "    # Return a dictionary containing the full history of posterior draws for this chain.\n",
        "    return {\n",
        "        'eta': eta_draws, 'sigma2_m': sigma2_m_draws,\n",
        "        'sigma2_pi': sigma2_pi_draws, 'pi_t': pi_t_draws\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# MCMC PARALLEL COORDINATOR\n",
        "# =============================================================================\n",
        "\n",
        "def _estimate_trade_elasticity_mcmc(\n",
        "    country_data: pd.DataFrame,\n",
        "    mcmc_config: Dict[str, Any]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Manages the parallel execution of multiple MCMC chains for a single country.\n",
        "\n",
        "    This function serves as a coordinator, using the `joblib` library to run\n",
        "    several independent MCMC chains simultaneously. Running multiple chains is\n",
        "    essential for diagnosing convergence (e.g., via the Gelman-Rubin statistic).\n",
        "\n",
        "    Args:\n",
        "        country_data (pd.DataFrame): The time-series data for a single country.\n",
        "        mcmc_config (Dict[str, Any]): Configuration for the MCMC estimation.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary of aggregated posterior draws. Each\n",
        "                               value is a numpy array with an added first\n",
        "                               dimension representing the chain index, e.g.,\n",
        "                               'pi_t' will have shape (num_chains, num_samples, T).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if 'num_chains' not in mcmc_config or mcmc_config['num_chains'] < 2:\n",
        "        raise ValueError(\"MCMC config must specify at least 2 chains for convergence diagnostics.\")\n",
        "\n",
        "    # Retrieve the number of chains to run from the configuration.\n",
        "    num_chains = mcmc_config['num_chains']\n",
        "\n",
        "    # Use joblib's Parallel and delayed to execute _run_mcmc_chain for each chain.\n",
        "    # n_jobs=-1 uses all available CPU cores for maximum efficiency.\n",
        "    chain_results = Parallel(n_jobs=-1)(\n",
        "        delayed(_run_mcmc_chain)(country_data, mcmc_config) for _ in range(num_chains)\n",
        "    )\n",
        "\n",
        "    # Aggregate the results from the list of dictionaries into a single dictionary of numpy arrays.\n",
        "    # We stack the arrays along a new first axis (axis=0) to represent the chains.\n",
        "    aggregated_results = {\n",
        "        param: np.stack([res[param] for res in chain_results], axis=0)\n",
        "        for param in chain_results[0].keys()\n",
        "    }\n",
        "\n",
        "    # Return the combined results from all chains.\n",
        "    return aggregated_results\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 2.2 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def estimate_trade_multiplier(\n",
        "    analysis_ready_data: Dict[str, pd.DataFrame],\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the empirical estimation of the dynamic trade multiplier.\n",
        "\n",
        "    This master function executes the full workflow for Task 2.2:\n",
        "    1.  Applies the HP filter to detrend macroeconomic data.\n",
        "    2.  For each country, estimates the time-varying income elasticity of imports\n",
        "        (pi_t) using a Bayesian state-space model via MCMC.\n",
        "    3.  Calculates the dynamic trade multiplier from the posterior estimates.\n",
        "    4.  Computes convergence diagnostics (R-hat).\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_data (Dict[str, pd.DataFrame]): Preprocessed data dictionary.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive report containing posterior draws,\n",
        "                        convergence diagnostics, and the final trade multiplier\n",
        "                        estimates for each country.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Task 2.2: Trade Multiplier Empirical Estimation ---\")\n",
        "\n",
        "    # --- Step 1: Apply HP Filter ---\n",
        "    print(\"Step 2.2.1: Applying Hodrick-Prescott filter...\")\n",
        "    lambda_param = master_config['empirical_analysis']['parameters']['hp_filter']['lambda']\n",
        "    macro_log_levels = analysis_ready_data['macro_log_levels']\n",
        "    hp_trends = _apply_hp_filter(\n",
        "        macro_log_levels[['gdp_const_lcu', 'imports_const_lcu', 'exports_const_lcu']],\n",
        "        lambda_param\n",
        "    )\n",
        "    print(\"Step 2.2.1: HP filtering complete.\")\n",
        "\n",
        "    # --- Step 2: Estimate Model via MCMC for each country ---\n",
        "    print(\"\\nStep 2.2.2: Estimating state-space model via MCMC...\")\n",
        "    mcmc_config = master_config['empirical_analysis']['parameters']['bayesian_model']\n",
        "    estimation_results = {}\n",
        "\n",
        "    # Combine necessary data into one DataFrame for estimation.\n",
        "    estimation_data = hp_trends.join(macro_log_levels[['reer']])\n",
        "    estimation_data.rename(columns={'reer': 'reer_log_levels'}, inplace=True)\n",
        "\n",
        "    for country in estimation_data.index.get_level_values('country_iso').unique():\n",
        "        print(f\"  - Processing country: {country}\")\n",
        "        country_data = estimation_data.loc[country].dropna()\n",
        "\n",
        "        # Run the MCMC estimation.\n",
        "        posterior_draws = _estimate_trade_elasticity_mcmc(country_data, mcmc_config)\n",
        "\n",
        "        # --- Step 3: Compute Convergence Diagnostics (R-hat) ---\n",
        "        r_hats = {}\n",
        "        for param, draws in posterior_draws.items():\n",
        "            if draws.ndim > 1 and draws.shape[0] > 1: # Need multiple chains\n",
        "                # Gelman-Rubin diagnostic (R-hat)\n",
        "                # W = within-chain variance, B = between-chain variance\n",
        "                W = np.mean(np.var(draws, axis=1, ddof=1))\n",
        "                B = draws.shape[1] * np.var(np.mean(draws, axis=1), ddof=1)\n",
        "                var_plus = (draws.shape[1] - 1) / draws.shape[1] * W + B / draws.shape[1]\n",
        "                r_hats[param] = np.sqrt(var_plus / W) if W > 0 else 1.0\n",
        "\n",
        "        # --- Step 4: Compute Trade Multiplier ---\n",
        "        # Use posterior mean of pi_t for the point estimate.\n",
        "        pi_t_posterior_mean = posterior_draws['pi_t'].mean(axis=(0, 1))\n",
        "        pi_t_posterior_mean_series = pd.Series(pi_t_posterior_mean, index=country_data.index)\n",
        "\n",
        "        # Get HP-filtered export growth.\n",
        "        export_growth_trend = analysis_ready_data['macro_growth']['exports_growth'].loc[country]\n",
        "\n",
        "        # Equation: Δy_BP = Δz^T / π_t\n",
        "        trade_multiplier = export_growth_trend / pi_t_posterior_mean_series\n",
        "\n",
        "        estimation_results[country] = {\n",
        "            'posterior_draws': posterior_draws,\n",
        "            'convergence_diagnostics': r_hats,\n",
        "            'trade_multiplier_estimate': trade_multiplier.to_frame('trade_multiplier')\n",
        "        }\n",
        "\n",
        "    print(\"Step 2.2.2: MCMC estimation complete.\")\n",
        "    print(\"\\n--- Task 2.2: Estimation Completed Successfully ---\")\n",
        "\n",
        "    return estimation_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 2.3.1: TIME-VARYING TRADE MULTIPLIER COMPUTATION\n",
        "# =============================================================================\n",
        "\n",
        "def _compute_trade_multiplier_from_posterior(\n",
        "    estimation_results: Dict[str, Any],\n",
        "    analysis_ready_data: Dict[str, pd.DataFrame]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Computes the time-varying trade multiplier from posterior MCMC draws.\n",
        "\n",
        "    This function processes the output of the Bayesian estimation, propagating\n",
        "    the full uncertainty from the posterior distribution of the import elasticity\n",
        "    (pi_t) to the final trade multiplier estimate. It calculates the posterior\n",
        "    mean, median, and a 90% credible interval for the multiplier for each country.\n",
        "\n",
        "    Args:\n",
        "        estimation_results (Dict[str, Any]): The output dictionary from the\n",
        "            `estimate_trade_multiplier` function, containing posterior draws.\n",
        "        analysis_ready_data (Dict[str, pd.DataFrame]): The dictionary of\n",
        "            preprocessed data, required for the HP-filtered export growth series.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary where keys are country ISO codes\n",
        "                                 and values are DataFrames containing the\n",
        "                                 time-series of the trade multiplier estimates\n",
        "                                 and their credible intervals.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the final multiplier DataFrames.\n",
        "    trade_multipliers = {}\n",
        "\n",
        "    # Iterate through each country for which estimation was performed.\n",
        "    for country, results in estimation_results.items():\n",
        "        # --- 1. Extract and Process Posterior Draws for pi_t ---\n",
        "        # Get the posterior draws for the time-varying elasticity pi_t.\n",
        "        pi_t_draws = results['posterior_draws']['pi_t']\n",
        "        # The shape is (num_chains, num_samples, T). Reshape to (total_samples, T).\n",
        "        n_chains, n_samples, T = pi_t_draws.shape\n",
        "        pi_t_samples = pi_t_draws.reshape(n_chains * n_samples, T)\n",
        "\n",
        "        # --- 2. Prepare Export Growth Data ---\n",
        "        # Extract the corresponding HP-filtered export growth series.\n",
        "        # The index of the estimation data matches the length T of pi_t.\n",
        "        country_index = results['trade_multiplier_estimate'].index\n",
        "        export_growth_trend = analysis_ready_data['macro_growth']['exports_growth'].loc[country].loc[country_index]\n",
        "\n",
        "        # --- 3. Compute Trade Multiplier for Each Posterior Sample ---\n",
        "        # Robustness: Clip pi_t draws to a plausible economic range to avoid division by zero.\n",
        "        pi_t_samples_clipped = np.clip(pi_t_samples, 0.5, 5.0)\n",
        "\n",
        "        # Propagate uncertainty via vectorized calculation.\n",
        "        # Equation: Δy_BP(i) = Δz^T / π_t(i) for each posterior sample i.\n",
        "        # We use broadcasting to divide the (T,) export growth vector by the (N, T) pi_t matrix.\n",
        "        multiplier_samples = export_growth_trend.values / pi_t_samples_clipped\n",
        "\n",
        "        # --- 4. Summarize the Posterior Distribution of the Multiplier ---\n",
        "        # Calculate posterior mean, median, and 90% credible interval (5th and 95th percentiles).\n",
        "        mean_estimate = np.mean(multiplier_samples, axis=0)\n",
        "        median_estimate = np.median(multiplier_samples, axis=0)\n",
        "        lower_ci = np.percentile(multiplier_samples, 5, axis=0)\n",
        "        upper_ci = np.percentile(multiplier_samples, 95, axis=0)\n",
        "\n",
        "        # --- 5. Assemble the Results into a DataFrame ---\n",
        "        multiplier_df = pd.DataFrame({\n",
        "            'mean_estimate': mean_estimate,\n",
        "            'median_estimate': median_estimate,\n",
        "            'lower_ci_5pct': lower_ci,\n",
        "            'upper_ci_95pct': upper_ci\n",
        "        }, index=country_index)\n",
        "\n",
        "        # Store the final DataFrame for the country.\n",
        "        trade_multipliers[country] = multiplier_df\n",
        "\n",
        "    return trade_multipliers\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 2.3.2: GROWTH RATE CORRELATION ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def _analyze_growth_correlations(\n",
        "    actual_gdp_growth: pd.DataFrame,\n",
        "    estimated_multipliers: Dict[str, pd.DataFrame]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Analyzes the correlation between actual GDP growth and the estimated trade multiplier.\n",
        "\n",
        "    This function computes key performance metrics to evaluate how well the\n",
        "    trade multiplier tracks actual economic growth, including Pearson correlation,\n",
        "    RMSE, MAE, and a time-varying rolling correlation.\n",
        "\n",
        "    Args:\n",
        "        actual_gdp_growth (pd.DataFrame): DataFrame of actual annual GDP growth.\n",
        "        estimated_multipliers (Dict[str, pd.DataFrame]): Dictionary of trade\n",
        "            multiplier estimates from the previous step.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary where keys are country ISO codes\n",
        "                                   and values are dictionaries containing the\n",
        "                                   calculated performance metrics.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store correlation results.\n",
        "    correlation_results = {}\n",
        "\n",
        "    # Iterate through each country.\n",
        "    for country, multiplier_df in estimated_multipliers.items():\n",
        "        # Extract the actual GDP growth series for the country.\n",
        "        actual_growth_series = actual_gdp_growth.loc[country]['gdp_growth']\n",
        "        # Extract the mean estimate of the trade multiplier.\n",
        "        multiplier_series = multiplier_df['mean_estimate']\n",
        "\n",
        "        # --- Align Data ---\n",
        "        # Combine into a single DataFrame and drop NaNs to ensure perfect alignment.\n",
        "        combined = pd.concat([actual_growth_series, multiplier_series], axis=1).dropna()\n",
        "\n",
        "        if combined.empty:\n",
        "            continue\n",
        "\n",
        "        # --- Calculate Performance Metrics ---\n",
        "        # 1. Pearson Correlation Coefficient.\n",
        "        correlation = combined.iloc[:, 0].corr(combined.iloc[:, 1])\n",
        "\n",
        "        # 2. Root Mean Squared Error (RMSE).\n",
        "        rmse = np.sqrt(np.mean((combined.iloc[:, 0] - combined.iloc[:, 1])**2))\n",
        "\n",
        "        # 3. Mean Absolute Error (MAE).\n",
        "        mae = np.mean(np.abs(combined.iloc[:, 0] - combined.iloc[:, 1]))\n",
        "\n",
        "        # 4. Rolling Correlation (10-year window).\n",
        "        rolling_correlation = combined.iloc[:, 0].rolling(window=10).corr(combined.iloc[:, 1])\n",
        "\n",
        "        # Store all results for the country.\n",
        "        correlation_results[country] = {\n",
        "            'pearson_correlation': correlation,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'rolling_correlation': rolling_correlation\n",
        "        }\n",
        "\n",
        "    return correlation_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 2.3.3: GROWTH DEVIATION DISTRIBUTIONAL ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def _analyze_growth_deviations(\n",
        "    actual_gdp_growth: pd.DataFrame,\n",
        "    estimated_multipliers: Dict[str, pd.DataFrame],\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Analyzes the statistical properties of the deviation between actual growth and the multiplier.\n",
        "\n",
        "    This function calculates the deviation series (actual growth - multiplier)\n",
        "    and then performs the same set of distributional analyses as in Task 2.1:\n",
        "    descriptive statistics, Anderson-Darling test, and QQ plot data generation.\n",
        "    This replicates the analysis shown in the bottom row of Figure 2 in the paper.\n",
        "\n",
        "    Args:\n",
        "        actual_gdp_growth (pd.DataFrame): DataFrame of actual annual GDP growth.\n",
        "        estimated_multipliers (Dict[str, pd.DataFrame]): Dictionary of trade\n",
        "            multiplier estimates.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary by country, containing reports on\n",
        "                                   the distributional properties of the deviation series.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the analysis results.\n",
        "    deviation_analysis = {}\n",
        "\n",
        "    # Retrieve the significance level for the normality test.\n",
        "    alpha_level = master_config['empirical_analysis']['parameters']['statistical_tests']['alpha_level']\n",
        "\n",
        "    # Iterate through each country.\n",
        "    for country, multiplier_df in estimated_multipliers.items():\n",
        "        # --- 1. Calculate the Deviation Series ---\n",
        "        actual_growth_series = actual_gdp_growth.loc[country]['gdp_growth']\n",
        "        multiplier_series = multiplier_df['mean_estimate']\n",
        "        # Combine and align the series.\n",
        "        combined = pd.concat([actual_growth_series, multiplier_series], axis=1).dropna()\n",
        "        # Calculate the deviation.\n",
        "        deviation_series = (combined.iloc[:, 0] - combined.iloc[:, 1]).to_frame(name='deviation')\n",
        "\n",
        "        if deviation_series.empty:\n",
        "            continue\n",
        "\n",
        "        # --- 2. Perform Distributional Analysis (reusing logic from Task 2.1) ---\n",
        "        # Note: We are applying the same functions to a different input series.\n",
        "\n",
        "        # Calculate descriptive statistics for the deviation series.\n",
        "        descriptive_stats = _analyze_fx_return_statistics(deviation_series.rename(columns={'deviation': 'fx_return'}))\n",
        "\n",
        "        # Perform the Anderson-Darling test on the deviation series.\n",
        "        normality_test = _perform_anderson_darling_test(deviation_series.rename(columns={'deviation': 'fx_return'}), alpha_level)\n",
        "\n",
        "        # Generate QQ plot data for the deviation series.\n",
        "        qq_data = _generate_qq_plot_data(deviation_series.rename(columns={'deviation': 'fx_return'}))\n",
        "\n",
        "        # Store the results for the country.\n",
        "        deviation_analysis[country] = {\n",
        "            'descriptive_statistics': descriptive_stats,\n",
        "            'normality_test': normality_test,\n",
        "            'qq_plot_data': qq_data[country] # _generate_qq_plot_data returns a nested dict\n",
        "        }\n",
        "\n",
        "    return deviation_analysis\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 2 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_empirical_analysis(\n",
        "    analysis_ready_data: Dict[str, pd.DataFrame],\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire empirical analysis pipeline (Task 2).\n",
        "\n",
        "    This master function executes all sub-tasks required to generate the\n",
        "    empirical stylized facts that motivate the paper's theoretical model. It\n",
        "    combines the analysis of FX returns (Task 2.1) with the estimation and\n",
        "    validation of the dynamic trade multiplier (Tasks 2.2 and 2.3).\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_data (Dict[str, pd.DataFrame]): The dictionary of\n",
        "            preprocessed, analysis-ready data from Task 1.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all results from\n",
        "                        the empirical analysis phase.\n",
        "    \"\"\"\n",
        "    print(\"========== Starting Task 2: Empirical Stylized Facts Analysis ==========\")\n",
        "\n",
        "    # --- Task 2.1: Analyze Exchange Rate Properties ---\n",
        "    fx_properties_report = analyze_exchange_rate_properties(analysis_ready_data, master_config)\n",
        "\n",
        "    # --- Task 2.2: Estimate the Trade Multiplier ---\n",
        "    estimation_results = estimate_trade_multiplier(analysis_ready_data, master_config)\n",
        "\n",
        "    # --- Task 2.3: Validate and Analyze the Trade Multiplier ---\n",
        "    print(\"\\n--- Starting Task 2.3: Trade Multiplier Validation and Analysis ---\")\n",
        "\n",
        "    # Sub-step 2.3.1: Compute multiplier with uncertainty.\n",
        "    print(\"Step 2.3.1: Computing trade multiplier with credible intervals...\")\n",
        "    trade_multiplier_final_estimates = _compute_trade_multiplier_from_posterior(\n",
        "        estimation_results, analysis_ready_data\n",
        "    )\n",
        "    print(\"Step 2.3.1: Computation complete.\")\n",
        "\n",
        "    # Sub-step 2.3.2: Analyze correlation with actual growth.\n",
        "    print(\"\\nStep 2.3.2: Analyzing correlation with actual GDP growth...\")\n",
        "    correlation_report = _analyze_growth_correlations(\n",
        "        analysis_ready_data['macro_growth'], trade_multiplier_final_estimates\n",
        "    )\n",
        "    print(\"Step 2.3.2: Correlation analysis complete.\")\n",
        "\n",
        "    # Sub-step 2.3.3: Analyze distributional properties of deviations.\n",
        "    print(\"\\nStep 2.3.3: Analyzing distribution of growth deviations...\")\n",
        "    deviation_report = _analyze_growth_deviations(\n",
        "        analysis_ready_data['macro_growth'], trade_multiplier_final_estimates, master_config\n",
        "    )\n",
        "    print(\"Step 2.3.3: Deviation analysis complete.\")\n",
        "\n",
        "    print(\"--- Task 2.3: Validation and Analysis Completed Successfully ---\")\n",
        "\n",
        "    # --- Final Aggregation ---\n",
        "    master_empirical_report = {\n",
        "        'fx_rate_properties': fx_properties_report,\n",
        "        'trade_multiplier_estimation': estimation_results,\n",
        "        'trade_multiplier_validation': {\n",
        "            'final_estimates_with_ci': trade_multiplier_final_estimates,\n",
        "            'correlation_analysis': correlation_report,\n",
        "            'deviation_distribution_analysis': deviation_report\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n========== Task 2: Empirical Analysis Pipeline Completed Successfully ==========\")\n",
        "\n",
        "    return master_empirical_report\n"
      ],
      "metadata": {
        "id": "0lNTh3YtrUbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK III: THEORETICAL MODEL IMPLEMENTATION\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 3.1.1: EXPECTATIONS FORMATION IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def _update_expectations(\n",
        "    delta_y_t_minus_1: float,\n",
        "    params: Dict[str, Any],\n",
        "    rng: Optional[np.random.Generator] = None\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the expected fundamental exchange rate based on past performance.\n",
        "\n",
        "    This function implements Equation (28) from the paper. The expected\n",
        "    fundamental exchange rate is a function of a baseline (PPP, assumed to be 0),\n",
        "    an adjustment for past economic growth, and an optional stochastic shock.\n",
        "    Stronger past growth leads to an expectation of currency appreciation.\n",
        "\n",
        "    Equation (28): E[f_t] = -Ω * Δy_{t-1} + ε_t\n",
        "\n",
        "    Args:\n",
        "        delta_y_t_minus_1 (float): The output growth rate from the previous period.\n",
        "        params (Dict[str, Any]): A dictionary of model parameters, must contain\n",
        "                                 'Omega' and 'sigma_epsilon'.\n",
        "        rng (Optional[np.random.Generator]): A numpy random number generator instance\n",
        "                                             for reproducible stochastic simulations.\n",
        "                                             If None, the deterministic component\n",
        "                                             only is calculated.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated expected fundamental exchange rate, E[f_t].\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not all(k in params for k in ['Omega', 'sigma_epsilon']):\n",
        "        raise KeyError(\"Parameters must include 'Omega' and 'sigma_epsilon'.\")\n",
        "\n",
        "    # Calculate the deterministic component of the expectation.\n",
        "    # A higher past growth rate (delta_y_t_minus_1) leads to a lower (more appreciated)\n",
        "    # expected fundamental rate.\n",
        "    deterministic_component = -params['Omega'] * delta_y_t_minus_1\n",
        "\n",
        "    # Initialize the stochastic shock to zero.\n",
        "    stochastic_shock = 0.0\n",
        "\n",
        "    # If a random number generator is provided and sigma is positive, add a shock.\n",
        "    # ε_t ~ N(0, σ_epsilon^2)\n",
        "    if rng is not None and params['sigma_epsilon'] > 0:\n",
        "        # Draw a random shock from a normal distribution.\n",
        "        stochastic_shock = rng.normal(loc=0.0, scale=params['sigma_epsilon'])\n",
        "\n",
        "    # The final expectation is the sum of the two components.\n",
        "    expected_fundamental_rate = deterministic_component + stochastic_shock\n",
        "\n",
        "    return expected_fundamental_rate\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 3.1.2: BEHAVIORAL STRATEGIES & MARKET CLEARING\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_market_clearing_growth(\n",
        "    e_t_minus_1: float,\n",
        "    e_t_minus_2: float,\n",
        "    expected_f_t: float,\n",
        "    params: Dict[str, Any]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the market-clearing output growth rate (Δy_MC).\n",
        "\n",
        "    This function implements the core market-clearing mechanism from Equation (17).\n",
        "    It first computes the net speculative demand based on the weighted average of\n",
        "    fundamentalist, chartist, and trend-extrapolator heuristics. It then uses this\n",
        "    to find the output growth rate that balances the FX market.\n",
        "\n",
        "    Equation (17): Δy_t^{MC} = (Δz^{NS}/π) - ((1-θ)/θπ)(μ+ρ)[Speculative Demand]\n",
        "\n",
        "    Args:\n",
        "        e_t_minus_1 (float): The exchange rate from the previous period (t-1).\n",
        "        e_t_minus_2 (float): The exchange rate from two periods ago (t-2),\n",
        "                             required for the trend-extrapolator strategy.\n",
        "        expected_f_t (float): The expected fundamental exchange rate for the current period.\n",
        "        params (Dict[str, Any]): A dictionary of model parameters.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated market-clearing output growth rate, Δy_t^{MC}.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_params = ['mu', 'rho', 'w_F', 'w_C', 'w_E', 'pi', 'theta', 'delta_z_ns']\n",
        "    if not all(k in params for k in required_params):\n",
        "        raise KeyError(f\"Parameters must include all of {required_params}.\")\n",
        "\n",
        "    # --- 1. Calculate Net Speculative Demand ---\n",
        "    # Calculate the deviation of the exchange rate from its expected fundamental.\n",
        "    deviation = expected_f_t - e_t_minus_1\n",
        "\n",
        "    # Fundamentalist demand component.\n",
        "    # Heuristic (Eq. 6): μ * (E[f_t] - e_{t-1})^3\n",
        "    # Numerical Stability: Clip the deviation before cubing to prevent overflow.\n",
        "    clipped_deviation = np.clip(deviation, -5.0, 5.0)\n",
        "    fundamentalist_demand = params['w_F'] * (clipped_deviation**3)\n",
        "\n",
        "    # Chartist demand component.\n",
        "    # Heuristic (Eq. 7): μ * (e_{t-1} - E[f_t]) = -μ * deviation\n",
        "    chartist_demand = params['w_C'] * (-deviation)\n",
        "\n",
        "    # Trend-extrapolator demand component.\n",
        "    # Heuristic (page 19): μ * (e_{t-1} - e_{t-2})\n",
        "    trend_extrapolator_demand = params['w_E'] * (e_t_minus_1 - e_t_minus_2)\n",
        "\n",
        "    # Combine the components to get the aggregate speculative demand term.\n",
        "    # This term is the weighted sum inside the brackets of Eq (17) and (31), scaled by (μ+ρ).\n",
        "    net_speculative_flow = (params['mu'] + params['rho']) * (\n",
        "        fundamentalist_demand + chartist_demand + trend_extrapolator_demand\n",
        "    )\n",
        "\n",
        "    # --- 2. Calculate Market-Clearing Growth Rate ---\n",
        "    # Calculate the balance-of-payments constrained growth rate (Thirlwall's Law).\n",
        "    # This is the growth rate when speculative demand is zero.\n",
        "    # Δy_BP = Δz^{NS} / π\n",
        "    delta_y_bp = params['delta_z_ns'] / params['pi']\n",
        "\n",
        "    # Calculate the gamma parameter, which scales the impact of speculative flows.\n",
        "    # γ' = (1-θ)(μ+ρ)/(θπ) -- Note: this is different from γ in Eq (29)\n",
        "    # The term in Eq (17) is (1-θ)/θπ * (μ+ρ) * [Spec Demand / (μ+ρ)]\n",
        "    gamma_scaler = (1 - params['theta']) / (params['theta'] * params['pi'])\n",
        "\n",
        "    # Apply the full Equation (17).\n",
        "    market_clearing_growth = delta_y_bp - gamma_scaler * net_speculative_flow\n",
        "\n",
        "    return market_clearing_growth\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 3.1 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def implement_core_dynamics() -> Dict[str, callable]:\n",
        "    \"\"\"\n",
        "    Provides the core computational functions of the theoretical model.\n",
        "\n",
        "    This function serves as an orchestrator for Task 3.1. It does not execute\n",
        "    a workflow but rather returns a dictionary of the essential, validated,\n",
        "    and production-grade functions that form the building blocks of the\n",
        "    dynamic system. This modular approach allows the main simulation engine\n",
        "    (Task 3.2) to be constructed cleanly from these tested components.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, callable]: A dictionary containing the core model functions:\n",
        "                             - 'update_expectations': Function to calculate E[f_t].\n",
        "                             - 'calculate_market_clearing_growth': Function for Δy_MC.\n",
        "    \"\"\"\n",
        "    print(\"--- Task 3.1: Core Dynamic System Implementation ---\")\n",
        "    print(\"Component functions `_update_expectations` and `_calculate_market_clearing_growth` are defined and ready.\")\n",
        "\n",
        "    # Return the functions themselves for use by other parts of the simulation pipeline.\n",
        "    core_functions = {\n",
        "        'update_expectations': _update_expectations,\n",
        "        'calculate_market_clearing_growth': _calculate_market_clearing_growth\n",
        "    }\n",
        "\n",
        "    print(\"--- Task 3.1: Completed Successfully ---\")\n",
        "\n",
        "    return core_functions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 3.2.1 & 3.2.2: UNIFIED DYNAMIC SYSTEM ITERATOR\n",
        "# =============================================================================\n",
        "\n",
        "def _iterate_dynamic_system(\n",
        "    current_state: Union[Tuple[float, float], Tuple[float, float, float]],\n",
        "    params: Dict[str, Any],\n",
        "    core_funcs: Dict[str, callable],\n",
        "    rng: Optional[np.random.Generator] = None\n",
        ") -> Union[Tuple[float, float], Tuple[float, float, float]]:\n",
        "    \"\"\"\n",
        "    Executes a single time step of the dynamic system.\n",
        "\n",
        "    This function is the core iterator for the simulation. It takes the current\n",
        "    state of the system and the model parameters, and computes the state for the\n",
        "    next time step by implementing the model's full system of difference equations.\n",
        "    It dynamically handles both the baseline (2-variable state) and the extended\n",
        "    model with trend-extrapolators (3-variable state).\n",
        "\n",
        "    Args:\n",
        "        current_state (Union[Tuple[float, float], Tuple[float, float, float]]):\n",
        "            The current state of the system.\n",
        "            - For baseline model: (e_{t-1}, Δy_{t-1})\n",
        "            - For extended model: (e_{t-1}, e_{t-2}, Δy_{t-1})\n",
        "        params (Dict[str, Any]): A dictionary of model parameters.\n",
        "        core_funcs (Dict[str, callable]): A dictionary containing the core\n",
        "            component functions from Task 3.1.\n",
        "        rng (Optional[np.random.Generator]): A numpy random number generator\n",
        "            for stochastic simulations.\n",
        "\n",
        "    Returns:\n",
        "        Union[Tuple[float, float], Tuple[float, float, float]]: The new state\n",
        "            of the system for the next time step (e_t, Δy_t) or\n",
        "            (e_t, e_{t-1}, Δy_t).\n",
        "    \"\"\"\n",
        "    # --- 1. Unpack State and Parameters ---\n",
        "    # Check if we are using the extended model with trend-extrapolators.\n",
        "    is_extended_model = params.get('w_E', 0.0) > 0\n",
        "\n",
        "    # Unpack the state variables based on the model type.\n",
        "    if is_extended_model:\n",
        "        # State for the second-order system with trend-extrapolators.\n",
        "        e_t_minus_1, e_t_minus_2, delta_y_t_minus_1 = current_state\n",
        "    else:\n",
        "        # State for the baseline first-order system.\n",
        "        e_t_minus_1, delta_y_t_minus_1 = current_state\n",
        "        # Set e_{t-2} to a neutral value (e.g., e_{t-1}) as it's not used.\n",
        "        e_t_minus_2 = e_t_minus_1\n",
        "\n",
        "    # --- 2. Execute Model Logic using Core Functions ---\n",
        "    # Step A: Update expectations based on past growth.\n",
        "    # Implements Equation (28): E[f_t] = -Ω * Δy_{t-1} + ε_t\n",
        "    expected_f_t = core_funcs['update_expectations'](\n",
        "        delta_y_t_minus_1, params, rng\n",
        "    )\n",
        "\n",
        "    # Step B: Calculate the market-clearing growth rate.\n",
        "    # Implements Equation (17) and its extension.\n",
        "    delta_y_mc_t = core_funcs['calculate_market_clearing_growth'](\n",
        "        e_t_minus_1, e_t_minus_2, expected_f_t, params\n",
        "    )\n",
        "\n",
        "    # --- 3. Update State Variables ---\n",
        "    # Step C: Update the output growth rate.\n",
        "    # Implements Equation (27): Δy_t = Δy_{t-1} + w_flex*β*(Δy_MC - Δy_{t-1})\n",
        "    delta_y_t = delta_y_t_minus_1 + params['w_flex'] * params['beta'] * (\n",
        "        delta_y_mc_t - delta_y_t_minus_1\n",
        "    )\n",
        "\n",
        "    # Step D: Update the exchange rate.\n",
        "    # Re-calculate the speculative flow term for clarity and accuracy.\n",
        "    deviation = expected_f_t - e_t_minus_1\n",
        "    clipped_deviation = np.clip(deviation, -5.0, 5.0)\n",
        "    fundamentalist_demand = params['w_F'] * (clipped_deviation**3)\n",
        "    chartist_demand = params['w_C'] * (-deviation)\n",
        "    trend_extrapolator_demand = params['w_E'] * (e_t_minus_1 - e_t_minus_2)\n",
        "    net_speculative_flow = (params['mu'] + params['rho']) * (\n",
        "        fundamentalist_demand + chartist_demand + trend_extrapolator_demand\n",
        "    )\n",
        "    # Implements Equation (21) and its extension: e_t = e_{t-1} + Net Speculative Flow\n",
        "    e_t = e_t_minus_1 + net_speculative_flow\n",
        "\n",
        "    # --- 4. Pack and Return New State ---\n",
        "    # Return the new state tuple in the correct format for the next iteration.\n",
        "    if is_extended_model:\n",
        "        return (e_t, e_t_minus_1, delta_y_t)\n",
        "    else:\n",
        "        return (e_t, delta_y_t)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 3.2.3: SIMULATION ENGINE\n",
        "# =============================================================================\n",
        "\n",
        "def run_simulation(\n",
        "    initial_state: Union[Tuple[float, float], Tuple[float, float, float]],\n",
        "    params: Dict[str, Any],\n",
        "    core_funcs: Dict[str, callable],\n",
        "    num_iterations: int,\n",
        "    transient_period: int,\n",
        "    seed: Optional[int] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs a full simulation of the dynamic system.\n",
        "\n",
        "    This function serves as the main engine for simulating the model over time.\n",
        "    It initializes the system, iterates it for a specified number of steps,\n",
        "    handles both deterministic and stochastic runs, and includes robust\n",
        "    detection for simulation divergence.\n",
        "\n",
        "    Args:\n",
        "        initial_state (Union[Tuple[float, float], Tuple[float, float, float]]):\n",
        "            The starting state of the system.\n",
        "        params (Dict[str, Any]): A dictionary of model parameters.\n",
        "        core_funcs (Dict[str, callable]): Dictionary of core model functions.\n",
        "        num_iterations (int): The total number of iterations to run.\n",
        "        transient_period (int): The number of initial iterations to discard\n",
        "                                to allow the system to settle on its attractor.\n",
        "        seed (Optional[int]): A seed for the random number generator to ensure\n",
        "                              reproducibility of stochastic simulations.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the simulation results, including:\n",
        "                        - 'trajectory': A numpy array of the state variables over time.\n",
        "                        - 'status': A string indicating if the simulation 'completed'\n",
        "                          or 'diverged'.\n",
        "                        - 'final_state': The final state tuple of the system.\n",
        "    \"\"\"\n",
        "    # --- 1. Initialization ---\n",
        "    # Validate inputs.\n",
        "    if not isinstance(num_iterations, int) or not isinstance(transient_period, int):\n",
        "        raise TypeError(\"Iteration counts must be integers.\")\n",
        "    if transient_period >= num_iterations:\n",
        "        raise ValueError(\"Transient period must be less than total iterations.\")\n",
        "\n",
        "    # Determine if the model is the extended version.\n",
        "    is_extended_model = params.get('w_E', 0.0) > 0\n",
        "    num_vars = 3 if is_extended_model else 2\n",
        "\n",
        "    # Initialize the random number generator for reproducibility.\n",
        "    rng = np.random.default_rng(seed) if seed is not None else None\n",
        "\n",
        "    # Pre-allocate numpy arrays to store the full trajectory for efficiency.\n",
        "    trajectory = np.zeros((num_iterations, num_vars))\n",
        "    # Set the initial state in the trajectory array.\n",
        "    trajectory[0, :] = initial_state\n",
        "\n",
        "    # --- 2. Simulation Loop ---\n",
        "    # Set the current state to the initial state.\n",
        "    current_state = initial_state\n",
        "    # Initialize the status flag.\n",
        "    status = 'completed'\n",
        "\n",
        "    # Iterate for the specified number of time steps.\n",
        "    for t in range(1, num_iterations):\n",
        "        # Call the iterator function to compute the next state.\n",
        "        current_state = _iterate_dynamic_system(current_state, params, core_funcs, rng)\n",
        "\n",
        "        # Store the new state in the trajectory array.\n",
        "        trajectory[t, :] = current_state[:num_vars]\n",
        "\n",
        "        # --- 3. Divergence Detection ---\n",
        "        # Check if any state variable has grown to an implausibly large value.\n",
        "        if np.any(np.abs(trajectory[t, :]) > 100.0):\n",
        "            # If so, flag the simulation as diverged and break the loop.\n",
        "            status = 'diverged'\n",
        "            break\n",
        "\n",
        "    # --- 4. Finalize and Return Results ---\n",
        "    # Discard the transient period from the start of the trajectory.\n",
        "    final_trajectory = trajectory[transient_period:, :]\n",
        "\n",
        "    # Prepare the final output dictionary.\n",
        "    results = {\n",
        "        'trajectory': final_trajectory,\n",
        "        'status': status,\n",
        "        'final_state': current_state,\n",
        "        'params': params,\n",
        "        'settings': {\n",
        "            'num_iterations': num_iterations,\n",
        "            'transient_period': transient_period,\n",
        "            'seed': seed\n",
        "        }\n",
        "    }\n",
        "    return results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 3.2 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def implement_evolution_engine() -> Dict[str, callable]:\n",
        "    \"\"\"\n",
        "    Provides the core functions for simulating the dynamic system.\n",
        "\n",
        "    This function orchestrates Task 3.2 by returning a dictionary of the\n",
        "    essential, production-grade functions that comprise the simulation engine.\n",
        "    This modular approach allows higher-level tasks (like bifurcation analysis)\n",
        "    to use these tested components directly.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, callable]: A dictionary containing the core simulation functions:\n",
        "                             - 'iterate_system': The single-step iterator.\n",
        "                             - 'run_simulation': The full simulation engine.\n",
        "    \"\"\"\n",
        "    print(\"--- Task 3.2: Dynamic System Evolution Engine ---\")\n",
        "    print(\"Component functions `_iterate_dynamic_system` and `run_simulation` are defined and ready.\")\n",
        "\n",
        "    # Return the functions for use by other parts of the pipeline.\n",
        "    engine_functions = {\n",
        "        'iterate_system': _iterate_dynamic_system,\n",
        "        'run_simulation': run_simulation\n",
        "    }\n",
        "\n",
        "    print(\"--- Task 3.2: Completed Successfully ---\")\n",
        "\n",
        "    return engine_functions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 3.3.1: FIXED POINT CALCULATION\n",
        "# =============================================================================\n",
        "\n",
        "def _find_equilibria(params: Dict[str, Any]) -> List[Tuple[float, float]]:\n",
        "    \"\"\"\n",
        "    Analytically calculates the equilibrium points (fixed points) of the system.\n",
        "\n",
        "    This function implements the solutions from Propositions 1 and 2 of the paper.\n",
        "    It determines the steady-state values for the exchange rate (e_bar) and the\n",
        "    output growth rate (delta_y_bar) based on the model's parameters.\n",
        "\n",
        "    Args:\n",
        "        params (Dict[str, Any]): A dictionary of model parameters.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[float, float]]: A list of tuples, where each tuple represents\n",
        "                                   the (e_bar, delta_y_bar) coordinates of an\n",
        "                                   equilibrium point.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_params = ['delta_z_ns', 'pi', 'Omega', 'w_C', 'w_F']\n",
        "    if not all(k in params for k in required_params):\n",
        "        raise KeyError(f\"Parameters must include all of {required_params}.\")\n",
        "\n",
        "    # Initialize a list to store the found equilibria.\n",
        "    equilibria = []\n",
        "\n",
        "    # --- 1. Calculate Steady-State Growth Rate (Δȳ) ---\n",
        "    # In any steady state, growth equals the dynamic trade multiplier.\n",
        "    # Equation: Δȳ = Δy_BP = Δz^{NS} / π\n",
        "    delta_y_bar = params['delta_z_ns'] / params['pi']\n",
        "\n",
        "    # --- 2. Calculate Steady-State Exchange Rate(s) (ē) ---\n",
        "    # The number and location of equilibria depend on the presence of chartists.\n",
        "\n",
        "    # Equilibrium P1 (Fundamental Equilibrium) always exists.\n",
        "    # Equation: ē₁ = -Ω * Δȳ\n",
        "    e_bar_1 = -params['Omega'] * delta_y_bar\n",
        "    equilibria.append((e_bar_1, delta_y_bar))\n",
        "\n",
        "    # Equilibria P2 and P3 (Chartist-driven) exist only if w_C > 0 and w_F > 0.\n",
        "    if params['w_C'] > 1e-9 and params['w_F'] > 1e-9:\n",
        "        # Equation: ē₂,₃ = -Ω * Δȳ ± sqrt(w_C / w_F)\n",
        "        deviation_term = np.sqrt(params['w_C'] / params['w_F'])\n",
        "\n",
        "        # Calculate the overvalued equilibrium exchange rate.\n",
        "        e_bar_2 = e_bar_1 + deviation_term\n",
        "        equilibria.append((e_bar_2, delta_y_bar))\n",
        "\n",
        "        # Calculate the undervalued equilibrium exchange rate.\n",
        "        e_bar_3 = e_bar_1 - deviation_term\n",
        "        equilibria.append((e_bar_3, delta_y_bar))\n",
        "\n",
        "    return equilibria\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 3.3.2: JACOBIAN MATRIX AND STABILITY ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def _analyze_local_stability(\n",
        "    equilibrium: Tuple[float, float],\n",
        "    params: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyzes the local stability of a given equilibrium point.\n",
        "\n",
        "    This function constructs the Jacobian matrix of the 2D dynamic system at a\n",
        "    specific equilibrium point and analyzes its eigenvalues to determine local\n",
        "    stability. It implements the stability conditions derived in Appendix B.\n",
        "\n",
        "    Args:\n",
        "        equilibrium (Tuple[float, float]): The (e_bar, delta_y_bar) coordinates\n",
        "                                           of the equilibrium point.\n",
        "        params (Dict[str, Any]): A dictionary of model parameters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the stability analysis, including\n",
        "                        the Jacobian matrix, its eigenvalues, and a stability\n",
        "                        classification.\n",
        "    \"\"\"\n",
        "    # --- 1. Unpack Inputs ---\n",
        "    e_bar, delta_y_bar = equilibrium\n",
        "\n",
        "    # Pre-calculate the gamma term from the growth equation for convenience.\n",
        "    # γ = (1-θ)(μ+ρ)/(θπ) from System (29)\n",
        "    gamma = (1 - params['theta']) * (params['mu'] + params['rho']) / (params['theta'] * params['pi'])\n",
        "\n",
        "    # --- 2. Calculate the Squared Deviation Term ---\n",
        "    # This term, (-ē - ΩΔȳ)², is central to the Jacobian elements.\n",
        "    squared_deviation = (e_bar + params['Omega'] * delta_y_bar)**2\n",
        "\n",
        "    # --- 3. Construct the Jacobian Matrix J = [[j11, j12], [j21, j22]] ---\n",
        "    # Element j11: ∂e_t / ∂e_{t-1}\n",
        "    # j11 = 1 + (μ+ρ) * [-3*w_F*(-ē-ΩΔȳ)² + w_C]\n",
        "    j11 = 1 + (params['mu'] + params['rho']) * (-3 * params['w_F'] * squared_deviation + params['w_C'])\n",
        "\n",
        "    # Element j12: ∂e_t / ∂Δy_{t-1}\n",
        "    # j12 = (μ+ρ) * [-3*w_F*Ω*(-ē-ΩΔȳ)² + w_C*Ω]\n",
        "    j12 = (params['mu'] + params['rho']) * params['Omega'] * (-3 * params['w_F'] * squared_deviation + params['w_C'])\n",
        "\n",
        "    # Element j21: ∂Δy_t / ∂e_{t-1}\n",
        "    # j21 = w_flex*β * {-γ*[-3*w_F*(-ē-ΩΔȳ)² + w_C]}\n",
        "    j21 = params['w_flex'] * params['beta'] * (-gamma * (-3 * params['w_F'] * squared_deviation + params['w_C']))\n",
        "\n",
        "    # Element j22: ∂Δy_t / ∂Δy_{t-1}\n",
        "    # j22 = 1 + w_flex*β * {-γ*[-3*w_F*Ω*(-ē-ΩΔȳ)² + w_C*Ω] - 1}\n",
        "    j22 = 1 + params['w_flex'] * params['beta'] * (\n",
        "        -gamma * params['Omega'] * (-3 * params['w_F'] * squared_deviation + params['w_C']) - 1\n",
        "    )\n",
        "\n",
        "    # Assemble the Jacobian matrix.\n",
        "    jacobian_matrix = np.array([[j11, j12], [j21, j22]])\n",
        "\n",
        "    # --- 4. Eigenvalue Analysis ---\n",
        "    # Calculate the eigenvalues of the Jacobian.\n",
        "    eigenvalues = np.linalg.eigvals(jacobian_matrix)\n",
        "    # Calculate the modulus (absolute value) of the eigenvalues.\n",
        "    eigenvalue_moduli = np.abs(eigenvalues)\n",
        "\n",
        "    # --- 5. Determine Stability and Classify ---\n",
        "    # Stability requires all eigenvalue moduli to be less than 1.\n",
        "    is_stable = np.all(eigenvalue_moduli < 1.0)\n",
        "\n",
        "    # Classify the equilibrium point based on the eigenvalues.\n",
        "    if is_stable:\n",
        "        stability_type = \"Stable\"\n",
        "    else:\n",
        "        # Check for saddle point (one eigenvalue inside, one outside the unit circle).\n",
        "        if np.any(eigenvalue_moduli < 1.0) and np.any(eigenvalue_moduli > 1.0):\n",
        "            stability_type = \"Saddle Point\"\n",
        "        # Check for instability indicating a potential Flip bifurcation.\n",
        "        elif np.any(np.real(eigenvalues) < -1.0):\n",
        "            stability_type = \"Unstable (Flip Bifurcation Possible)\"\n",
        "        # Check for instability indicating a potential Neimark-Sacker bifurcation.\n",
        "        elif np.any(eigenvalue_moduli > 1.0) and np.all(np.iscomplex(eigenvalues)):\n",
        "             stability_type = \"Unstable (Neimark-Sacker Bifurcation Possible)\"\n",
        "        else:\n",
        "            stability_type = \"Unstable\"\n",
        "\n",
        "    return {\n",
        "        'equilibrium_point': equilibrium,\n",
        "        'jacobian_matrix': jacobian_matrix,\n",
        "        'eigenvalues': eigenvalues,\n",
        "        'is_stable': is_stable,\n",
        "        'stability_type': stability_type\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 3.3 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_equilibria_and_stability(params: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full equilibrium and stability analysis for the model.\n",
        "\n",
        "    This function first finds all analytical equilibrium points for a given set\n",
        "    of parameters and then performs a local stability analysis on each point by\n",
        "    computing the Jacobian and its eigenvalues.\n",
        "\n",
        "    Args:\n",
        "        params (Dict[str, Any]): A dictionary of model parameters.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: A list of dictionaries, where each dictionary is a\n",
        "                              comprehensive stability report for one equilibrium point.\n",
        "    \"\"\"\n",
        "    print(\"--- Task 3.3: Equilibrium Analysis and Stability Assessment ---\")\n",
        "\n",
        "    # --- Step 1: Find all equilibrium points ---\n",
        "    print(\"Step 3.3.1: Finding equilibrium points...\")\n",
        "    equilibria = _find_equilibria(params)\n",
        "    print(f\"Found {len(equilibria)} equilibrium point(s).\")\n",
        "\n",
        "    # --- Step 2: Analyze stability for each equilibrium ---\n",
        "    print(\"\\nStep 3.3.2: Analyzing local stability of each point...\")\n",
        "    stability_reports = []\n",
        "    for eq in equilibria:\n",
        "        # Perform the stability analysis for the current equilibrium.\n",
        "        report = _analyze_local_stability(eq, params)\n",
        "        stability_reports.append(report)\n",
        "        print(f\"  - Equilibrium at e_bar={eq[0]:.4f} is: {report['stability_type']}\")\n",
        "\n",
        "    print(\"--- Task 3.3: Completed Successfully ---\")\n",
        "    return stability_reports\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 3 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def implement_theoretical_model() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the implementation of the entire theoretical model.\n",
        "\n",
        "    This master function aggregates the component functions from all sub-tasks\n",
        "    of Task 3, providing a complete, self-contained toolkit for building,\n",
        "    simulating, and analyzing the theoretical model.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the core functions and\n",
        "                        orchestrators for the theoretical model.\n",
        "    \"\"\"\n",
        "    print(\"========== Starting Task 3: Theoretical Model Implementation ==========\")\n",
        "\n",
        "    # --- Task 3.1: Get Core Dynamic Component Functions ---\n",
        "    core_dynamics_funcs = implement_core_dynamics()\n",
        "\n",
        "    # --- Task 3.2: Get Dynamic System Evolution Engine Functions ---\n",
        "    evolution_engine_funcs = implement_evolution_engine()\n",
        "\n",
        "    # --- Task 3.3: Get Equilibrium and Stability Analysis Orchestrator ---\n",
        "    # This task's orchestrator is the primary function to be returned.\n",
        "\n",
        "    # --- Final Aggregation ---\n",
        "    model_toolkit = {\n",
        "        'core_dynamics': core_dynamics_funcs,\n",
        "        'evolution_engine': evolution_engine_funcs,\n",
        "        'stability_analysis': {\n",
        "            'find_equilibria': _find_equilibria,\n",
        "            'analyze_local_stability': _analyze_local_stability,\n",
        "            'run_full_analysis': analyze_equilibria_and_stability\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n========== Task 3: Theoretical Model Toolkit Assembled Successfully ==========\")\n",
        "    return model_toolkit\n"
      ],
      "metadata": {
        "id": "7-Uwe4v7wnZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK IV: NUMERICAL DYNAMICS ANALYSIS\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 4.1.1: PARAMETER SWEEP ENGINE\n",
        "# =============================================================================\n",
        "\n",
        "def _run_single_simulation_for_sweep(\n",
        "    param_value: float,\n",
        "    bifurcation_config: Dict[str, Any],\n",
        "    fixed_params: Dict[str, Any],\n",
        "    global_settings: Dict[str, Any],\n",
        "    core_funcs: Dict[str, Callable],\n",
        "    run_simulation_func: Callable[..., Dict[str, Any]]\n",
        ") -> Tuple[float, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Executes a single simulation run for a specific parameter value in a bifurcation sweep.\n",
        "\n",
        "    This function is a high-fidelity worker designed for parallel execution. It takes a\n",
        "    single value of the bifurcation parameter, dynamically constructs the full\n",
        "    parameter set for the model (handling interdependencies like trader weights),\n",
        "    determines the correct initial state, runs a complete simulation via the\n",
        "    provided engine, and returns the asymptotic trajectory. Its primary role is to\n",
        "    encapsulate one unit of work within the larger, computationally intensive\n",
        "    bifurcation analysis.\n",
        "\n",
        "    Args:\n",
        "        param_value (float): The specific value of the bifurcation parameter for this run.\n",
        "        bifurcation_config (Dict[str, Any]): Configuration for the sweep, defining\n",
        "            the name of the parameter being varied (e.g., 'mu', 'w_F').\n",
        "        fixed_params (Dict[str, Any]): A dictionary of model parameters that are\n",
        "            held constant during the sweep.\n",
        "        global_settings (Dict[str, Any]): Global simulation settings, including\n",
        "            initial conditions and iteration counts.\n",
        "        core_funcs (Dict[str, Callable]): A dictionary containing the core model\n",
        "            component functions ('update_expectations', etc.).\n",
        "        run_simulation_func (Callable[..., Dict[str, Any]]): The main simulation\n",
        "            engine function (e.g., `run_simulation` from Task 3.2).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, np.ndarray]: A tuple containing:\n",
        "            - The parameter value used for this simulation.\n",
        "            - A numpy array representing the asymptotic trajectory of the system's\n",
        "              state variables after the transient period has been discarded.\n",
        "    \"\"\"\n",
        "    # --- 1. Configure Parameters for the Current Run ---\n",
        "    # Create a mutable copy of the fixed parameters to avoid side effects.\n",
        "    current_params = fixed_params.copy()\n",
        "\n",
        "    # Get the name of the parameter being varied in this bifurcation analysis.\n",
        "    bifurcation_param_name = bifurcation_config['name']\n",
        "\n",
        "    # Set the specific value for the bifurcation parameter for this simulation instance.\n",
        "    current_params[bifurcation_param_name] = param_value\n",
        "\n",
        "    # --- 2. Handle Dependent Parameter Constraints ---\n",
        "    # This block ensures that critical model constraints, like the sum of trader\n",
        "    # weights equaling 1, are maintained even as one weight is varied.\n",
        "    if bifurcation_param_name == 'w_F':\n",
        "        # If the fundamentalist weight is the bifurcation parameter, dynamically calculate the chartist weight.\n",
        "        # Equation: w_C = 1.0 - w_F - w_E\n",
        "        current_params['w_C'] = 1.0 - param_value - current_params.get('w_E', 0.0)\n",
        "    elif bifurcation_param_name == 'w_E':\n",
        "        # If the extrapolator weight is the bifurcation parameter, dynamically calculate the chartist weight.\n",
        "        # Equation: w_C = 1.0 - w_F - w_E\n",
        "        current_params['w_C'] = 1.0 - current_params.get('w_F', 0.0) - param_value\n",
        "\n",
        "    # --- 3. Set Initial Conditions Based on Model Type ---\n",
        "    # Determine if the current parameter set corresponds to the extended model (with trend-extrapolators).\n",
        "    is_extended_model = current_params.get('w_E', 0.0) > 0\n",
        "\n",
        "    # Select the appropriate initial state tuple based on the model's order.\n",
        "    # The extended model is a second-order system and requires an additional lagged state variable (e_{t-2}).\n",
        "    initial_state = (\n",
        "        global_settings['initial_conditions']['e0'],\n",
        "        global_settings['initial_conditions']['e_neg1'],\n",
        "        global_settings['initial_conditions']['delta_y0']\n",
        "    ) if is_extended_model else (\n",
        "        global_settings['initial_conditions']['e0'],\n",
        "        global_settings['initial_conditions']['delta_y0']\n",
        "    )\n",
        "\n",
        "    # --- 4. Execute the Simulation ---\n",
        "    # Call the main simulation engine with the fully specified configuration for this run.\n",
        "    # The number of iterations is the sum of the transient period and the period to be plotted/analyzed.\n",
        "    sim_results = run_simulation_func(\n",
        "        initial_state=initial_state,\n",
        "        params=current_params,\n",
        "        core_funcs=core_funcs,\n",
        "        num_iterations=global_settings['transient_iterations'] + global_settings['plot_iterations'],\n",
        "        transient_period=global_settings['transient_iterations']\n",
        "    )\n",
        "\n",
        "    # --- 5. Return Results ---\n",
        "    # Return the bifurcation parameter value and the resulting asymptotic trajectory.\n",
        "    # The trajectory is the portion of the simulation after the transient has been discarded,\n",
        "    # representing the system's long-run behavior (attractor).\n",
        "    return param_value, sim_results['trajectory']\n",
        "\n",
        "def _run_bifurcation_sweep(\n",
        "    bifurcation_config: Dict[str, Any],\n",
        "    fixed_params: Dict[str, Any],\n",
        "    global_settings: Dict[str, Any],\n",
        "    core_funcs: Dict[str, callable],\n",
        "    run_simulation_func: callable\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a parameter sweep to generate data for a bifurcation diagram.\n",
        "\n",
        "    This function systematically varies a single parameter across a specified\n",
        "    range, running a full simulation for each value. It leverages parallel\n",
        "    processing to perform this computationally intensive task efficiently.\n",
        "\n",
        "    Args:\n",
        "        bifurcation_config (Dict[str, Any]): Configuration for the sweep,\n",
        "            including the parameter name, range, and number of steps.\n",
        "        fixed_params (Dict[str, Any]): A dictionary of model parameters that\n",
        "            are held constant during the sweep.\n",
        "        global_settings (Dict[str, Any]): Global simulation settings.\n",
        "        core_funcs (Dict[str, callable]): Core model logic functions.\n",
        "        run_simulation_func (callable): The main simulation engine function.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the raw results of the sweep,\n",
        "                        including the parameter values and the corresponding\n",
        "                        asymptotic state trajectories.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup the Parameter Grid ---\n",
        "    param_name = bifurcation_config['name']\n",
        "    param_range = bifurcation_config['range']\n",
        "    param_steps = bifurcation_config['steps']\n",
        "    # Create a linearly spaced array of parameter values to sweep over.\n",
        "    param_values = np.linspace(param_range[0], param_range[1], param_steps)\n",
        "\n",
        "    # --- 2. Run Simulations in Parallel ---\n",
        "    # Use joblib.Parallel to distribute the simulations across all available CPU cores.\n",
        "    # The `tqdm` wrapper provides a progress bar for monitoring.\n",
        "    print(f\"Running bifurcation sweep for parameter '{param_name}'...\")\n",
        "    results = Parallel(n_jobs=-1)(\n",
        "        delayed(_run_single_simulation_for_sweep)(\n",
        "            val, bifurcation_config, fixed_params, global_settings, core_funcs, run_simulation_func\n",
        "        ) for val in tqdm(param_values, desc=f\"Sweeping {param_name}\")\n",
        "    )\n",
        "\n",
        "    # --- 3. Collate Results ---\n",
        "    # Unpack the results from the parallel execution.\n",
        "    # The results are a list of tuples, so we sort them by parameter value to ensure order.\n",
        "    sorted_results = sorted(results, key=lambda x: x[0])\n",
        "\n",
        "    # Separate the parameter values and the trajectories.\n",
        "    param_values_out = [res[0] for res in sorted_results]\n",
        "    asymptotic_trajectories = [res[1] for res in sorted_results]\n",
        "\n",
        "    return {\n",
        "        'bifurcation_param_name': param_name,\n",
        "        'bifurcation_param_values': np.array(param_values_out),\n",
        "        'asymptotic_trajectories': asymptotic_trajectories\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 4.1.3: BIFURCATION DIAGRAM VISUALIZATION PREPARATION\n",
        "# =============================================================================\n",
        "\n",
        "def _prepare_bifurcation_plot_data(\n",
        "    sweep_results: Dict[str, Any],\n",
        "    fixed_params: Dict[str, Any],\n",
        "    find_equilibria_func: callable\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Prepares the raw bifurcation sweep data for plotting.\n",
        "\n",
        "    This function transforms the list of trajectories from the sweep into a\n",
        "    format suitable for a scatter plot. It unpacks the state variables,\n",
        "    creates corresponding x and y coordinates, and assigns colors to distinguish\n",
        "    between different attractors based on their position relative to the\n",
        "    fundamental equilibrium.\n",
        "\n",
        "    Args:\n",
        "        sweep_results (Dict[str, Any]): The raw output from the sweep function.\n",
        "        fixed_params (Dict[str, Any]): The fixed parameters used in the sweep,\n",
        "            needed to calculate the fundamental equilibrium for coloring.\n",
        "        find_equilibria_func (callable): The function to find equilibrium points.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing plottable numpy arrays for\n",
        "                        x-coordinates, y-coordinates (for each state variable),\n",
        "                        and color information.\n",
        "    \"\"\"\n",
        "    # --- 1. Unpack Sweep Results ---\n",
        "    param_values = sweep_results['bifurcation_param_values']\n",
        "    trajectories = sweep_results['asymptotic_trajectories']\n",
        "\n",
        "    # --- 2. Prepare Data Structures for Plotting ---\n",
        "    x_coords, y_coords_e, y_coords_dy, colors = [], [], [], []\n",
        "\n",
        "    # --- 3. Process Each Simulation Result ---\n",
        "    for i, param_val in enumerate(param_values):\n",
        "        # Get the trajectory for this parameter value.\n",
        "        traj = trajectories[i]\n",
        "        # Extract the exchange rate (e) and growth rate (Δy) columns.\n",
        "        e_values = traj[:, 0]\n",
        "        dy_values = traj[:, -1] # Always the last column\n",
        "\n",
        "        # --- 4. Determine Color based on Equilibrium Position ---\n",
        "        # Calculate the fundamental equilibrium for the current parameter set.\n",
        "        current_params = fixed_params.copy()\n",
        "        current_params[sweep_results['bifurcation_param_name']] = param_val\n",
        "        equilibria = find_equilibria_func(current_params)\n",
        "        # The fundamental equilibrium is always the first one found.\n",
        "        e_fundamental = equilibria[0][0]\n",
        "\n",
        "        # Assign colors: 'C0' (blue) for attractors above fundamental, 'C1' (orange) for below.\n",
        "        # This replicates the coloring scheme in the paper's Figure 4.\n",
        "        color_val = ['C0' if e > e_fundamental else 'C1' for e in e_values]\n",
        "\n",
        "        # --- 5. Append Data for Plotting ---\n",
        "        # For each point in the asymptotic trajectory, add its coordinates and color.\n",
        "        x_coords.extend([param_val] * len(e_values))\n",
        "        y_coords_e.extend(e_values)\n",
        "        y_coords_dy.extend(dy_values)\n",
        "        colors.extend(color_val)\n",
        "\n",
        "    return {\n",
        "        'x_coords': np.array(x_coords),\n",
        "        'y_coords_e': np.array(y_coords_e),\n",
        "        'y_coords_dy': np.array(y_coords_dy),\n",
        "        'colors': np.array(colors),\n",
        "        'param_name': sweep_results['bifurcation_param_name']\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 4.1 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_bifurcation_analysis(\n",
        "    scenario_name: str,\n",
        "    master_config: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full bifurcation analysis for a given scenario.\n",
        "\n",
        "    This function manages the end-to-end process of generating the data for a\n",
        "    bifurcation diagram. It retrieves the scenario configuration, runs the\n",
        "    computationally intensive parameter sweep, and then prepares the data in a\n",
        "    format ready for visualization.\n",
        "\n",
        "    Args:\n",
        "        scenario_name (str): The name of the figure scenario to run (e.g., 'fig4a').\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the prepared plot data and the\n",
        "                        raw sweep results for auditability.\n",
        "    \"\"\"\n",
        "    print(f\"========== Starting Task 4.1: Bifurcation Analysis for {scenario_name} ==========\")\n",
        "\n",
        "    # --- 1. Retrieve Configuration and Functions ---\n",
        "    scenario_config = master_config['theoretical_simulation']['figure_scenarios'][scenario_name]\n",
        "    global_settings = master_config['theoretical_simulation']['global_settings']\n",
        "\n",
        "    # Extract required functions from the model toolkit.\n",
        "    core_funcs = model_toolkit['core_dynamics']\n",
        "    run_simulation_func = model_toolkit['evolution_engine']['run_simulation']\n",
        "    find_equilibria_func = model_toolkit['stability_analysis']['find_equilibria']\n",
        "\n",
        "    # --- 2. Run the Parameter Sweep ---\n",
        "    raw_sweep_results = _run_bifurcation_sweep(\n",
        "        bifurcation_config=scenario_config['bifurcation_param'],\n",
        "        fixed_params=scenario_config['fixed_params'],\n",
        "        global_settings=global_settings,\n",
        "        core_funcs=core_funcs,\n",
        "        run_simulation_func=run_simulation_func\n",
        "    )\n",
        "\n",
        "    # --- 3. Prepare Data for Visualization ---\n",
        "    print(\"\\nPreparing sweep results for plotting...\")\n",
        "    plot_data = _prepare_bifurcation_plot_data(\n",
        "        sweep_results=raw_sweep_results,\n",
        "        fixed_params=scenario_config['fixed_params'],\n",
        "        find_equilibria_func=find_equilibria_func\n",
        "    )\n",
        "    print(\"Data preparation complete.\")\n",
        "\n",
        "    # --- 4. Assemble Final Report ---\n",
        "    final_report = {\n",
        "        'plot_data': plot_data,\n",
        "        'raw_sweep_results': raw_sweep_results,\n",
        "        'scenario_config': scenario_config\n",
        "    }\n",
        "\n",
        "    print(f\"\\n========== Task 4.1: Bifurcation Analysis for {scenario_name} Completed Successfully ==========\")\n",
        "\n",
        "    return final_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 4.2.1: INITIAL CONDITION GRID GENERATION\n",
        "# =============================================================================\n",
        "\n",
        "def _generate_initial_condition_grid(\n",
        "    basin_config: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, np.ndarray, List[Tuple[float, ...]]]:\n",
        "    \"\"\"\n",
        "    Generates a 2D grid of initial conditions for basin of attraction analysis.\n",
        "\n",
        "    This function creates a grid of starting points (e_0, Δy_0) spanning a\n",
        "    specified region of the phase space. It returns both the meshgrid arrays\n",
        "    for easy plotting and a flattened list of coordinate tuples for efficient\n",
        "    parallel processing.\n",
        "\n",
        "    Args:\n",
        "        basin_config (Dict[str, Any]): A dictionary containing the configuration\n",
        "            for the grid, including 'e0_range', 'delta_y0_range', and 'resolution'.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray, List[Tuple[float, ...]]]:\n",
        "            - e0_grid: A 2D numpy array of initial exchange rate values.\n",
        "            - delta_y0_grid: A 2D numpy array of initial growth rate values.\n",
        "            - initial_conditions: A flattened list of (e0, delta_y0) tuples.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_keys = ['e0_range', 'delta_y0_range', 'resolution']\n",
        "    if not all(k in basin_config for k in required_keys):\n",
        "        raise KeyError(f\"Basin config must include all of {required_keys}.\")\n",
        "\n",
        "    # --- 1. Create Linearly Spaced Axes ---\n",
        "    # Create the x-axis (initial exchange rate, e0) values.\n",
        "    e0_values = np.linspace(basin_config['e0_range'][0], basin_config['e0_range'][1], basin_config['resolution'][0])\n",
        "    # Create the y-axis (initial growth rate, Δy0) values.\n",
        "    delta_y0_values = np.linspace(basin_config['delta_y0_range'][0], basin_config['delta_y0_range'][1], basin_config['resolution'][1])\n",
        "\n",
        "    # --- 2. Create the 2D Meshgrid ---\n",
        "    # `meshgrid` creates coordinate matrices from coordinate vectors.\n",
        "    e0_grid, delta_y0_grid = np.meshgrid(e0_values, delta_y0_values)\n",
        "\n",
        "    # --- 3. Flatten the Grid for Iteration ---\n",
        "    # Create a list of (e0, delta_y0) tuples for every point on the grid.\n",
        "    # This format is ideal for mapping to a parallel processing pool.\n",
        "    initial_conditions = list(zip(e0_grid.flatten(), delta_y0_grid.flatten()))\n",
        "\n",
        "    return e0_grid, delta_y0_grid, initial_conditions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 4.2.2: BASIN CLASSIFICATION ENGINE\n",
        "# =============================================================================\n",
        "\n",
        "def _classify_single_point(\n",
        "    initial_condition: Tuple[float, float],\n",
        "    params: Dict[str, Any],\n",
        "    basin_config: Dict[str, Any],\n",
        "    stable_equilibria: List[Tuple[float, float]],\n",
        "    core_funcs: Dict[str, Callable],\n",
        "    run_simulation_func: Callable[..., Dict[str, Any]]\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Worker function to simulate and classify a single initial condition's fate.\n",
        "\n",
        "    This function is the core computational unit for the basin of attraction\n",
        "    analysis. It takes a single starting point in the phase space, runs a\n",
        "    simulation until it terminates (either by converging, diverging, or hitting\n",
        "    the maximum iteration limit), and then classifies the outcome based on its\n",
        "    proximity to known stable equilibria. It is designed for efficient,\n",
        "    independent execution in a parallel processing environment.\n",
        "\n",
        "    Args:\n",
        "        initial_condition (Tuple[float, float]): A tuple (e_0, Δy_0) representing\n",
        "            the starting point for the simulation.\n",
        "        params (Dict[str, Any]): The dictionary of fixed model parameters for the simulation.\n",
        "        basin_config (Dict[str, Any]): Configuration for the basin analysis,\n",
        "            containing 'max_iterations' and 'convergence_tolerance'.\n",
        "        stable_equilibria (List[Tuple[float, float]]): A list of pre-calculated\n",
        "            stable equilibrium points (attractors) to check convergence against.\n",
        "        core_funcs (Dict[str, Callable]): A dictionary of the core model logic functions.\n",
        "        run_simulation_func (Callable[..., Dict[str, Any]]): The main simulation engine.\n",
        "\n",
        "    Returns:\n",
        "        int: An integer code representing the classification of the initial condition:\n",
        "             - 0: The trajectory diverged to infinity.\n",
        "             - 1: Converged to the first stable equilibrium in the list.\n",
        "             - 2: Converged to the second stable equilibrium in the list.\n",
        "             - ... and so on for other stable equilibria.\n",
        "             - -1: The trajectory did not diverge but did not converge to any\n",
        "                   known stable equilibrium within the tolerance (may indicate a\n",
        "                   chaotic or periodic attractor).\n",
        "    \"\"\"\n",
        "    # --- 1. Construct the Full Initial State ---\n",
        "    # Check if the model is the extended version (with trend-extrapolators)\n",
        "    # based on the presence and value of the 'w_E' parameter.\n",
        "    is_extended_model = params.get('w_E', 0.0) > 0\n",
        "\n",
        "    # The extended model is a second-order system and requires an additional\n",
        "    # lagged state variable, e_{t-2}. We construct the appropriate state tuple.\n",
        "    if is_extended_model:\n",
        "        # For the extended model, the state is (e_{t-1}, e_{t-2}, Δy_{t-1}).\n",
        "        # We initialize e_{t-2} to be the same as e_{t-1} for this analysis.\n",
        "        initial_state = (initial_condition[0], initial_condition[0], initial_condition[1])\n",
        "    else:\n",
        "        # For the baseline model, the state is simply (e_{t-1}, Δy_{t-1}).\n",
        "        initial_state = initial_condition\n",
        "\n",
        "    # --- 2. Run the Simulation ---\n",
        "    # Execute a simulation starting from the constructed initial state.\n",
        "    # `transient_period` is set to 0 because we are interested in the entire\n",
        "    # path to determine convergence from the start.\n",
        "    # `seed` is None for this deterministic analysis.\n",
        "    sim_results = run_simulation_func(\n",
        "        initial_state=initial_state,\n",
        "        params=params,\n",
        "        core_funcs=core_funcs,\n",
        "        num_iterations=basin_config['max_iterations'],\n",
        "        transient_period=0,\n",
        "        seed=None\n",
        "    )\n",
        "\n",
        "    # --- 3. Classify the Simulation Outcome ---\n",
        "    # Check the status flag returned by the simulation engine.\n",
        "    if sim_results['status'] == 'diverged':\n",
        "        # If the state variables grew uncontrollably, classify as divergent.\n",
        "        return 0\n",
        "\n",
        "    # If the simulation completed without diverging, check for convergence.\n",
        "    # We only need the (e, Δy) components for distance calculation.\n",
        "    final_point = sim_results['final_state'][:2]\n",
        "\n",
        "    # Calculate the Euclidean distance from the simulation's final point to each known stable equilibrium.\n",
        "    distances = [np.linalg.norm(np.array(final_point) - np.array(eq)) for eq in stable_equilibria]\n",
        "\n",
        "    # Find the index of the stable equilibrium that is closest to the final point.\n",
        "    closest_idx = np.argmin(distances)\n",
        "\n",
        "    # Check if the minimum distance is within the specified convergence tolerance.\n",
        "    if distances[closest_idx] < basin_config['convergence_tolerance']:\n",
        "        # If it is, the trajectory has converged. Return an integer code corresponding\n",
        "        # to the attractor (1 for the first in the list, 2 for the second, etc.).\n",
        "        return closest_idx + 1\n",
        "    else:\n",
        "        # If the trajectory ended but is not close to any known stable fixed point,\n",
        "        # it may have settled on a periodic or chaotic attractor.\n",
        "        return -1\n",
        "\n",
        "\n",
        "def _classify_basin_points(\n",
        "    initial_conditions: List[Tuple[float, ...]],\n",
        "    params: Dict[str, Any],\n",
        "    basin_config: Dict[str, Any],\n",
        "    stable_equilibria: List[Tuple[float, float]],\n",
        "    core_funcs: Dict[str, Callable],\n",
        "    run_simulation_func: Callable[..., Dict[str, Any]]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Classifies a grid of initial conditions by running simulations in parallel.\n",
        "\n",
        "    This function serves as a parallel coordinator. It takes a list of all\n",
        "    initial conditions from the grid and distributes the task of classifying\n",
        "    each point to multiple CPU cores using `joblib`. This massively speeds up\n",
        "    the computationally expensive process of generating the basin of attraction data.\n",
        "\n",
        "    Args:\n",
        "        initial_conditions (List[Tuple[float, ...]]): A flattened list of all\n",
        "            (e_0, Δy_0) points to be classified.\n",
        "        params (Dict[str, Any]): The dictionary of fixed model parameters.\n",
        "        basin_config (Dict[str, Any]): Configuration for the basin analysis.\n",
        "        stable_equilibria (List[Tuple[float, float]]): List of stable attractors.\n",
        "        core_funcs (Dict[str, Callable]): Dictionary of core model logic functions.\n",
        "        run_simulation_func (Callable[..., Dict[str, Any]]): The main simulation engine.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D numpy array of integer classification codes, where the\n",
        "                    order of the codes corresponds to the order of the input\n",
        "                    `initial_conditions` list.\n",
        "    \"\"\"\n",
        "    # Announce the start of the computationally intensive classification process.\n",
        "    print(f\"Classifying {len(initial_conditions)} initial conditions in parallel...\")\n",
        "\n",
        "    # Use joblib.Parallel to create a pool of worker processes.\n",
        "    # `n_jobs=-1` automatically uses all available CPU cores.\n",
        "    # `delayed` creates a lightweight \"promise\" to call the worker function\n",
        "    # with its specific arguments.\n",
        "    # The list comprehension generates a sequence of these promises, one for each\n",
        "    # initial condition, which are then executed in parallel.\n",
        "    # `tqdm` is wrapped around the iterable to provide a real-time progress bar.\n",
        "    classification_codes = Parallel(n_jobs=-1)(\n",
        "        delayed(_classify_single_point)(\n",
        "            ic, params, basin_config, stable_equilibria, core_funcs, run_simulation_func\n",
        "        ) for ic in tqdm(initial_conditions, desc=\"Classifying basin\")\n",
        "    )\n",
        "\n",
        "    # Convert the list of integer results into a numpy array and return it.\n",
        "    return np.array(classification_codes)\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 4.2 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_basin_analysis(\n",
        "    scenario_name: str,\n",
        "    master_config: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full basin of attraction analysis for a given scenario.\n",
        "\n",
        "    This function manages the end-to-end process of generating a basin of\n",
        "    attraction diagram. It generates a grid of initial conditions, runs\n",
        "    simulations in parallel to classify the long-term outcome of each point,\n",
        "    and returns the resulting classification matrix ready for visualization.\n",
        "\n",
        "    Args:\n",
        "        scenario_name (str): The name of the figure scenario (e.g., 'fig5').\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the classification matrix and\n",
        "                        the grid coordinates, ready for plotting.\n",
        "    \"\"\"\n",
        "    print(f\"========== Starting Task 4.2: Basin of Attraction Analysis for {scenario_name} ==========\")\n",
        "\n",
        "    # --- 1. Retrieve Configuration and Functions ---\n",
        "    scenario_config = master_config['theoretical_simulation']['figure_scenarios'][scenario_name]\n",
        "    params = scenario_config['fixed_params']\n",
        "    basin_config = scenario_config['basin_plot_settings']\n",
        "\n",
        "    core_funcs = model_toolkit['core_dynamics']\n",
        "    run_simulation_func = model_toolkit['evolution_engine']['run_simulation']\n",
        "    stability_analyzer = model_toolkit['stability_analysis']\n",
        "\n",
        "    # --- 2. Find Stable Equilibria to Serve as Targets ---\n",
        "    print(\"Step 4.2.0: Finding stable equilibria...\")\n",
        "    stability_reports = stability_analyzer['run_full_analysis'](params)\n",
        "    stable_equilibria = [\n",
        "        report['equilibrium_point'] for report in stability_reports if report['is_stable']\n",
        "    ]\n",
        "    if not stable_equilibria:\n",
        "        raise RuntimeError(\"No stable equilibria found for the given parameters. Cannot generate basin plot.\")\n",
        "    print(f\"Found {len(stable_equilibria)} stable attractors to classify against.\")\n",
        "\n",
        "    # --- 3. Generate the Grid of Initial Conditions ---\n",
        "    print(\"\\nStep 4.2.1: Generating grid of initial conditions...\")\n",
        "    e0_grid, delta_y0_grid, initial_conditions = _generate_initial_condition_grid(basin_config)\n",
        "    print(f\"Grid generated with {len(initial_conditions)} points.\")\n",
        "\n",
        "    # --- 4. Run the Classification Engine in Parallel ---\n",
        "    classification_codes = _classify_basin_points(\n",
        "        initial_conditions=initial_conditions,\n",
        "        params=params,\n",
        "        basin_config=basin_config,\n",
        "        stable_equilibria=stable_equilibria,\n",
        "        core_funcs=core_funcs,\n",
        "        run_simulation_func=run_simulation_func\n",
        "    )\n",
        "\n",
        "    # --- 5. Reshape Results and Assemble Report ---\n",
        "    print(\"\\nAssembling final report...\")\n",
        "    # Reshape the 1D array of classification codes back into a 2D grid.\n",
        "    classification_matrix = classification_codes.reshape(e0_grid.shape)\n",
        "\n",
        "    final_report = {\n",
        "        'classification_matrix': classification_matrix,\n",
        "        'e0_grid': e0_grid,\n",
        "        'delta_y0_grid': delta_y0_grid,\n",
        "        'stable_equilibria': stable_equilibria,\n",
        "        'scenario_config': scenario_config\n",
        "    }\n",
        "\n",
        "    print(f\"\\n========== Task 4.2: Basin Analysis for {scenario_name} Completed Successfully ==========\")\n",
        "\n",
        "    return final_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 4.3.1: LARGEST LYAPUNOV EXPONENT CALCULATION\n",
        "# =============================================================================\n",
        "\n",
        "def _get_jacobian_at_point(\n",
        "    state: Tuple[float, float],\n",
        "    params: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the Jacobian matrix of the 2D system at a specific state point.\n",
        "\n",
        "    This is a helper function that encapsulates the Jacobian calculation logic\n",
        "    from Task 3.3, making it reusable for Lyapunov exponent estimation.\n",
        "\n",
        "    Args:\n",
        "        state (Tuple[float, float]): The (e, Δy) state vector at which to\n",
        "                                     evaluate the Jacobian.\n",
        "        params (Dict[str, Any]): A dictionary of model parameters.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The 2x2 Jacobian matrix evaluated at the given state.\n",
        "    \"\"\"\n",
        "    # Unpack the state variables.\n",
        "    e_bar, delta_y_bar = state\n",
        "\n",
        "    # Pre-calculate the gamma term from the growth equation.\n",
        "    gamma = (1 - params['theta']) * (params['mu'] + params['rho']) / (params['theta'] * params['pi'])\n",
        "\n",
        "    # Calculate the squared deviation term, which is central to the Jacobian elements.\n",
        "    # Note: For an arbitrary point on the trajectory, this is NOT zero.\n",
        "    squared_deviation = (e_bar + params['Omega'] * delta_y_bar)**2\n",
        "\n",
        "    # Construct the Jacobian Matrix J = [[j11, j12], [j21, j22]]\n",
        "    # Element j11: ∂e_t / ∂e_{t-1}\n",
        "    j11 = 1 + (params['mu'] + params['rho']) * (-3 * params['w_F'] * squared_deviation + params['w_C'])\n",
        "    # Element j12: ∂e_t / ∂Δy_{t-1}\n",
        "    j12 = (params['mu'] + params['rho']) * params['Omega'] * (-3 * params['w_F'] * squared_deviation + params['w_C'])\n",
        "    # Element j21: ∂Δy_t / ∂e_{t-1}\n",
        "    j21 = params['w_flex'] * params['beta'] * (-gamma * (-3 * params['w_F'] * squared_deviation + params['w_C']))\n",
        "    # Element j22: ∂Δy_t / ∂Δy_{t-1}\n",
        "    j22 = 1 + params['w_flex'] * params['beta'] * (\n",
        "        -gamma * params['Omega'] * (-3 * params['w_F'] * squared_deviation + params['w_C']) - 1\n",
        "    )\n",
        "\n",
        "    # Assemble and return the Jacobian matrix.\n",
        "    return np.array([[j11, j12], [j21, j22]])\n",
        "\n",
        "\n",
        "def calculate_largest_lyapunov_exponent(\n",
        "    params: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any],\n",
        "    num_iterations: int = 20000,\n",
        "    transient_period: int = 5000\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Largest Lyapunov Exponent (LLE) for the 2D system.\n",
        "\n",
        "    This function quantifies the degree of chaos in the system by measuring the\n",
        "    average exponential rate of divergence of nearby trajectories. A positive LLE\n",
        "    is a strong indicator of chaotic dynamics. The calculation follows the standard\n",
        "    algorithm of evolving a perturbation vector along a trajectory and repeatedly\n",
        "    applying Gram-Schmidt reorthonormalization.\n",
        "\n",
        "    Args:\n",
        "        params (Dict[str, Any]): A dictionary of model parameters for the simulation.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions.\n",
        "        num_iterations (int): The total number of iterations to run the trajectory for.\n",
        "                              A long trajectory is needed for a stable estimate.\n",
        "        transient_period (int): The number of initial iterations to discard to ensure\n",
        "                                the trajectory is on the attractor.\n",
        "\n",
        "    Returns:\n",
        "        float: The estimated Largest Lyapunov Exponent (LLE).\n",
        "    \"\"\"\n",
        "    # --- 1. Retrieve Core Functions ---\n",
        "    run_simulation_func = model_toolkit['evolution_engine']['run_simulation']\n",
        "    core_funcs = model_toolkit['core_dynamics']\n",
        "\n",
        "    # --- 2. Generate a Long Trajectory on the Attractor ---\n",
        "    # Define a standard initial state.\n",
        "    initial_state = (\n",
        "        master_config['theoretical_simulation']['global_settings']['initial_conditions']['e0'],\n",
        "        master_config['theoretical_simulation']['global_settings']['initial_conditions']['delta_y0']\n",
        "    )\n",
        "    # Run a simulation to get the trajectory.\n",
        "    sim_results = run_simulation_func(\n",
        "        initial_state=initial_state,\n",
        "        params=params,\n",
        "        core_funcs=core_funcs,\n",
        "        num_iterations=num_iterations,\n",
        "        transient_period=transient_period\n",
        "    )\n",
        "    trajectory = sim_results['trajectory']\n",
        "\n",
        "    # If the simulation diverged, chaos is irrelevant. Return NaN.\n",
        "    if sim_results['status'] == 'diverged':\n",
        "        return np.nan\n",
        "\n",
        "    # --- 3. LLE Calculation Algorithm ---\n",
        "    # Initialize a random, normalized perturbation vector.\n",
        "    perturbation_vector = np.random.rand(2)\n",
        "    perturbation_vector /= np.linalg.norm(perturbation_vector)\n",
        "\n",
        "    # Initialize a variable to accumulate the sum of log growth factors.\n",
        "    sum_log_growth_factors = 0.0\n",
        "\n",
        "    # Iterate along the generated trajectory.\n",
        "    for t in range(len(trajectory)):\n",
        "        # Get the current state from the trajectory.\n",
        "        current_state = tuple(trajectory[t, :])\n",
        "\n",
        "        # Compute the Jacobian matrix at the current state.\n",
        "        jacobian = _get_jacobian_at_point(current_state, params)\n",
        "\n",
        "        # Evolve the perturbation vector by applying the Jacobian.\n",
        "        evolved_vector = jacobian @ perturbation_vector\n",
        "\n",
        "        # Calculate the norm (magnitude) of the evolved vector. This is the growth factor.\n",
        "        norm = np.linalg.norm(evolved_vector)\n",
        "\n",
        "        # Add the logarithm of the growth factor to the accumulator.\n",
        "        # Equation: sum += log(||J(x_t)v_{t-1}||)\n",
        "        sum_log_growth_factors += np.log(norm)\n",
        "\n",
        "        # Re-normalize the vector to unit length for the next iteration (Gram-Schmidt).\n",
        "        perturbation_vector = evolved_vector / norm\n",
        "\n",
        "    # The LLE is the average of the accumulated log growth factors.\n",
        "    # Equation: λ₁ = (1 / N) * Σ log(||J(x_t)v_{t-1}||)\n",
        "    lle = sum_log_growth_factors / len(trajectory)\n",
        "\n",
        "    return lle\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 4.3.2: CHAOS QUANTIFICATION AND ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_correlation_dimension(\n",
        "    trajectory: np.ndarray,\n",
        "    num_radii: int = 20\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Estimates the correlation dimension of an attractor using the Grassberger-Procaccia algorithm.\n",
        "\n",
        "    This helper function calculates a measure of the fractal dimension of the\n",
        "    attractor represented by the trajectory. A non-integer dimension is a\n",
        "    hallmark of a \"strange attractor,\" characteristic of chaotic systems.\n",
        "\n",
        "    Args:\n",
        "        trajectory (np.ndarray): A 2D numpy array of shape (n_points, n_vars)\n",
        "                                 representing the system's trajectory.\n",
        "        num_radii (int): The number of radius points to use for the log-log regression.\n",
        "\n",
        "    Returns:\n",
        "        float: The estimated correlation dimension (D₂).\n",
        "    \"\"\"\n",
        "    # --- 1. Calculate Pairwise Distances ---\n",
        "    # `pdist` efficiently computes the distance between all pairs of points in the trajectory.\n",
        "    # This is the most computationally intensive step.\n",
        "    distances = pdist(trajectory, metric='euclidean')\n",
        "\n",
        "    # --- 2. Define Radii for Correlation Integral ---\n",
        "    # We create a logarithmically spaced set of radii `r` to scan across the scales of the attractor.\n",
        "    # This ensures we capture behavior at both small and large scales.\n",
        "    # Filter out zero distances to avoid issues with log(0).\n",
        "    non_zero_distances = distances[distances > 0]\n",
        "    if len(non_zero_distances) == 0:\n",
        "        return 0.0 # All points are identical, dimension is 0.\n",
        "\n",
        "    min_r = np.min(non_zero_distances)\n",
        "    max_r = np.max(distances)\n",
        "    radii = np.logspace(np.log10(min_r), np.log10(max_r), num=num_radii)\n",
        "\n",
        "    # --- 3. Compute the Correlation Integral C(r) ---\n",
        "    # C(r) is the probability that two randomly chosen points on the attractor are\n",
        "    # closer than r. We calculate it for each radius in our set.\n",
        "    num_pairs = len(distances)\n",
        "    correlation_integral = np.array([np.sum(distances <= r) for r in radii]) / num_pairs\n",
        "\n",
        "    # --- 4. Estimate Dimension via Log-Log Regression ---\n",
        "    # The correlation dimension D₂ is the slope of log(C(r)) versus log(r).\n",
        "    # We must filter out any radii for which C(r) is 0, as log(0) is undefined.\n",
        "    valid_indices = correlation_integral > 0\n",
        "\n",
        "    # We need at least 3 points to perform a meaningful linear regression.\n",
        "    if np.sum(valid_indices) < 3:\n",
        "        return np.nan  # Not enough data to estimate the dimension.\n",
        "\n",
        "    # Get the log of the radii and the log of the correlation integral.\n",
        "    log_radii = np.log(radii[valid_indices])\n",
        "    log_corr_integral = np.log(correlation_integral[valid_indices])\n",
        "\n",
        "    # Perform a linear regression to find the slope.\n",
        "    try:\n",
        "        slope, _, _, _, _ = linregress(log_radii, log_corr_integral)\n",
        "        correlation_dim = slope\n",
        "    except ValueError:\n",
        "        correlation_dim = np.nan # Handle potential errors in regression.\n",
        "\n",
        "    return correlation_dim\n",
        "\n",
        "\n",
        "def quantify_chaos(\n",
        "    trajectory: np.ndarray,\n",
        "    params: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Quantifies the chaotic properties of a system trajectory using multiple metrics.\n",
        "\n",
        "    This function provides a comprehensive, multi-faceted assessment of the\n",
        "    dynamics of an attractor. It is a crucial tool for moving beyond visual\n",
        "    inspection to quantitatively confirm and characterize chaotic behavior.\n",
        "    The metrics computed are:\n",
        "    1.  Largest Lyapunov Exponent (LLE): The definitive indicator of sensitive\n",
        "        dependence on initial conditions (chaos). LLE > 0 implies chaos.\n",
        "    2.  Correlation Dimension (D₂): A measure of the attractor's fractal dimension.\n",
        "        A non-integer value suggests a strange attractor.\n",
        "    3.  Approximate Entropy (ApEn): A measure of the system's unpredictability\n",
        "        and complexity. Higher values indicate greater irregularity.\n",
        "\n",
        "    Args:\n",
        "        trajectory (np.ndarray): A 2D numpy array of shape (n_points, n_vars)\n",
        "                                 representing the system's trajectory on its attractor.\n",
        "        params (Dict[str, Any]): The dictionary of model parameters used to generate\n",
        "                                 the trajectory, required for LLE calculation.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary containing the calculated chaos metrics.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(trajectory, np.ndarray) or trajectory.ndim != 2:\n",
        "        raise TypeError(\"`trajectory` must be a 2D numpy array.\")\n",
        "    if trajectory.shape[0] < 100:\n",
        "        raise ValueError(\"Trajectory is too short for meaningful chaos analysis.\")\n",
        "\n",
        "    # --- 1. Largest Lyapunov Exponent (LLE) ---\n",
        "    # This is the primary indicator of chaos.\n",
        "    # We reuse the dedicated function, which requires the full model toolkit.\n",
        "    # Note: This recalculates the LLE based on the provided trajectory's parameters,\n",
        "    # which is more robust than assuming the trajectory was generated by a specific\n",
        "    # simulation call.\n",
        "    lle = calculate_largest_lyapunov_exponent(\n",
        "        params=params,\n",
        "        model_toolkit=model_toolkit,\n",
        "        # The trajectory is already post-transient.\n",
        "        num_iterations=len(trajectory),\n",
        "        transient_period=0\n",
        "    )\n",
        "\n",
        "    # --- 2. Correlation Dimension (D₂) ---\n",
        "    # This measures the geometric complexity of the attractor.\n",
        "    correlation_dim = _calculate_correlation_dimension(trajectory)\n",
        "\n",
        "    # --- 3. Approximate Entropy (ApEn) ---\n",
        "    # This measures the unpredictability of the time series.\n",
        "    # We analyze the primary state variable: the exchange rate (first column).\n",
        "    exchange_rate_series = trajectory[:, 0]\n",
        "    # Standard parameters: embedding dimension m=2, tolerance r=0.2*std.\n",
        "    apen = nolds.apen(exchange_rate_series, emb_dim=2, tolerance=0.2 * np.std(exchange_rate_series))\n",
        "\n",
        "    # --- 4. Assemble the Final Report ---\n",
        "    return {\n",
        "        'largest_lyapunov_exponent': lle,\n",
        "        'correlation_dimension': correlation_dim,\n",
        "        'approximate_entropy': apen\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 4.3.3: PARAMETER SENSITIVITY ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def _run_sensitivity_perturbation(\n",
        "    params: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Executes a single simulation run and returns a summary statistic of the attractor.\n",
        "\n",
        "    This is a high-fidelity worker function designed for use within the parameter\n",
        "    sensitivity analysis pipeline. It takes a complete parameter set, runs a\n",
        "    simulation until the system settles on its long-run attractor, and then\n",
        "    calculates a single scalar value that summarizes the attractor's location.\n",
        "    This scalar output is then used to numerically estimate the derivative of the\n",
        "    system's behavior with respect to its parameters.\n",
        "\n",
        "    Args:\n",
        "        params (Dict[str, Any]): The complete dictionary of model parameters for this\n",
        "                                 specific simulation run.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions,\n",
        "                                        providing access to the simulation engine.\n",
        "\n",
        "    Returns:\n",
        "        float: A summary statistic of the long-run attractor. Specifically, it\n",
        "               returns the mean of the asymptotic exchange rate trajectory. If the\n",
        "               simulation diverges, it returns a large penalty value (999.0) to\n",
        "               signal instability.\n",
        "    \"\"\"\n",
        "    # Retrieve the core simulation engine from the provided toolkit.\n",
        "    run_simulation_func = model_toolkit['evolution_engine']['run_simulation']\n",
        "    core_funcs = model_toolkit['core_dynamics']\n",
        "\n",
        "    # Execute a standard simulation run with the given parameters.\n",
        "    # A fixed initial state is used to ensure comparability across different parameter sets.\n",
        "    # The simulation runs for a sufficient number of iterations with a transient period\n",
        "    # to ensure the system has settled onto its attractor.\n",
        "    sim_results = run_simulation_func(\n",
        "        initial_state=(0.01, 0.00003),\n",
        "        params=params,\n",
        "        core_funcs=core_funcs,\n",
        "        num_iterations=2000,\n",
        "        transient_period=1000\n",
        "    )\n",
        "\n",
        "    # Check the status of the simulation.\n",
        "    if sim_results['status'] == 'diverged':\n",
        "        # If the system was unstable and diverged, return a large, fixed penalty value.\n",
        "        # This indicates an extreme sensitivity or a move into an unstable parameter region.\n",
        "        return 999.0\n",
        "\n",
        "    # If the simulation completed successfully, calculate the summary statistic.\n",
        "    # The mean of the exchange rate over the asymptotic trajectory is used as a robust\n",
        "    # measure of the attractor's central location in the phase space.\n",
        "    return np.mean(sim_results['trajectory'][:, 0])\n",
        "\n",
        "\n",
        "def analyze_global_parameter_sensitivity(\n",
        "    base_params: Dict[str, Any],\n",
        "    sensitivity_config: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a global sensitivity analysis using the Sobol method.\n",
        "\n",
        "    This function provides a comprehensive, variance-based sensitivity analysis\n",
        "    to quantify the importance of each model parameter. It assesses how the\n",
        "    uncertainty in the model's output (the mean of the asymptotic exchange rate)\n",
        "    can be apportioned to the uncertainty in its input parameters. This is far\n",
        "    more powerful than local methods as it explores the entire parameter space\n",
        "    and captures interaction effects.\n",
        "\n",
        "    The analysis calculates:\n",
        "    - S1 (First-order index): The direct contribution of a parameter to output variance.\n",
        "    - ST (Total-order index): The total contribution of a parameter, including all\n",
        "      its interactions with other parameters.\n",
        "\n",
        "    Args:\n",
        "        base_params (Dict[str, Any]): The baseline set of model parameters.\n",
        "        sensitivity_config (Dict[str, Any]): Configuration for the analysis,\n",
        "            specifying the number of samples `N`, and a `bounds` dictionary\n",
        "            mapping parameter names to their `[min, max]` uncertainty ranges.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by parameter name, reporting the S1 and ST\n",
        "                      Sobol indices and their confidence intervals.\n",
        "    \"\"\"\n",
        "    # --- 1. Define the SALib Problem ---\n",
        "    # The \"problem\" dictionary defines the parameter space for the analysis.\n",
        "    problem = {\n",
        "        'num_vars': len(sensitivity_config['bounds']),\n",
        "        'names': list(sensitivity_config['bounds'].keys()),\n",
        "        'bounds': list(sensitivity_config['bounds'].values())\n",
        "    }\n",
        "\n",
        "    # --- 2. Generate Parameter Samples ---\n",
        "    # Use Saltelli sampling, which is specifically designed for Sobol analysis.\n",
        "    # The number of model evaluations will be N * (2 * D + 2), where D is num_vars.\n",
        "    num_samples = sensitivity_config.get('N', 1024)\n",
        "    param_samples = saltelli.sample(problem, num_samples, calc_second_order=False)\n",
        "    print(f\"Generated {param_samples.shape[0]} parameter samples for Sobol analysis.\")\n",
        "\n",
        "    # --- 3. Evaluate the Model in Parallel ---\n",
        "    # Create a list of tasks for parallel execution. Each task is a simulation\n",
        "    # run with a parameter set from the Saltelli sample.\n",
        "    tasks = []\n",
        "    for param_set in param_samples:\n",
        "        # Create a full parameter dictionary for this run.\n",
        "        current_params = base_params.copy()\n",
        "        # Update the base parameters with the values from the current sample.\n",
        "        current_params.update(dict(zip(problem['names'], param_set)))\n",
        "        # Add the simulation run to the list of tasks.\n",
        "        tasks.append(delayed(_run_sensitivity_perturbation)(current_params, model_toolkit))\n",
        "\n",
        "    # Execute all simulation runs in parallel using all available CPU cores.\n",
        "    print(\"Evaluating model for each parameter sample...\")\n",
        "    model_outputs = Parallel(n_jobs=-1)(\n",
        "        t for t in tqdm(tasks, desc=\"Running GSA simulations\")\n",
        "    )\n",
        "    model_outputs = np.array(model_outputs)\n",
        "\n",
        "    # --- 4. Perform Sobol Analysis ---\n",
        "    # Use the generated samples and the model outputs to calculate the Sobol indices.\n",
        "    print(\"Analyzing results to compute Sobol indices...\")\n",
        "    sobol_indices = sobol.analyze(problem, model_outputs, calc_second_order=False)\n",
        "\n",
        "    # --- 5. Format and Return the Results ---\n",
        "    # Convert the dictionary output from SALib into a clean pandas DataFrame.\n",
        "    results_df = pd.DataFrame({\n",
        "        'S1': sobol_indices['S1'],\n",
        "        'S1_conf': sobol_indices['S1_conf'],\n",
        "        'ST': sobol_indices['ST'],\n",
        "        'ST_conf': sobol_indices['ST_conf']\n",
        "    }, index=problem['names'])\n",
        "\n",
        "    # Sort by the total-order index to rank parameters by importance.\n",
        "    results_df.sort_values(by='ST', ascending=False, inplace=True)\n",
        "\n",
        "    print(\"Global sensitivity analysis complete.\")\n",
        "    return results_df\n",
        "\n",
        "\n",
        "def analyze_local_parameter_sensitivity(\n",
        "    base_params: Dict[str, Any],\n",
        "    params_to_test: List[str],\n",
        "    model_toolkit: Dict[str, Any],\n",
        "    perturbation_size: float = 0.01\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a local sensitivity analysis on model parameters using numerical differentiation.\n",
        "\n",
        "    This function estimates the partial derivative of a key model output (the mean\n",
        "    of the asymptotic exchange rate) with respect to specified model parameters.\n",
        "    It employs the central difference method, a standard numerical technique, which\n",
        "    involves running simulations for small perturbations around a baseline\n",
        "    parameter set. The analysis is parallelized for efficiency.\n",
        "\n",
        "    This method is \"local\" because it measures sensitivity at a single point in\n",
        "    the parameter space and does not capture non-linearities or interaction\n",
        "    effects across the full range of parameter values.\n",
        "\n",
        "    Args:\n",
        "        base_params (Dict[str, Any]): The baseline set of model parameters around\n",
        "                                     which sensitivity will be measured.\n",
        "        params_to_test (List[str]): A list of parameter names (keys in `base_params`)\n",
        "                                    to be analyzed.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions,\n",
        "                                        providing access to the simulation engine.\n",
        "        perturbation_size (float): The relative size of the perturbation to apply\n",
        "                                   (e.g., 0.01 corresponds to a +/- 1% change).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by parameter name, reporting the raw\n",
        "                      sensitivity estimate (numerical partial derivative) and a\n",
        "                      normalized, elasticity-like sensitivity score.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the simulation with the baseline parameters diverges, as\n",
        "                      sensitivity analysis is not meaningful in that case.\n",
        "    \"\"\"\n",
        "    # Announce the start of the local sensitivity analysis.\n",
        "    print(\"--- Starting Task 4.3.3: Local Parameter Sensitivity Analysis ---\")\n",
        "\n",
        "    # --- 1. Run Baseline Simulation ---\n",
        "    # Execute a simulation with the unperturbed, baseline parameters to get the\n",
        "    # reference output value.\n",
        "    baseline_output = _run_sensitivity_perturbation(base_params, model_toolkit)\n",
        "\n",
        "    # Check if the baseline simulation was stable. If not, halt the analysis.\n",
        "    if baseline_output == 999.0:\n",
        "        raise RuntimeError(\"Baseline simulation diverged. Cannot perform local sensitivity analysis.\")\n",
        "\n",
        "    # --- 2. Prepare Perturbation Simulation Tasks ---\n",
        "    # Initialize a list to hold the tasks for parallel execution.\n",
        "    perturbation_tasks = []\n",
        "\n",
        "    # Iterate through each parameter designated for testing.\n",
        "    for param in params_to_test:\n",
        "        # Create two copies of the base parameters for the positive and negative perturbations.\n",
        "        p_plus_params, p_minus_params = base_params.copy(), base_params.copy()\n",
        "\n",
        "        # Calculate the absolute perturbation amount (epsilon).\n",
        "        epsilon = base_params[param] * perturbation_size\n",
        "\n",
        "        # Apply the positive perturbation (p + ε).\n",
        "        p_plus_params[param] += epsilon\n",
        "        # Apply the negative perturbation (p - ε).\n",
        "        p_minus_params[param] -= epsilon\n",
        "\n",
        "        # Create delayed execution \"promises\" for both perturbed simulations.\n",
        "        # These are added to the task list to be run in parallel.\n",
        "        perturbation_tasks.append(delayed(_run_sensitivity_perturbation)(p_plus_params, model_toolkit))\n",
        "        perturbation_tasks.append(delayed(_run_sensitivity_perturbation)(p_minus_params, model_toolkit))\n",
        "\n",
        "    # --- 3. Execute Perturbed Simulations in Parallel ---\n",
        "    # Use joblib.Parallel to distribute the simulation tasks across all available CPU cores.\n",
        "    # The `tqdm` wrapper provides a progress bar for monitoring.\n",
        "    perturbed_outputs = Parallel(n_jobs=-1)(\n",
        "        t for t in tqdm(perturbation_tasks, desc=\"Analyzing Local Sensitivity\")\n",
        "    )\n",
        "\n",
        "    # --- 4. Calculate and Collate Sensitivity Results ---\n",
        "    # Initialize a list to store the final results for each parameter.\n",
        "    sensitivity_results = []\n",
        "\n",
        "    # Iterate through the parameters and their corresponding simulation outputs.\n",
        "    for i, param in enumerate(params_to_test):\n",
        "        # Unpack the results for the positive (p+ε) and negative (p-ε) perturbations.\n",
        "        output_plus = perturbed_outputs[2*i]\n",
        "        output_minus = perturbed_outputs[2*i + 1]\n",
        "\n",
        "        # Recalculate epsilon for the denominator.\n",
        "        epsilon = base_params[param] * perturbation_size\n",
        "\n",
        "        # Calculate the numerical partial derivative using the central difference formula.\n",
        "        # Equation: ∂(output)/∂(param) ≈ (output(p+ε) - output(p-ε)) / (2*ε)\n",
        "        sensitivity = (output_plus - output_minus) / (2 * epsilon)\n",
        "\n",
        "        # Calculate a normalized, unitless sensitivity score (similar to an elasticity).\n",
        "        # This helps in comparing the relative importance of parameters with different scales.\n",
        "        normalized_sensitivity = sensitivity * (base_params[param] / baseline_output)\n",
        "\n",
        "        # Append the calculated metrics to the results list.\n",
        "        sensitivity_results.append({\n",
        "            'parameter': param,\n",
        "            'sensitivity_estimate': sensitivity,\n",
        "            'normalized_sensitivity': normalized_sensitivity\n",
        "        })\n",
        "\n",
        "    # Convert the list of dictionaries into a pandas DataFrame, indexed by the parameter name.\n",
        "    return pd.DataFrame(sensitivity_results).set_index('parameter')\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 4 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_numerical_dynamics_analysis(\n",
        "    analysis_type: str,\n",
        "    scenario_name: str,\n",
        "    master_config: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any],\n",
        "    local_sensitivity_config: Optional[Dict[str, Any]] = None,\n",
        "    global_sensitivity_config: Optional[Dict[str, Any]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates all numerical analyses of the model's dynamics (Task 4).\n",
        "\n",
        "    This master function serves as the primary entry point for exploring the\n",
        "    theoretical model's rich dynamical behavior. It acts as a dispatcher,\n",
        "    routing requests to the appropriate specialized analysis function based on the\n",
        "    `analysis_type` argument. It supports bifurcation analysis, basin of\n",
        "    attraction mapping, chaos quantification, and both local and global\n",
        "    parameter sensitivity analysis.\n",
        "\n",
        "    Args:\n",
        "        analysis_type (str): The type of analysis to perform. Supported options:\n",
        "            'bifurcation', 'basin_of_attraction', 'chaos_quantification',\n",
        "            'local_sensitivity', 'global_sensitivity'.\n",
        "        scenario_name (str): The name of the figure or analysis scenario as defined\n",
        "            in the master configuration dictionary (e.g., 'fig4a', 'fig7').\n",
        "        master_config (Dict[str, Any]): The complete master configuration dictionary.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions\n",
        "            from previous tasks.\n",
        "        local_sensitivity_config (Optional[Dict[str, Any]]): Configuration for\n",
        "            local sensitivity analysis, required if analysis_type is 'local_sensitivity'.\n",
        "            Must specify 'params_to_test'.\n",
        "        global_sensitivity_config (Optional[Dict[str, Any]]): Configuration for\n",
        "            global sensitivity analysis, required if analysis_type is 'global_sensitivity'.\n",
        "            Must specify 'N' (samples) and 'bounds' (parameter ranges).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the comprehensive results of the\n",
        "                        requested numerical analysis.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unsupported `analysis_type` is provided or if a\n",
        "                    required configuration dictionary is missing for a sensitivity analysis.\n",
        "    \"\"\"\n",
        "    # Announce the start of the task, specifying the chosen analysis and scenario.\n",
        "    print(f\"========== Starting Task 4: Numerical Dynamics Analysis ==========\")\n",
        "    print(f\"Analysis Type: '{analysis_type}', Scenario: '{scenario_name}'\")\n",
        "\n",
        "    # Dispatch the request to the appropriate analysis function based on `analysis_type`.\n",
        "    if analysis_type == 'bifurcation':\n",
        "        # If 'bifurcation' is requested, call the bifurcation analysis orchestrator.\n",
        "        results = orchestrate_bifurcation_analysis(scenario_name, master_config, model_toolkit)\n",
        "\n",
        "    elif analysis_type == 'basin_of_attraction':\n",
        "        # If 'basin_of_attraction' is requested, call the basin analysis orchestrator.\n",
        "        results = orchestrate_basin_analysis(scenario_name, master_config, model_toolkit)\n",
        "\n",
        "    elif analysis_type == 'chaos_quantification':\n",
        "        # If 'chaos_quantification' is requested, configure and run the analysis.\n",
        "        # Retrieve the fixed parameters for the specified scenario.\n",
        "        params = master_config['theoretical_simulation']['figure_scenarios'][scenario_name]['fixed_params']\n",
        "        # Run a long simulation to get a well-defined attractor.\n",
        "        sim_results = model_toolkit['evolution_engine']['run_simulation'](\n",
        "            initial_state=(0.01, 0.00003), params=params, core_funcs=model_toolkit['core_dynamics'],\n",
        "            num_iterations=25000, transient_period=5000\n",
        "        )\n",
        "        # Call the chaos quantification function on the resulting trajectory.\n",
        "        results = quantify_chaos(sim_results['trajectory'], params, model_toolkit)\n",
        "\n",
        "    elif analysis_type == 'local_sensitivity':\n",
        "        # If 'local_sensitivity' is requested, validate config and run the analysis.\n",
        "        # Ensure that the necessary sensitivity configuration has been provided.\n",
        "        if local_sensitivity_config is None:\n",
        "            raise ValueError(\"`local_sensitivity_config` must be provided for 'local_sensitivity' analysis.\")\n",
        "        # Retrieve the baseline parameters for the specified scenario.\n",
        "        base_params = master_config['theoretical_simulation']['figure_scenarios'][scenario_name]['fixed_params']\n",
        "        # Call the local parameter sensitivity analysis function.\n",
        "        results = analyze_local_parameter_sensitivity(\n",
        "            base_params=base_params,\n",
        "            params_to_test=local_sensitivity_config['params_to_test'],\n",
        "            model_toolkit=model_toolkit,\n",
        "            perturbation_size=local_sensitivity_config.get('perturbation_size', 0.01)\n",
        "        )\n",
        "\n",
        "    elif analysis_type == 'global_sensitivity':\n",
        "        # If 'global_sensitivity' is requested, validate config and run the analysis.\n",
        "        if global_sensitivity_config is None:\n",
        "            raise ValueError(\"`global_sensitivity_config` must be provided for 'global_sensitivity' analysis.\")\n",
        "        # Retrieve the baseline parameters, which can inform the center of the bounds.\n",
        "        base_params = master_config['theoretical_simulation']['figure_scenarios'][scenario_name]['fixed_params']\n",
        "        # Call the global parameter sensitivity analysis function.\n",
        "        results = analyze_global_parameter_sensitivity(\n",
        "            base_params=base_params,\n",
        "            sensitivity_config=global_sensitivity_config,\n",
        "            model_toolkit=model_toolkit\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # If the analysis_type is not recognized, raise an informative error.\n",
        "        raise ValueError(\n",
        "            f\"Unsupported analysis_type: '{analysis_type}'. \"\n",
        "            f\"Supported options are: 'bifurcation', 'basin_of_attraction', \"\n",
        "            f\"'chaos_quantification', 'local_sensitivity', 'global_sensitivity'.\"\n",
        "        )\n",
        "\n",
        "    # Announce the successful completion of the task.\n",
        "    print(f\"\\n========== Task 4: Analysis for {scenario_name} Completed Successfully ==========\")\n",
        "\n",
        "    # Return the final results dictionary.\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "U0ntoOv01znl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK V: BAYESIAN ECONOMETRIC ESTIMATION\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 5.1.1: KALMAN FILTER SETUP AND INITIALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "def _setup_kalman_filter_matrices(\n",
        "    country_data: pd.DataFrame,\n",
        "    params: Dict[str, float],\n",
        "    priors: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Sets up the matrices and initial conditions for the state-space model.\n",
        "\n",
        "    This function translates the economic model from Appendix A.2 into the\n",
        "    canonical state-space representation required by the Kalman filter algorithms.\n",
        "\n",
        "    Model:\n",
        "    - Measurement: m_t^T = η*rer_t + π_t*y_t^T + ε_m,  ε_m ~ N(0, σ_m²)\n",
        "    - State:       π_t = π_{t-1} + ε_π,             ε_π ~ N(0, σ_π²)\n",
        "\n",
        "    Canonical Form:\n",
        "    - Measurement: y'_t = H_t * α_t + v_t, v_t ~ N(0, R)\n",
        "    - State:       α_t = F * α_{t-1} + w_t, w_t ~ N(0, Q)\n",
        "\n",
        "    Args:\n",
        "        country_data (pd.DataFrame): The time-series data for a single country.\n",
        "        params (Dict[str, float]): A dictionary of the current values of the\n",
        "                                   model's fixed parameters (η, σ_m², σ_π²).\n",
        "        priors (Dict[str, Any]): The dictionary of prior distributions.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all necessary components for the\n",
        "                        Kalman filter: y', H, F, Q, R, a0 (prior mean), P0 (prior var).\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Observation Data (y') ---\n",
        "    # The \"observation\" in the canonical form is the part of the measurement\n",
        "    # equation that is not explained by the fixed parameters.\n",
        "    # y'_t = m_t^T - η * rer_t\n",
        "    y_prime = (\n",
        "        country_data['imports_const_lcu_trend'].values -\n",
        "        country_data['reer_log_levels'].values * params['eta']\n",
        "    )\n",
        "\n",
        "    # --- 2. Define State-Space Matrices ---\n",
        "    # The state vector α_t is the time-varying import elasticity [π_t].\n",
        "\n",
        "    # Measurement matrix H_t: Links the state to the observation.\n",
        "    # H_t = [y_t^T]\n",
        "    H = country_data[['gdp_const_lcu_trend']].values\n",
        "\n",
        "    # Transition matrix F: Governs the evolution of the state.\n",
        "    # For a random walk, F = [[1.0]].\n",
        "    F = np.array([[1.0]])\n",
        "\n",
        "    # Measurement error covariance R.\n",
        "    # R = σ_m²\n",
        "    R = params['sigma2_m']\n",
        "\n",
        "    # State error covariance Q.\n",
        "    # Q = σ_π²\n",
        "    Q = params['sigma2_pi']\n",
        "\n",
        "    # --- 3. Define Initial State Priors ---\n",
        "    # a0: Prior mean of the initial state π_0.\n",
        "    a0 = priors['pi_0']['mu']\n",
        "    # P0: Prior variance of the initial state π_0.\n",
        "    P0 = priors['pi_0']['sigma']**2\n",
        "\n",
        "    return {\n",
        "        'y_prime': y_prime, 'H': H, 'F': F, 'Q': Q, 'R': R, 'a0': a0, 'P0': P0\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 5.1.2: FORWARD FILTERING IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def _kalman_forward_filter(\n",
        "    ss_model: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Implements the forward pass of the Kalman filter.\n",
        "\n",
        "    This function iterates forward in time to compute the filtered estimates of\n",
        "    the state vector and its covariance. It also calculates the log-likelihood\n",
        "    of the observations given the model parameters, which is essential for\n",
        "    model comparison and can be used in certain MCMC methods.\n",
        "\n",
        "    Args:\n",
        "        ss_model (Dict[str, Any]): A dictionary containing the fully specified\n",
        "                                   state-space model from the setup function.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the full history of the filter's\n",
        "                        outputs, including filtered states, covariances, and the\n",
        "                        total log-likelihood.\n",
        "    \"\"\"\n",
        "    # --- 1. Unpack Model Components ---\n",
        "    y, H, F, Q, R, a0, P0 = (\n",
        "        ss_model['y_prime'], ss_model['H'], ss_model['F'], ss_model['Q'],\n",
        "        ss_model['R'], ss_model['a0'], ss_model['P0']\n",
        "    )\n",
        "    T = len(y)\n",
        "\n",
        "    # --- 2. Initialize Storage Arrays ---\n",
        "    # Store predicted (a_t|t-1) and filtered (a_t|t) states and covariances.\n",
        "    predicted_states = np.zeros((T, 1))\n",
        "    predicted_covs = np.zeros((T, 1, 1))\n",
        "    filtered_states = np.zeros((T, 1))\n",
        "    filtered_covs = np.zeros((T, 1, 1))\n",
        "    log_likelihood = 0.0\n",
        "\n",
        "    # --- 3. Run Forward Recursions ---\n",
        "    # Initialize the prediction for the first time step (t=1).\n",
        "    a_pred, P_pred = np.array([a0]), np.array([[P0]])\n",
        "\n",
        "    for t in range(T):\n",
        "        # Store the predicted state and covariance for this time step.\n",
        "        predicted_states[t], predicted_covs[t] = a_pred, P_pred\n",
        "\n",
        "        # Calculate the prediction error (innovation) and its variance.\n",
        "        v = y[t] - H[t] @ a_pred\n",
        "        S = H[t] @ P_pred @ H[t].T + R\n",
        "        S_inv = 1.0 / S # Since S is scalar\n",
        "\n",
        "        # Calculate the Kalman Gain.\n",
        "        K = P_pred @ H[t].T * S_inv\n",
        "\n",
        "        # Update step: Correct the prediction with the new observation.\n",
        "        a_filt = a_pred + K * v\n",
        "        P_filt = P_pred - K @ S @ K.T\n",
        "\n",
        "        # Store the filtered state and covariance.\n",
        "        filtered_states[t], filtered_covs[t] = a_filt, P_filt\n",
        "\n",
        "        # Prediction for the next step (t+1).\n",
        "        a_pred = F @ a_filt\n",
        "        P_pred = F @ P_filt @ F.T + Q\n",
        "\n",
        "        # Accumulate the log-likelihood.\n",
        "        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(S) + v**2 * S_inv)\n",
        "\n",
        "    return {\n",
        "        'predicted_states': predicted_states, 'predicted_covs': predicted_covs,\n",
        "        'filtered_states': filtered_states, 'filtered_covs': filtered_covs,\n",
        "        'log_likelihood': log_likelihood.item()\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 5.1.3: BACKWARD SMOOTHING IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def _kalman_backward_smoother(\n",
        "    filter_results: Dict[str, np.ndarray],\n",
        "    ss_model: Dict[str, Any]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Implements the Rauch-Tung-Striebel (RTS) backward smoother.\n",
        "\n",
        "    This function takes the full output of the Kalman forward filter and iterates\n",
        "    backward in time to compute the smoothed estimates of the state vector.\n",
        "    The smoothed estimate at time t, a_{t|T}, uses all information in the\n",
        "    sample and is generally more accurate than the filtered estimate, a_{t|t}.\n",
        "\n",
        "    Args:\n",
        "        filter_results (Dict[str, np.ndarray]): The output dictionary from the\n",
        "                                                `_kalman_forward_filter` function.\n",
        "        ss_model (Dict[str, Any]): The dictionary defining the state-space model.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary containing the smoothed states and\n",
        "                               their corresponding covariances.\n",
        "    \"\"\"\n",
        "    # --- 1. Unpack Inputs ---\n",
        "    F = ss_model['F']\n",
        "    predicted_states = filter_results['predicted_states']\n",
        "    predicted_covs = filter_results['predicted_covs']\n",
        "    filtered_states = filter_results['filtered_states']\n",
        "    filtered_covs = filter_results['filtered_covs']\n",
        "    T = len(filtered_states)\n",
        "\n",
        "    # --- 2. Initialize Storage Arrays ---\n",
        "    smoothed_states = np.zeros_like(filtered_states)\n",
        "    smoothed_covs = np.zeros_like(filtered_covs)\n",
        "\n",
        "    # --- 3. Run Backward Recursions ---\n",
        "    # Initialize the smoother with the final filtered estimate.\n",
        "    smoothed_states[-1] = filtered_states[-1]\n",
        "    smoothed_covs[-1] = filtered_covs[-1]\n",
        "\n",
        "    # Iterate backward from T-1 down to 0.\n",
        "    for t in range(T - 2, -1, -1):\n",
        "        # Calculate the smoothing gain, J_t.\n",
        "        # J_t = P_{t|t} * F' * (P_{t+1|t})^{-1}\n",
        "        # We use `solve` for numerical stability instead of direct inversion.\n",
        "        J = solve(predicted_covs[t+1], F @ filtered_covs[t], assume_a='pos').T\n",
        "\n",
        "        # Update the state estimate and covariance.\n",
        "        # a_{t|T} = a_{t|t} + J_t * (a_{t+1|T} - a_{t+1|t})\n",
        "        smoothed_states[t] = filtered_states[t] + J @ (smoothed_states[t+1] - predicted_states[t+1])\n",
        "        # P_{t|T} = P_{t|t} + J_t * (P_{t+1|T} - P_{t+1|t}) * J_t'\n",
        "        smoothed_covs[t] = filtered_covs[t] + J @ (smoothed_covs[t+1] - predicted_covs[t+1]) @ J.T\n",
        "\n",
        "    return {'smoothed_states': smoothed_states, 'smoothed_covs': smoothed_covs}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 5.1 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def implement_state_space_model() -> Dict[str, Callable]:\n",
        "    \"\"\"\n",
        "    Provides the core functions for state-space model implementation.\n",
        "\n",
        "    This function orchestrates Task 5.1 by returning a dictionary of the\n",
        "    essential, production-grade functions that form the building blocks of the\n",
        "    Kalman filter and smoother. This modular toolkit is used by the MCMC\n",
        "    estimation engine in Task 5.2.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Callable]: A dictionary containing the core state-space functions:\n",
        "                             - 'setup': Function to initialize model matrices.\n",
        "                             - 'filter': The Kalman forward filter.\n",
        "                             - 'smoother': The RTS backward smoother.\n",
        "    \"\"\"\n",
        "    print(\"--- Task 5.1: State-Space Model Implementation ---\")\n",
        "    print(\"Component functions for setup, filtering, and smoothing are defined and ready.\")\n",
        "\n",
        "    # Return the functions for use by the MCMC pipeline.\n",
        "    ss_toolkit = {\n",
        "        'setup': _setup_kalman_filter_matrices,\n",
        "        'filter': _kalman_forward_filter,\n",
        "        'smoother': _kalman_backward_smoother\n",
        "    }\n",
        "\n",
        "    print(\"--- Task 5.1: Completed Successfully ---\")\n",
        "\n",
        "    return ss_toolkit\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 5.2.1: GIBBS SAMPLER IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def _run_mcmc_chain(\n",
        "    country_data: pd.DataFrame,\n",
        "    mcmc_config: Dict[str, Any],\n",
        "    ffbs_func: Callable[..., np.ndarray]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Executes a single MCMC chain for the state-space model for one country.\n",
        "\n",
        "    This function implements a Gibbs sampler to draw from the posterior\n",
        "    distribution of the model parameters: the time-varying import elasticity\n",
        "    (pi_t), the price elasticity (eta), and the error variances. It serves as\n",
        "    the core worker function for the parallel MCMC engine.\n",
        "\n",
        "    Args:\n",
        "        country_data (pd.DataFrame): A DataFrame containing the necessary\n",
        "                                     time-series data for a single country.\n",
        "        mcmc_config (Dict[str, Any]): A dictionary containing MCMC settings\n",
        "                                      (iterations, burn-in) and prior specifications.\n",
        "        ffbs_func (Callable[..., np.ndarray]): The Forward-Filter, Backward-Sampler\n",
        "                                               function for drawing the state vector.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary where keys are parameter names and\n",
        "                               values are numpy arrays containing the posterior\n",
        "                               draws from this single chain.\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Data for Estimation ---\n",
        "    # Extract the observation vector y_t (HP-filtered imports).\n",
        "    # Measurement Equation: m_t^T = η*rer_t + π_t*y_t^T + ε_m\n",
        "    y = country_data['imports_const_lcu_trend'].values\n",
        "    # Extract the design matrix for the fixed coefficient eta (price elasticity).\n",
        "    X_eta = country_data[['reer_log_levels']].values\n",
        "    # Extract the design matrix for the time-varying coefficient pi_t (import elasticity).\n",
        "    X_pi = country_data[['gdp_const_lcu_trend']].values\n",
        "    # Get the number of time periods.\n",
        "    T = len(y)\n",
        "\n",
        "    # --- 2. Unpack Priors from Configuration ---\n",
        "    priors = mcmc_config['priors']\n",
        "    eta_prior_mu, eta_prior_var = priors['eta']['mu'], priors['eta']['sigma']**2\n",
        "    pi0_mu, pi0_var = priors['pi_0']['mu'], priors['pi_0']['sigma']**2\n",
        "    sm_alpha, sm_beta = priors['sigma_m']['alpha'], priors['sigma_m']['beta']\n",
        "    spi_alpha, spi_beta = priors['sigma_pi']['alpha'], priors['sigma_pi']['beta']\n",
        "\n",
        "    # --- 3. Initialize Parameters and State Vector ---\n",
        "    # Draw initial values for each parameter from its prior distribution.\n",
        "    eta = stats.norm.rvs(eta_prior_mu, np.sqrt(eta_prior_var))\n",
        "    sigma2_m = 1 / stats.gamma.rvs(sm_alpha, scale=1/sm_beta)\n",
        "    sigma2_pi = 1 / stats.gamma.rvs(spi_alpha, scale=1/spi_beta)\n",
        "    # Initialize the state vector at its prior mean.\n",
        "    pi_t = np.full(T, pi0_mu)\n",
        "\n",
        "    # --- 4. Set up MCMC Storage Arrays ---\n",
        "    total_iter = mcmc_config['mcmc_iterations']\n",
        "    burn_in = mcmc_config['burn_in_samples']\n",
        "    n_samples = total_iter - burn_in\n",
        "    eta_draws = np.zeros(n_samples)\n",
        "    sigma2_m_draws = np.zeros(n_samples)\n",
        "    sigma2_pi_draws = np.zeros(n_samples)\n",
        "    pi_t_draws = np.zeros((n_samples, T))\n",
        "\n",
        "    # --- 5. Main Gibbs Sampling Loop ---\n",
        "    for i in range(total_iter):\n",
        "        # --- Block 1: Sample the state vector pi_t using FFBS ---\n",
        "        # Adjust the observation vector by subtracting the effect of the fixed parameter.\n",
        "        y_star = y - (X_eta @ eta).flatten()\n",
        "        # Call the FFBS algorithm to get a new draw for the entire path of pi_t.\n",
        "        pi_t = ffbs_func(\n",
        "            y=y_star, X=X_pi, F=np.array([[1.0]]), Q=sigma2_pi, R=sigma2_m,\n",
        "            pi_0=pi0_mu, P_0=pi0_var\n",
        "        )\n",
        "\n",
        "        # --- Block 2: Sample eta (price elasticity) from its Normal posterior ---\n",
        "        # Adjust the observation vector by subtracting the effect of the time-varying state.\n",
        "        y_tilde = y - (X_pi * pi_t[:, np.newaxis]).flatten()\n",
        "        # Calculate posterior variance for eta using standard Bayesian linear regression formulas.\n",
        "        V_eta_inv = 1/eta_prior_var + (X_eta.T @ X_eta) / sigma2_m\n",
        "        V_eta = 1 / V_eta_inv\n",
        "        # Calculate posterior mean for eta.\n",
        "        m_eta = V_eta * (eta_prior_mu/eta_prior_var + (X_eta.T @ y_tilde) / sigma2_m)\n",
        "        # Draw eta from its full conditional posterior.\n",
        "        eta = stats.norm.rvs(m_eta.item(), np.sqrt(V_eta.item()))\n",
        "\n",
        "        # --- Block 3: Sample sigma2_m (measurement variance) from its Inverse-Gamma posterior ---\n",
        "        # Calculate the residuals from the measurement equation.\n",
        "        residuals_m = y - (X_eta @ eta).flatten() - (X_pi * pi_t[:, np.newaxis]).flatten()\n",
        "        # Calculate the posterior shape parameter for the Inverse-Gamma distribution.\n",
        "        sm_alpha_post = sm_alpha + T / 2\n",
        "        # Calculate the posterior scale parameter for the Inverse-Gamma distribution.\n",
        "        sm_beta_post = sm_beta + (residuals_m.T @ residuals_m).item() / 2\n",
        "        # Draw sigma2_m from its full conditional posterior.\n",
        "        sigma2_m = 1 / stats.gamma.rvs(sm_alpha_post, scale=1/sm_beta_post)\n",
        "\n",
        "        # --- Block 4: Sample sigma2_pi (state variance) from its Inverse-Gamma posterior ---\n",
        "        # Calculate the residuals from the state equation (a random walk).\n",
        "        residuals_pi = np.diff(pi_t, prepend=pi0_mu)\n",
        "        # Calculate the posterior shape parameter.\n",
        "        spi_alpha_post = spi_alpha + T / 2\n",
        "        # Calculate the posterior scale parameter.\n",
        "        spi_beta_post = spi_beta + (residuals_pi.T @ residuals_pi).item() / 2\n",
        "        # Draw sigma2_pi from its full conditional posterior.\n",
        "        sigma2_pi = 1 / stats.gamma.rvs(spi_alpha_post, scale=1/spi_beta_post)\n",
        "\n",
        "        # --- 6. Store Draws After Burn-in Period ---\n",
        "        if i >= burn_in:\n",
        "            # Calculate the storage index.\n",
        "            idx = i - burn_in\n",
        "            # Store the current draw for each parameter.\n",
        "            eta_draws[idx] = eta\n",
        "            sigma2_m_draws[idx] = sigma2_m\n",
        "            sigma2_pi_draws[idx] = sigma2_pi\n",
        "            pi_t_draws[idx, :] = pi_t\n",
        "\n",
        "    # Return a dictionary containing the full history of posterior draws for this chain.\n",
        "    return {\n",
        "        'eta': eta_draws, 'sigma2_m': sigma2_m_draws,\n",
        "        'sigma2_pi': sigma2_pi_draws, 'pi_t': pi_t_draws\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 5.2.2: CONVERGENCE DIAGNOSTICS IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_ess(pooled_draws: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the Effective Sample Size (ESS) for a set of MCMC draws.\n",
        "\n",
        "    ESS is a metric that quantifies the amount of independent information in an\n",
        "    autocorrelated sequence of MCMC samples. A low ESS indicates high\n",
        "    autocorrelation and less reliable posterior summaries.\n",
        "\n",
        "    Args:\n",
        "        pooled_draws (np.ndarray): A 2D numpy array of shape (total_samples, num_params)\n",
        "                                   containing the pooled draws from all MCMC chains.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D numpy array containing the ESS value for each parameter/column.\n",
        "    \"\"\"\n",
        "    # Get the total number of samples and the number of parameters.\n",
        "    total_samples, num_params = pooled_draws.shape\n",
        "\n",
        "    # Initialize an array to store the ESS for each parameter.\n",
        "    ess_values = np.zeros(num_params)\n",
        "\n",
        "    # Calculate ESS for each parameter column.\n",
        "    for i in range(num_params):\n",
        "        # Extract the chain for the current parameter.\n",
        "        chain = pooled_draws[:, i]\n",
        "\n",
        "        # Calculate the autocorrelation function (ACF) using statsmodels.\n",
        "        # nlags is chosen to be large enough to capture the decay.\n",
        "        autocorrelations = acf(chain, nlags=min(100, len(chain) // 5), fft=True)\n",
        "\n",
        "        # Use a robust heuristic to truncate the sum of autocorrelations.\n",
        "        # Sum positive pairs of autocorrelations (ρ_2k + ρ_{2k+1}).\n",
        "        sum_of_rhos = 0.0\n",
        "        for k in range(1, len(autocorrelations) - 1, 2):\n",
        "            rho_pair_sum = autocorrelations[k] + autocorrelations[k+1]\n",
        "            if rho_pair_sum > 0:\n",
        "                sum_of_rhos += rho_pair_sum\n",
        "            else:\n",
        "                # Stop when the ACF becomes noisy (negative pair sum).\n",
        "                break\n",
        "\n",
        "        # Calculate the Integrated Autocorrelation Time (τ).\n",
        "        # Equation: τ = 1 + 2 * Σ ρ_k\n",
        "        integrated_autocorr_time = 1 + 2 * sum_of_rhos\n",
        "\n",
        "        # Calculate the Effective Sample Size.\n",
        "        # Equation: ESS = N / τ\n",
        "        ess = total_samples / integrated_autocorr_time\n",
        "        ess_values[i] = ess\n",
        "\n",
        "    return ess_values\n",
        "\n",
        "\n",
        "def _assess_mcmc_convergence(\n",
        "    posterior_draws: Dict[str, np.ndarray]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Assesses MCMC convergence using both Gelman-Rubin and ESS diagnostics.\n",
        "\n",
        "    This function provides a comprehensive, two-factor assessment of MCMC chain\n",
        "    convergence, which is the professional standard for ensuring reliable\n",
        "    posterior inference. It checks:\n",
        "    1.  R-hat (Gelman-Rubin): Whether multiple chains have converged to the same\n",
        "        target distribution (R-hat < 1.1).\n",
        "    2.  Effective Sample Size (ESS): Whether the chains contain a sufficient\n",
        "        amount of independent information (ESS > 400).\n",
        "\n",
        "    Args:\n",
        "        posterior_draws (Dict[str, np.ndarray]): A dictionary of posterior draws,\n",
        "            where each value is an array with the first dimension representing\n",
        "            the MCMC chain index.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the calculated R-hat and ESS\n",
        "                        values for each parameter, and an overall boolean\n",
        "                        convergence status.\n",
        "    \"\"\"\n",
        "    # Initialize dictionaries to store diagnostic results.\n",
        "    r_hat_values = {}\n",
        "    ess_values = {}\n",
        "    # Flag to track overall convergence, starting with an optimistic assumption.\n",
        "    all_converged = True\n",
        "\n",
        "    # Iterate through each parameter's posterior draws.\n",
        "    for param_name, draws in posterior_draws.items():\n",
        "        # --- 1. R-hat (Gelman-Rubin) Diagnostic ---\n",
        "        # R-hat requires at least two chains for comparison.\n",
        "        if draws.ndim < 2 or draws.shape[0] < 2:\n",
        "            r_hat_values[param_name] = np.ones_like(np.mean(draws, axis=(0,1)))\n",
        "        else:\n",
        "            # Get the number of chains (M) and samples per chain (N).\n",
        "            num_chains, num_samples = draws.shape[0], draws.shape[1]\n",
        "            # Calculate within-chain variance (W).\n",
        "            within_chain_variance = np.mean(np.var(draws, axis=1, ddof=1), axis=0)\n",
        "            # Calculate between-chain variance (B).\n",
        "            between_chain_variance = num_samples * np.var(np.mean(draws, axis=1), axis=0, ddof=1)\n",
        "            # Estimate the marginal posterior variance (Var+).\n",
        "            variance_plus = ((num_samples - 1) / num_samples * within_chain_variance +\n",
        "                             between_chain_variance / num_samples)\n",
        "            # Compute R-hat, adding a small epsilon for numerical stability.\n",
        "            r_hat = np.sqrt(variance_plus / (within_chain_variance + 1e-10))\n",
        "            r_hat_values[param_name] = r_hat\n",
        "\n",
        "        # --- 2. Effective Sample Size (ESS) Diagnostic ---\n",
        "        # Pool draws from all chains into a single long sample.\n",
        "        # Reshape e.g., (chains, samples, T) -> (chains * samples, T)\n",
        "        pooled_draws = draws.reshape(-1, *draws.shape[2:])\n",
        "        if pooled_draws.ndim == 1: # Ensure it's a 2D array for the helper\n",
        "            pooled_draws = pooled_draws[:, np.newaxis]\n",
        "        # Calculate ESS for the pooled chain(s).\n",
        "        ess = _calculate_ess(pooled_draws)\n",
        "        ess_values[param_name] = ess\n",
        "\n",
        "        # --- 3. Update Overall Convergence Status ---\n",
        "        # Check if any R-hat value exceeds the 1.1 threshold.\n",
        "        if np.any(r_hat_values[param_name] > 1.1):\n",
        "            all_converged = False\n",
        "        # Check if any ESS value is below the 400 threshold.\n",
        "        if np.any(ess_values[param_name] < 400):\n",
        "            all_converged = False\n",
        "\n",
        "    # --- 4. Assemble the Final Report ---\n",
        "    return {\n",
        "        'r_hat_values': r_hat_values,\n",
        "        'ess_values': ess_values,\n",
        "        'all_converged': all_converged,\n",
        "        'convergence_criteria': {\n",
        "            'r_hat_threshold': 1.1,\n",
        "            'ess_threshold': 400\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 5.2.3: POSTERIOR SUMMARY AND INFERENCE\n",
        "# =============================================================================\n",
        "\n",
        "def _summarize_posterior_draws(\n",
        "    posterior_draws: Dict[str, np.ndarray],\n",
        "    time_index: pd.Index\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Summarizes the posterior distributions of the model parameters.\n",
        "\n",
        "    This function takes the raw MCMC chain output, pools the draws from all\n",
        "    chains, and computes key summary statistics: mean, median, standard\n",
        "    deviation, and a 90% credible interval (from the 5th to 95th percentiles).\n",
        "\n",
        "    Args:\n",
        "        posterior_draws (Dict[str, np.ndarray]): The dictionary of raw MCMC draws.\n",
        "        time_index (pd.Index): The time index (years) corresponding to the\n",
        "                               time-varying parameter pi_t.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary where keys are parameter names\n",
        "                                 and values are DataFrames containing the\n",
        "                                 posterior summary statistics.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the summary DataFrames.\n",
        "    posterior_summaries = {}\n",
        "\n",
        "    # Iterate through each parameter's draws.\n",
        "    for param_name, draws in posterior_draws.items():\n",
        "        # --- 1. Pool Draws from All Chains ---\n",
        "        # Reshape the array to combine all chains into a single long sample.\n",
        "        # e.g., (num_chains, num_samples, T) -> (num_chains * num_samples, T)\n",
        "        num_chains, num_samples = draws.shape[0], draws.shape[1]\n",
        "        pooled_draws = draws.reshape(num_chains * num_samples, -1)\n",
        "        # Squeeze removes dimensions of size 1 for scalar parameters.\n",
        "        pooled_draws = np.squeeze(pooled_draws)\n",
        "\n",
        "        # --- 2. Compute Summary Statistics ---\n",
        "        # Calculate posterior mean, median, std, and percentiles along the sample axis (axis=0).\n",
        "        summary_data = {\n",
        "            'mean': np.mean(pooled_draws, axis=0),\n",
        "            'median': np.median(pooled_draws, axis=0),\n",
        "            'std_dev': np.std(pooled_draws, axis=0),\n",
        "            'ci_5pct': np.percentile(pooled_draws, 5, axis=0),\n",
        "            'ci_95pct': np.percentile(pooled_draws, 95, axis=0)\n",
        "        }\n",
        "\n",
        "        # --- 3. Structure the Output ---\n",
        "        # For the time-varying parameter pi_t, the index should be the time index.\n",
        "        if param_name == 'pi_t':\n",
        "            summary_df = pd.DataFrame(summary_data, index=time_index)\n",
        "        else:\n",
        "            # For scalar parameters, the index is simply the parameter name.\n",
        "            summary_df = pd.DataFrame(summary_data, index=[param_name])\n",
        "\n",
        "        posterior_summaries[param_name] = summary_df\n",
        "\n",
        "    return posterior_summaries\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 5.2 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_mcmc_analysis(\n",
        "    country_data: pd.DataFrame,\n",
        "    mcmc_config: Dict[str, Any],\n",
        "    ffbs_func: Callable[..., np.ndarray]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire MCMC analysis pipeline for a single country.\n",
        "\n",
        "    This function manages the workflow for Task 5.2. It runs multiple MCMC chains\n",
        "    in parallel, performs convergence diagnostics on the raw output, and if\n",
        "    convergence is achieved, it computes and returns the final posterior summary\n",
        "    statistics.\n",
        "\n",
        "    Args:\n",
        "        country_data (pd.DataFrame): The time-series data for a single country.\n",
        "        mcmc_config (Dict[str, Any]): Configuration for the MCMC estimation.\n",
        "        ffbs_func (Callable[..., np.ndarray]): The Forward-Filter, Backward-Sampler function.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing the raw posterior\n",
        "                        draws, a convergence report, and (if converged) a\n",
        "                        summary of the posterior distributions.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting Task 5.2: MCMC Estimation Engine Analysis for country ---\")\n",
        "\n",
        "    # --- Step 1: Run Multiple MCMC Chains in Parallel ---\n",
        "    # This step coordinates the execution of the Gibbs sampler (_run_mcmc_chain).\n",
        "    print(f\"  - Running {mcmc_config['num_chains']} MCMC chains in parallel...\")\n",
        "    chain_results = Parallel(n_jobs=-1)(\n",
        "        delayed(_run_mcmc_chain)(country_data, mcmc_config, ffbs_func)\n",
        "        for _ in range(mcmc_config['num_chains'])\n",
        "    )\n",
        "\n",
        "    # Aggregate results from all chains into a single dictionary of arrays.\n",
        "    posterior_draws = {\n",
        "        param: np.stack([res[param] for res in chain_results], axis=0)\n",
        "        for param in chain_results[0].keys()\n",
        "    }\n",
        "    print(\"  - MCMC sampling complete.\")\n",
        "\n",
        "    # --- Step 2: Assess MCMC Convergence ---\n",
        "    print(\"  - Assessing chain convergence...\")\n",
        "    convergence_report = _assess_mcmc_convergence(posterior_draws)\n",
        "\n",
        "    # --- Step 3: Summarize Posterior (if converged) ---\n",
        "    posterior_summary = None\n",
        "    if convergence_report['all_converged']:\n",
        "        print(\"  - Convergence successful. Summarizing posterior distributions...\")\n",
        "        # Get the time index for the pi_t parameter from the input data.\n",
        "        time_index = country_data.index\n",
        "        # Compute the summary statistics.\n",
        "        posterior_summary = _summarize_posterior_draws(posterior_draws, time_index)\n",
        "        print(\"  - Posterior summarization complete.\")\n",
        "    else:\n",
        "        # If convergence failed, issue a clear warning.\n",
        "        print(\"  - WARNING: MCMC chains did not fully converge. Posterior summaries will not be generated.\")\n",
        "        failing_params = [p for p, r in convergence_report['r_hat_values'].items() if np.any(r > 1.1)]\n",
        "        print(f\"    Check R-hat values for failing parameters: {failing_params}\")\n",
        "\n",
        "    # --- Step 4: Assemble Final Report ---\n",
        "    final_report = {\n",
        "        'raw_posterior_draws': posterior_draws,\n",
        "        'convergence_report': convergence_report,\n",
        "        'posterior_summary': posterior_summary # This will be None if convergence failed\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Task 5.2: MCMC Analysis Completed ---\")\n",
        "    return final_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 5.3.1: POSTERIOR PREDICTIVE TRADE MULTIPLIER COMPUTATION\n",
        "# =============================================================================\n",
        "\n",
        "def _compute_posterior_trade_multiplier(\n",
        "    mcmc_analysis_results: Dict[str, Any],\n",
        "    analysis_ready_data: Dict[str, pd.DataFrame]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Computes the final trade multiplier estimates with full uncertainty quantification.\n",
        "\n",
        "    This function uses the summarized posterior distributions of the time-varying\n",
        "    import elasticity (pi_t) to calculate the posterior mean and 90% credible\n",
        "    interval for the dynamic trade multiplier for each country.\n",
        "\n",
        "    Args:\n",
        "        mcmc_analysis_results (Dict[str, Any]): The output from the MCMC analysis\n",
        "            orchestrator, containing the 'posterior_summary' for each country.\n",
        "        analysis_ready_data (Dict[str, pd.DataFrame]): The dictionary of\n",
        "            preprocessed data, needed for the HP-filtered export growth series.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary mapping each country to a DataFrame\n",
        "                                 of its final trade multiplier time-series estimates.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the final multiplier DataFrames.\n",
        "    final_multiplier_estimates = {}\n",
        "\n",
        "    # Iterate through each country in the MCMC results.\n",
        "    for country, results in mcmc_analysis_results.items():\n",
        "        # Proceed only if the posterior summary exists (i.e., MCMC converged).\n",
        "        if 'posterior_summary' not in results or results['posterior_summary'] is None:\n",
        "            continue\n",
        "\n",
        "        # --- 1. Extract Posterior Summaries for pi_t ---\n",
        "        pi_t_summary = results['posterior_summary']['pi_t']\n",
        "\n",
        "        # --- 2. Extract and Align Export Growth Data ---\n",
        "        export_growth_series = analysis_ready_data['macro_growth']['exports_growth'].loc[country]\n",
        "\n",
        "        # Align the two series to ensure they cover the exact same time periods.\n",
        "        pi_t_aligned, export_growth_aligned = pi_t_summary.align(\n",
        "            export_growth_series, join='inner', axis=0\n",
        "        )\n",
        "\n",
        "        # --- 3. Compute the Trade Multiplier and Credible Interval ---\n",
        "        # Equation (A.1): Δy_t^{BP} = Δz_t^T / π_t\n",
        "        # Point estimate using the posterior mean of pi_t.\n",
        "        mean_multiplier = export_growth_aligned / pi_t_aligned['mean']\n",
        "\n",
        "        # Calculate the credible interval for the multiplier.\n",
        "        # Note the inversion: lower bound of pi -> upper bound of multiplier.\n",
        "        upper_ci = export_growth_aligned / pi_t_aligned['ci_5pct']\n",
        "        lower_ci = export_growth_aligned / pi_t_aligned['ci_95pct']\n",
        "\n",
        "        # --- 4. Assemble the Results DataFrame ---\n",
        "        final_df = pd.DataFrame({\n",
        "            'trade_multiplier_mean': mean_multiplier,\n",
        "            'lower_ci_90pct': lower_ci,\n",
        "            'upper_ci_90pct': upper_ci\n",
        "        }, index=pi_t_aligned.index)\n",
        "\n",
        "        final_multiplier_estimates[country] = final_df\n",
        "\n",
        "    return final_multiplier_estimates\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 5.3.2: MODEL VALIDATION AND GOODNESS OF FIT\n",
        "# =============================================================================\n",
        "\n",
        "def _validate_trade_multiplier_model(\n",
        "    final_multiplier_estimates: Dict[str, pd.DataFrame],\n",
        "    analysis_ready_data: Dict[str, pd.DataFrame]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validates the trade multiplier model's performance against actual GDP growth.\n",
        "\n",
        "    This function assesses goodness-of-fit using standard metrics (R², RMSE, MAE)\n",
        "    and performs diagnostic checks on the model's residuals to check for\n",
        "    unexplained patterns (autocorrelation, non-normality).\n",
        "\n",
        "    Args:\n",
        "        final_multiplier_estimates (Dict[str, pd.DataFrame]): The final trade\n",
        "            multiplier estimates with credible intervals.\n",
        "        analysis_ready_data (Dict[str, pd.DataFrame]): Preprocessed data, needed\n",
        "            for the actual GDP growth series.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary by country, containing reports on\n",
        "                                   goodness-of-fit and residual diagnostics.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store validation results.\n",
        "    validation_results = {}\n",
        "\n",
        "    # Iterate through each country with multiplier estimates.\n",
        "    for country, estimates_df in final_multiplier_estimates.items():\n",
        "        # --- 1. Prepare Actual and Predicted Series ---\n",
        "        actual_growth = analysis_ready_data['macro_growth']['gdp_growth'].loc[country]\n",
        "        predicted_growth = estimates_df['trade_multiplier_mean']\n",
        "\n",
        "        # Align series and drop any remaining NaNs.\n",
        "        y_true, y_pred = actual_growth.align(predicted_growth, join='inner', axis=0)\n",
        "        y_true.dropna(inplace=True)\n",
        "        y_pred.dropna(inplace=True)\n",
        "        y_true, y_pred = y_true.align(y_pred, join='inner', axis=0)\n",
        "\n",
        "        if y_true.empty:\n",
        "            continue\n",
        "\n",
        "        # --- 2. Calculate Goodness-of-Fit Metrics ---\n",
        "        gof_metrics = {\n",
        "            'r_squared': r2_score(y_true, y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "            'mae': mean_absolute_error(y_true, y_pred)\n",
        "        }\n",
        "\n",
        "        # --- 3. Perform Residual Diagnostics ---\n",
        "        # Calculate the residual series.\n",
        "        residuals = y_true - y_pred\n",
        "\n",
        "        # Ljung-Box test for autocorrelation in residuals.\n",
        "        lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
        "\n",
        "        # Anderson-Darling test for normality of residuals.\n",
        "        ad_test = stats.anderson(residuals, dist='norm')\n",
        "        ad_critical_5pct = ad_test.critical_values[2]\n",
        "\n",
        "        residual_diagnostics = {\n",
        "            'ljung_box_p_value_lag10': lb_test['lb_pvalue'].iloc[0],\n",
        "            'ad_statistic': ad_test.statistic,\n",
        "            'ad_reject_normality_5pct': ad_test.statistic > ad_critical_5pct\n",
        "        }\n",
        "\n",
        "        # --- 4. Assemble Report ---\n",
        "        validation_results[country] = {\n",
        "            'goodness_of_fit_metrics': gof_metrics,\n",
        "            'residual_diagnostics': residual_diagnostics\n",
        "        }\n",
        "\n",
        "    return validation_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 5.3.3: CROSS-COUNTRY CONSISTENCY ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def _analyze_cross_country_consistency(\n",
        "    mcmc_analysis_results: Dict[str, Any],\n",
        "    validation_results: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Summarizes and compares estimation and validation results across countries.\n",
        "\n",
        "    This function aggregates key parameter estimates and performance metrics\n",
        "    into summary tables, facilitating a high-level comparison of the model's\n",
        "    consistency and performance across the different economies in the sample.\n",
        "\n",
        "    Args:\n",
        "        mcmc_analysis_results (Dict[str, Any]): The output from the MCMC analysis.\n",
        "        validation_results (Dict[str, Any]): The output from the model validation.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing summary DataFrames for\n",
        "                                 key parameters and performance metrics.\n",
        "    \"\"\"\n",
        "    # Initialize lists to build summary DataFrames.\n",
        "    param_summary_list = []\n",
        "    perf_summary_list = []\n",
        "\n",
        "    # Iterate through each country.\n",
        "    for country in mcmc_analysis_results.keys():\n",
        "        # --- 1. Aggregate Parameter Summaries ---\n",
        "        if mcmc_analysis_results[country].get('posterior_summary'):\n",
        "            summary = mcmc_analysis_results[country]['posterior_summary']\n",
        "            param_summary_list.append({\n",
        "                'country_iso': country,\n",
        "                'eta_mean': summary['eta']['mean'].iloc[0],\n",
        "                'avg_pi_t_mean': summary['pi_t']['mean'].mean(),\n",
        "                'sigma2_m_mean': summary['sigma2_m']['mean'].iloc[0],\n",
        "                'sigma2_pi_mean': summary['sigma2_pi']['mean'].iloc[0]\n",
        "            })\n",
        "\n",
        "        # --- 2. Aggregate Performance Metrics ---\n",
        "        if country in validation_results:\n",
        "            perf = validation_results[country]['goodness_of_fit_metrics']\n",
        "            perf['country_iso'] = country\n",
        "            perf_summary_list.append(perf)\n",
        "\n",
        "    # --- 3. Create Summary DataFrames ---\n",
        "    param_summary_df = pd.DataFrame(param_summary_list).set_index('country_iso')\n",
        "    perf_summary_df = pd.DataFrame(perf_summary_list).set_index('country_iso')\n",
        "\n",
        "    return {\n",
        "        'parameter_summary': param_summary_df,\n",
        "        'performance_summary': perf_summary_df\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 5 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_bayesian_estimation(\n",
        "    analysis_ready_data: Dict[str, pd.DataFrame],\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire Bayesian estimation and validation pipeline (Task 5).\n",
        "\n",
        "    This master function manages the full workflow, from running the core MCMC\n",
        "    estimation for each country to validating the final trade multiplier model\n",
        "    and summarizing the results in a cross-country analysis.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_data (Dict[str, pd.DataFrame]): Preprocessed data.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all results from\n",
        "                        the Bayesian estimation and validation phase.\n",
        "    \"\"\"\n",
        "    print(\"========== Starting Task 5: Bayesian Econometric Estimation ==========\")\n",
        "\n",
        "    # --- Task 5.1 & 5.2 (Combined Execution) ---\n",
        "    # The core estimation and MCMC analysis are tightly coupled.\n",
        "    # We will run them for each country and store the results.\n",
        "\n",
        "    # Retrieve necessary functions and configurations.\n",
        "    mcmc_config = master_config['empirical_analysis']['parameters']['bayesian_model']\n",
        "    ffbs_func = _forward_filter_backward_sampler # From previous response\n",
        "\n",
        "    # Prepare the combined dataset needed for estimation.\n",
        "    hp_trends = _apply_hp_filter(\n",
        "        analysis_ready_data['macro_log_levels'][['gdp_const_lcu', 'imports_const_lcu']],\n",
        "        master_config['empirical_analysis']['parameters']['hp_filter']['lambda']\n",
        "    )\n",
        "    estimation_data = hp_trends.join(analysis_ready_data['macro_log_levels'][['reer']])\n",
        "    estimation_data.rename(columns={'reer': 'reer_log_levels'}, inplace=True)\n",
        "\n",
        "    full_mcmc_results = {}\n",
        "    for country in estimation_data.index.get_level_values('country_iso').unique():\n",
        "        country_data = estimation_data.loc[country].dropna()\n",
        "        if country_data.empty:\n",
        "            continue\n",
        "\n",
        "        # Run the full MCMC analysis (sampling, convergence, summary).\n",
        "        full_mcmc_results[country] = orchestrate_mcmc_analysis(\n",
        "            country_data, mcmc_config, ffbs_func\n",
        "        )\n",
        "\n",
        "    # --- Task 5.3: Estimation and Validation ---\n",
        "    print(\"\\n--- Starting Task 5.3: Trade Multiplier Estimation and Validation ---\")\n",
        "\n",
        "    # Sub-step 5.3.1: Compute final multiplier estimates with uncertainty.\n",
        "    final_estimates = _compute_posterior_trade_multiplier(\n",
        "        full_mcmc_results, analysis_ready_data\n",
        "    )\n",
        "\n",
        "    # Sub-step 5.3.2: Validate the model's goodness-of-fit.\n",
        "    validation_report = _validate_trade_multiplier_model(\n",
        "        final_estimates, analysis_ready_data\n",
        "    )\n",
        "\n",
        "    # Sub-step 5.3.3: Perform cross-country consistency analysis.\n",
        "    consistency_report = _analyze_cross_country_consistency(\n",
        "        full_mcmc_results, validation_report\n",
        "    )\n",
        "\n",
        "    # --- Final Aggregation ---\n",
        "    master_estimation_report = {\n",
        "        'country_level_mcmc_analysis': full_mcmc_results,\n",
        "        'final_multiplier_estimates': final_estimates,\n",
        "        'model_validation_report': validation_report,\n",
        "        'cross_country_summary': consistency_report\n",
        "    }\n",
        "\n",
        "    print(\"\\n========== Task 5: Bayesian Estimation Pipeline Completed Successfully ==========\")\n",
        "    return master_estimation_report\n"
      ],
      "metadata": {
        "id": "XqZSesE_6A9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK VI: STATISTICAL VALIDATION AND DISTRIBUTIONAL ANALYSIS\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 6.1.1: ANDERSON-DARLING TEST FOR MULTIPLE SERIES\n",
        "# =============================================================================\n",
        "\n",
        "def comprehensive_anderson_darling_tests(\n",
        "    data_dict: Dict[str, np.ndarray],\n",
        "    alpha_level: float = 0.05\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs Anderson-Darling normality tests on multiple series with FDR correction.\n",
        "\n",
        "    This function iterates through a dictionary of time series, performs the\n",
        "    Anderson-Darling test for normality on each, and provides a comprehensive\n",
        "    report. Crucially, it includes an interpolated p-value for easier\n",
        "    interpretation and applies the Benjamini-Hochberg procedure to control the\n",
        "    False Discovery Rate (FDR) across the multiple hypotheses being tested.\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary where keys are series names\n",
        "            and values are 1D numpy arrays of the data to be tested.\n",
        "        alpha_level (float): The nominal significance level for the tests.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by series name, reporting the test\n",
        "                      statistic, interpolated p-value, and the FDR-adjusted p-value.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store the results of each individual test.\n",
        "    results_list = []\n",
        "\n",
        "    # Iterate through each series provided in the input dictionary.\n",
        "    for name, series in data_dict.items():\n",
        "        # --- Data Preparation ---\n",
        "        # Ensure the series is a numpy array and remove any NaN values.\n",
        "        clean_series = np.asarray(series)\n",
        "        clean_series = clean_series[~np.isnan(clean_series)]\n",
        "\n",
        "        # The test requires a minimum number of samples.\n",
        "        if len(clean_series) < 8:\n",
        "            # If the series is too short, record a NaN result and skip.\n",
        "            results_list.append({'series_name': name, 'statistic': np.nan, 'p_value': np.nan})\n",
        "            continue\n",
        "\n",
        "        # --- Test Execution ---\n",
        "        # Perform the Anderson-Darling test against a normal distribution.\n",
        "        ad_result = stats.anderson(clean_series, dist='norm')\n",
        "\n",
        "        # --- P-value Interpolation ---\n",
        "        # `scipy.stats.anderson` returns critical values, not a p-value. We interpolate.\n",
        "        # The significance levels provided by scipy are [15%, 10%, 5%, 2.5%, 1%].\n",
        "        sig_levels = np.array([0.15, 0.10, 0.05, 0.025, 0.01])\n",
        "        # Linearly interpolate the p-value from the statistic and critical values.\n",
        "        p_value = np.interp(ad_result.statistic, ad_result.critical_values, sig_levels)\n",
        "\n",
        "        # Handle cases where the statistic is outside the range of critical values.\n",
        "        if ad_result.statistic > ad_result.critical_values[-1]:\n",
        "            p_value = sig_levels[-1]\n",
        "        elif ad_result.statistic < ad_result.critical_values[0]:\n",
        "            p_value = sig_levels[0]\n",
        "\n",
        "        # Append the raw test results to the list.\n",
        "        results_list.append({'series_name': name, 'statistic': ad_result.statistic, 'p_value': p_value})\n",
        "\n",
        "    # Convert the list of results into a DataFrame.\n",
        "    results_df = pd.DataFrame(results_list).set_index('series_name')\n",
        "\n",
        "    # --- Multiple Testing Correction ---\n",
        "    # Apply the Benjamini-Hochberg FDR correction to the raw p-values.\n",
        "    if not results_df['p_value'].dropna().empty:\n",
        "        # `fdrcorrection` returns a boolean array of rejections and the adjusted p-values.\n",
        "        rejected, p_adjusted = fdrcorrection(results_df['p_value'].dropna(), alpha=alpha_level)\n",
        "        # Add the adjusted p-values to the results DataFrame.\n",
        "        results_df.loc[results_df['p_value'].notna(), 'p_value_adjusted_fdr'] = p_adjusted\n",
        "        results_df.loc[results_df['p_value'].notna(), 'reject_null_fdr'] = rejected\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 6.1.2: QQ PLOT ANALYSIS AND TAIL BEHAVIOR ASSESSMENT\n",
        "# =============================================================================\n",
        "\n",
        "def comprehensive_qq_analysis(\n",
        "    data_dict: Dict[str, np.ndarray]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs a comprehensive QQ plot analysis and quantifies tail behavior.\n",
        "\n",
        "    This function goes beyond simple plot generation. For each series, it\n",
        "    calculates the data needed for a QQ plot against a normal distribution,\n",
        "    quantifies the goodness-of-fit of the plot's linear trend (R-squared),\n",
        "    and computes metrics to specifically measure the deviation in the tails,\n",
        "    along with the overall excess kurtosis (fat-tailedness).\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary of 1D numpy arrays to analyze.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary where each key is a series name,\n",
        "            and the value is a nested dictionary of analysis results.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the analysis for all series.\n",
        "    analysis_results = {}\n",
        "\n",
        "    for name, series in data_dict.items():\n",
        "        # --- Data Preparation ---\n",
        "        clean_series = np.asarray(series)\n",
        "        clean_series = clean_series[~np.isnan(clean_series)]\n",
        "        if len(clean_series) < 10:\n",
        "            continue\n",
        "\n",
        "        # --- 1. QQ Plot Data Generation ---\n",
        "        # `probplot` generates sample and theoretical quantiles.\n",
        "        (sample_quantiles, theoretical_quantiles), (slope, intercept, r_squared) = stats.probplot(clean_series, dist=\"norm\", fit=True)\n",
        "\n",
        "        # --- 2. Tail Behavior Quantification ---\n",
        "        # Define the tail region as the bottom and top 10% of the data.\n",
        "        tail_size = int(len(clean_series) * 0.10)\n",
        "        # Calculate the mean absolute deviation in the lower tail.\n",
        "        lower_tail_dev = np.mean(np.abs(sample_quantiles[:tail_size] - theoretical_quantiles[:tail_size]))\n",
        "        # Calculate the mean absolute deviation in the upper tail.\n",
        "        upper_tail_dev = np.mean(np.abs(sample_quantiles[-tail_size:] - theoretical_quantiles[-tail_size:]))\n",
        "\n",
        "        # --- 3. Fat Tail Assessment ---\n",
        "        # Calculate Fisher's kurtosis (excess kurtosis). Positive values indicate fat tails.\n",
        "        excess_kurt = stats.kurtosis(clean_series, fisher=True)\n",
        "\n",
        "        # --- 4. Assemble Report ---\n",
        "        analysis_results[name] = {\n",
        "            'goodness_of_fit_r2': r_squared,\n",
        "            'lower_tail_deviation': lower_tail_dev,\n",
        "            'upper_tail_deviation': upper_tail_dev,\n",
        "            'excess_kurtosis': excess_kurt,\n",
        "            'plot_data': {\n",
        "                'sample_quantiles': sample_quantiles,\n",
        "                'theoretical_quantiles': theoretical_quantiles\n",
        "            }\n",
        "        }\n",
        "    return analysis_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 6.1.3: ADDITIONAL DISTRIBUTIONAL TESTS\n",
        "# =============================================================================\n",
        "\n",
        "def additional_distributional_tests(\n",
        "    data_dict: Dict[str, np.ndarray]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a battery of standard normality tests on multiple series.\n",
        "\n",
        "    This function provides a suite of complementary tests to the Anderson-Darling\n",
        "    test, each sensitive to different types of deviations from normality:\n",
        "    - Kolmogorov-Smirnov: Sensitive to deviations in the center of the distribution.\n",
        "    - Shapiro-Wilk: A powerful all-around test, especially for smaller samples.\n",
        "    - Jarque-Bera: Specifically tests for joint normality of skewness and kurtosis.\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary of 1D numpy arrays to analyze.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by series name, reporting the test\n",
        "                      statistic and p-value for each of the three tests.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store results.\n",
        "    results_list = []\n",
        "\n",
        "    for name, series in data_dict.items():\n",
        "        # --- Data Preparation ---\n",
        "        clean_series = np.asarray(series)\n",
        "        clean_series = clean_series[~np.isnan(clean_series)]\n",
        "\n",
        "        # Standardize the series (z-score) for the K-S test.\n",
        "        if len(clean_series) > 1:\n",
        "            std_series = (clean_series - np.mean(clean_series)) / np.std(clean_series)\n",
        "        else:\n",
        "            std_series = clean_series\n",
        "\n",
        "        # --- Test Execution ---\n",
        "        # Initialize a dictionary for this series' results.\n",
        "        res = {'series_name': name}\n",
        "\n",
        "        # Kolmogorov-Smirnov test (requires standardized data).\n",
        "        if len(std_series) > 1:\n",
        "            ks_stat, ks_p = stats.kstest(std_series, 'norm')\n",
        "            res.update({'ks_statistic': ks_stat, 'ks_pvalue': ks_p})\n",
        "\n",
        "        # Shapiro-Wilk test (best for n < 5000).\n",
        "        if 3 <= len(clean_series) < 5000:\n",
        "            shapiro_stat, shapiro_p = stats.shapiro(clean_series)\n",
        "            res.update({'shapiro_statistic': shapiro_stat, 'shapiro_pvalue': shapiro_p})\n",
        "\n",
        "        # Jarque-Bera test.\n",
        "        if len(clean_series) > 20: # More reliable with larger samples\n",
        "            jb_stat, jb_p = stats.jarque_bera(clean_series)\n",
        "            res.update({'jarque_bera_statistic': jb_stat, 'jarque_bera_pvalue': jb_p})\n",
        "\n",
        "        results_list.append(res)\n",
        "\n",
        "    # Convert the list of results into a DataFrame.\n",
        "    return pd.DataFrame(results_list).set_index('series_name')\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 6.1 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_distributional_analysis(\n",
        "    data_dict: Dict[str, np.ndarray],\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive distributional analysis for multiple data series.\n",
        "\n",
        "    This master function runs a full suite of distributional tests and analyses,\n",
        "    providing a multi-faceted, rigorous assessment of the statistical properties\n",
        "    of the input data series, with a focus on testing for normality and\n",
        "    characterizing non-normal features like fat tails.\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary where keys are series names\n",
        "            and values are the 1D numpy arrays of data to be analyzed.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive report containing the results from all\n",
        "                        distributional analysis components.\n",
        "    \"\"\"\n",
        "    print(\"========== Starting Task 6.1: Distributional Testing Implementation ==========\")\n",
        "\n",
        "    # --- Step 1: Run Comprehensive Anderson-Darling Tests ---\n",
        "    print(\"  - Running Anderson-Darling tests with FDR correction...\")\n",
        "    ad_results = comprehensive_anderson_darling_tests(\n",
        "        data_dict,\n",
        "        master_config['empirical_analysis']['parameters']['statistical_tests']['alpha_level']\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Run Comprehensive QQ Plot and Tail Analysis ---\n",
        "    print(\"  - Running QQ plot analysis and quantifying tail behavior...\")\n",
        "    qq_results = comprehensive_qq_analysis(data_dict)\n",
        "\n",
        "    # --- Step 3: Run Additional Normality Tests ---\n",
        "    print(\"  - Running supplementary normality tests (K-S, Shapiro, Jarque-Bera)...\")\n",
        "    additional_tests_results = additional_distributional_tests(data_dict)\n",
        "\n",
        "    # --- Step 4: Assemble Final Report ---\n",
        "    final_report = {\n",
        "        'anderson_darling_tests': ad_results,\n",
        "        'qq_and_tail_analysis': qq_results,\n",
        "        'additional_normality_tests': additional_tests_results\n",
        "    }\n",
        "\n",
        "    print(\"\\n========== Task 6.1: Distributional Analysis Completed Successfully ==========\")\n",
        "    return final_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 6.2.1: HILL ESTIMATOR AND TAIL INDEX CALCULATION\n",
        "# =============================================================================\n",
        "\n",
        "def estimate_tail_indices(\n",
        "    data_dict: Dict[str, np.ndarray]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Estimates the tail index for multiple series using the Hill estimator.\n",
        "\n",
        "    This function analyzes the right tail of each distribution to estimate the\n",
        "    tail index (xi), a key parameter of power-law behavior. It provides a\n",
        "    default estimate and the data necessary to generate a Hill plot, which is a\n",
        "    critical diagnostic for choosing the number of order statistics (k) to use.\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary where keys are series names\n",
        "            and values are 1D numpy arrays of the data. For two-tailed\n",
        "            distributions like returns, data should be absolute values.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary of reports, one for each series,\n",
        "            containing the default tail index estimate, its confidence interval,\n",
        "            and the data for constructing a Hill plot.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the analysis for all series.\n",
        "    analysis_results = {}\n",
        "\n",
        "    for name, series in data_dict.items():\n",
        "        # --- Data Preparation ---\n",
        "        clean_series = np.asarray(series)\n",
        "        clean_series = clean_series[~np.isnan(clean_series) & (clean_series > 0)]\n",
        "        if len(clean_series) < 20:\n",
        "            continue\n",
        "\n",
        "        # Sort the data in ascending order to get the order statistics.\n",
        "        sorted_series = np.sort(clean_series)\n",
        "        n = len(sorted_series)\n",
        "\n",
        "        # --- 1. Generate Hill Plot Data ---\n",
        "        # Calculate the Hill estimate for a range of k values.\n",
        "        k_values = np.arange(10, int(n * 0.1), 5)\n",
        "        hill_estimates = []\n",
        "        for k in k_values:\n",
        "            if k == 0: continue\n",
        "            # Hill Estimator Equation: ξ̂ = (1/k) * Σ_{i=1 to k} [log(X_{n-i+1}) - log(X_{n-k})]\n",
        "            log_order_stats = np.log(sorted_series[-k:])\n",
        "            hill_est = np.mean(log_order_stats) - np.log(sorted_series[-k-1])\n",
        "            hill_estimates.append(hill_est)\n",
        "\n",
        "        hill_plot_data = pd.DataFrame({'k': k_values, 'tail_index_xi': hill_estimates})\n",
        "\n",
        "        # --- 2. Calculate Default Estimate and Confidence Interval ---\n",
        "        # Use a common heuristic for k.\n",
        "        k_default = int(np.sqrt(n))\n",
        "        log_order_stats_def = np.log(sorted_series[-k_default:])\n",
        "        default_xi = np.mean(log_order_stats_def) - np.log(sorted_series[-k_default-1])\n",
        "\n",
        "        # Asymptotic standard error.\n",
        "        # SE(ξ̂) = ξ̂ / sqrt(k)\n",
        "        se_xi = default_xi / np.sqrt(k_default)\n",
        "\n",
        "        # 95% confidence interval using normal approximation.\n",
        "        ci_lower = default_xi - 1.96 * se_xi\n",
        "        ci_upper = default_xi + 1.96 * se_xi\n",
        "\n",
        "        # --- 3. Assemble Report ---\n",
        "        analysis_results[name] = {\n",
        "            'default_tail_index': default_xi,\n",
        "            'confidence_interval_95pct': (ci_lower, ci_upper),\n",
        "            'k_for_default_estimate': k_default,\n",
        "            'hill_plot_data': hill_plot_data\n",
        "        }\n",
        "    return analysis_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 6.2.2: GENERALIZED PARETO DISTRIBUTION FITTING\n",
        "# =============================================================================\n",
        "\n",
        "def fit_gpd_to_tails(\n",
        "    data_dict: Dict[str, np.ndarray],\n",
        "    threshold_quantile: float = 0.95\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Fits a Generalized Pareto Distribution (GPD) to the tails of multiple series.\n",
        "\n",
        "    This function implements the Peaks-Over-Threshold (POT) method. For each\n",
        "    series, it selects a high threshold, identifies all exceedances, and fits a\n",
        "    GPD to these excesses using Maximum Likelihood Estimation.\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary of 1D numpy arrays.\n",
        "        threshold_quantile (float): The quantile to use for setting the threshold.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary of reports, one for each series,\n",
        "            containing the threshold, fitted GPD parameters, and goodness-of-fit data.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the analysis results.\n",
        "    analysis_results = {}\n",
        "\n",
        "    for name, series in data_dict.items():\n",
        "        # --- Data Preparation ---\n",
        "        clean_series = np.asarray(series)\n",
        "        clean_series = clean_series[~np.isnan(clean_series)]\n",
        "        if len(clean_series) < 100:\n",
        "            continue\n",
        "\n",
        "        # --- 1. Select Threshold and Identify Excesses ---\n",
        "        # Set the threshold `u` at the specified high quantile.\n",
        "        threshold = np.quantile(clean_series, threshold_quantile)\n",
        "        # Get all data points that exceed the threshold.\n",
        "        exceedances = clean_series[clean_series > threshold]\n",
        "        # Calculate the excesses over the threshold.\n",
        "        excesses = exceedances - threshold\n",
        "\n",
        "        # We need a sufficient number of excesses for a stable fit.\n",
        "        if len(excesses) < 50:\n",
        "            continue\n",
        "\n",
        "        # --- 2. Fit the GPD using MLE ---\n",
        "        # `genpareto.fit` returns shape (xi), loc (threshold), scale (sigma).\n",
        "        shape, loc, scale = stats.genpareto.fit(excesses, floc=0) # Fix location at 0\n",
        "\n",
        "        # --- 3. Assemble Report ---\n",
        "        analysis_results[name] = {\n",
        "            'threshold': threshold,\n",
        "            'num_excesses': len(excesses),\n",
        "            'gpd_params': {'shape_xi': shape, 'scale_sigma': scale}\n",
        "        }\n",
        "    return analysis_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 6.2.3: EXTREME VALUE ANALYSIS AND RISK METRICS\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_extreme_risk_metrics(\n",
        "    gpd_fit_results: Dict[str, Dict[str, Any]],\n",
        "    data_dict: Dict[str, np.ndarray],\n",
        "    quantiles: List[float] = [0.99, 0.999]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Calculates Value at Risk (VaR) and Expected Shortfall (ES) from fitted GPDs.\n",
        "\n",
        "    This function uses the parameters of the fitted Generalized Pareto\n",
        "    Distributions to estimate extreme quantiles (VaR) and the expected value\n",
        "    beyond those quantiles (ES), which are critical metrics in risk management.\n",
        "\n",
        "    Args:\n",
        "        gpd_fit_results (Dict[str, Dict[str, Any]]): The output from the\n",
        "            `fit_gpd_to_tails` function.\n",
        "        data_dict (Dict[str, np.ndarray]): The original data, needed for sample sizes.\n",
        "        quantiles (List[float]): A list of high quantiles for which to calculate\n",
        "                                 VaR and ES.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary mapping each series name to a\n",
        "                                 DataFrame of its calculated risk metrics.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the risk metric results.\n",
        "    risk_metrics_results = {}\n",
        "\n",
        "    for name, fit_result in gpd_fit_results.items():\n",
        "        # --- 1. Unpack GPD Fit Results and Data ---\n",
        "        u = fit_result['threshold']\n",
        "        xi = fit_result['gpd_params']['shape_xi']\n",
        "        sigma = fit_result['gpd_params']['scale_sigma']\n",
        "        N_u = fit_result['num_excesses']\n",
        "        n = len(data_dict[name])\n",
        "\n",
        "        # Initialize a list to store results for different quantiles.\n",
        "        metrics_list = []\n",
        "\n",
        "        for q in quantiles:\n",
        "            # --- 2. Calculate Value at Risk (VaR) ---\n",
        "            # Equation: VaR_q = u + (σ/ξ) * [((n/N_u)*(1-q))⁻ξ - 1]\n",
        "            # This formula inverts the GPD survival function.\n",
        "            term = (n / N_u) * (1 - q)\n",
        "            var = u + (sigma / xi) * (pow(term, -xi) - 1) if xi != 0 else u + sigma * np.log(1/term)\n",
        "\n",
        "            # --- 3. Calculate Expected Shortfall (ES) ---\n",
        "            # ES is the expected value conditional on exceeding VaR.\n",
        "            # Equation: ES_q = VaR_q + (σ + ξ*(VaR_q - u)) / (1 - ξ)\n",
        "            if xi < 1:\n",
        "                es = (var + sigma - xi * u) / (1 - xi)\n",
        "            else:\n",
        "                es = np.inf # ES is infinite for heavy-tailed distributions with xi >= 1\n",
        "\n",
        "            metrics_list.append({'quantile': q, 'VaR': var, 'Expected_Shortfall': es})\n",
        "\n",
        "        # --- 4. Assemble Report ---\n",
        "        risk_metrics_results[name] = pd.DataFrame(metrics_list).set_index('quantile')\n",
        "\n",
        "    return risk_metrics_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 6.2 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_extreme_value_analysis(\n",
        "    data_dict: Dict[str, np.ndarray]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive fat tail and extreme value analysis.\n",
        "\n",
        "    This master function runs a full suite of EVT-based analyses. It estimates\n",
        "    tail indices using the Hill estimator, fits GPDs to tail excesses, and\n",
        "    calculates key risk metrics like VaR and Expected Shortfall.\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary where keys are series names\n",
        "            and values are the 1D numpy arrays of data to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive report containing the results from all\n",
        "                        extreme value analysis components.\n",
        "    \"\"\"\n",
        "    print(\"========== Starting Task 6.2: Fat Tail and Extreme Value Analysis ==========\")\n",
        "\n",
        "    # --- Step 1: Estimate Tail Indices with Hill Estimator ---\n",
        "    print(\"  - Step 6.2.1: Estimating tail indices using Hill estimator...\")\n",
        "    # For returns, we analyze the tail of absolute values.\n",
        "    abs_data_dict = {name: np.abs(series) for name, series in data_dict.items()}\n",
        "    hill_results = estimate_tail_indices(abs_data_dict)\n",
        "\n",
        "    # --- Step 2: Fit Generalized Pareto Distribution to Tails ---\n",
        "    print(\"  - Step 6.2.2: Fitting GPD to tail excesses...\")\n",
        "    gpd_fit_results = fit_gpd_to_tails(abs_data_dict)\n",
        "\n",
        "    # --- Step 3: Calculate Extreme Risk Metrics ---\n",
        "    print(\"  - Step 6.2.3: Calculating VaR and Expected Shortfall...\")\n",
        "    risk_metrics = calculate_extreme_risk_metrics(gpd_fit_results, abs_data_dict)\n",
        "\n",
        "    # --- Step 4: Assemble Final Report ---\n",
        "    final_report = {\n",
        "        'hill_estimator_analysis': hill_results,\n",
        "        'gpd_fit_analysis': gpd_fit_results,\n",
        "        'extreme_risk_metrics': risk_metrics\n",
        "    }\n",
        "\n",
        "    print(\"\\n========== Task 6.2: Extreme Value Analysis Completed Successfully ==========\")\n",
        "    return final_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 6.3.1: AUTOCORRELATION AND PERSISTENCE ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_temporal_dependence(\n",
        "    data_dict: Dict[str, np.ndarray],\n",
        "    max_lags: int = 20\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Analyzes autocorrelation structure and persistence in multiple time series.\n",
        "\n",
        "    This function computes the Autocorrelation (ACF) and Partial Autocorrelation\n",
        "    (PACF) functions, tests for significant serial correlation using the Ljung-Box\n",
        "    test, and quantifies persistence by fitting an AR(1) model to estimate the\n",
        "    half-life of a shock.\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary of 1D numpy arrays to analyze.\n",
        "        max_lags (int): The maximum number of lags to consider for ACF/PACF.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary of reports, one for each series,\n",
        "            containing the results of the temporal dependence analysis.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the analysis results.\n",
        "    analysis_results = {}\n",
        "\n",
        "    for name, series in data_dict.items():\n",
        "        # --- Data Preparation ---\n",
        "        clean_series = np.asarray(series)\n",
        "        clean_series = clean_series[~np.isnan(clean_series)]\n",
        "        if len(clean_series) < max_lags + 5:\n",
        "            continue\n",
        "\n",
        "        # --- 1. ACF/PACF and Ljung-Box Test ---\n",
        "        # Calculate the Ljung-Box test for autocorrelation.\n",
        "        lb_test = acorr_ljungbox(clean_series, lags=[max_lags], return_df=True)\n",
        "\n",
        "        # --- 2. Persistence Analysis via AR(1) Model ---\n",
        "        ar1_coefficient = np.nan\n",
        "        half_life = np.nan\n",
        "        try:\n",
        "            # Fit an Autoregressive model of order 1.\n",
        "            ar_model = AutoReg(clean_series, lags=1).fit()\n",
        "            # Extract the AR(1) coefficient (rho).\n",
        "            rho = ar_model.params[1]\n",
        "            # Calculate half-life only if the process is stationary and persistent.\n",
        "            if 0 < rho < 1:\n",
        "                ar1_coefficient = rho\n",
        "                # Equation: Half-Life = log(0.5) / log(|ρ|)\n",
        "                half_life = np.log(0.5) / np.log(rho)\n",
        "        except Exception:\n",
        "            # Handle cases where the model fails to fit.\n",
        "            pass\n",
        "\n",
        "        # --- 3. Assemble Report ---\n",
        "        analysis_results[name] = {\n",
        "            'ljung_box_p_value': lb_test['lb_pvalue'].iloc[0],\n",
        "            'ar1_coefficient': ar1_coefficient,\n",
        "            'half_life_periods': half_life,\n",
        "            'acf': acf(clean_series, nlags=max_lags),\n",
        "            'pacf': pacf(clean_series, nlags=max_lags)\n",
        "        }\n",
        "    return analysis_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 6.3.2: STRUCTURAL BREAK AND REGIME CHANGE DETECTION\n",
        "# =============================================================================\n",
        "\n",
        "def detect_structural_changes(\n",
        "    data_dict: Dict[str, np.ndarray],\n",
        "    penalty_value: float = 3.0\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Detects structural change points in multiple time series using the Pelt algorithm.\n",
        "\n",
        "    This function uses a state-of-the-art search algorithm to find the optimal\n",
        "    number and location of change points in a time series, corresponding to\n",
        "    abrupt shifts in the underlying data generating process.\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary of 1D numpy arrays to analyze.\n",
        "        penalty_value (float): A penalty term to control the sensitivity of the\n",
        "            detection. Higher values lead to fewer detected breaks.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary of reports, one for each series,\n",
        "            containing the list of detected break points.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the analysis results.\n",
        "    analysis_results = {}\n",
        "\n",
        "    for name, series in data_dict.items():\n",
        "        # --- Data Preparation ---\n",
        "        clean_series = np.asarray(series)\n",
        "        clean_series = clean_series[~np.isnan(clean_series)]\n",
        "        if len(clean_series) < 20:\n",
        "            continue\n",
        "\n",
        "        # --- 1. Initialize the Change Point Detection Algorithm ---\n",
        "        # We use the Pelt algorithm for exact and efficient search.\n",
        "        # 'rbf' is the radial basis function cost, sensitive to changes in distribution.\n",
        "        algo = rpt.Pelt(model=\"rbf\").fit(clean_series)\n",
        "\n",
        "        # --- 2. Predict the Break Points ---\n",
        "        # The `predict` method finds the optimal segmentation given a penalty.\n",
        "        # The penalty is often scaled by log(n).\n",
        "        pen = penalty_value * np.log(len(clean_series))\n",
        "        break_points = algo.predict(pen=pen)\n",
        "\n",
        "        # --- 3. Assemble Report ---\n",
        "        # The result is a list of indices *after* which a break occurs.\n",
        "        analysis_results[name] = {\n",
        "            'break_points_indices': break_points[:-1] # Last point is end of series\n",
        "        }\n",
        "    return analysis_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 6.3.3: VOLATILITY CLUSTERING AND ARCH EFFECTS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_volatility_patterns(\n",
        "    data_dict: Dict[str, np.ndarray]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Analyzes volatility clustering by testing for ARCH effects and fitting a GARCH model.\n",
        "\n",
        "    This function provides a standard workflow for modeling the conditional\n",
        "    volatility of financial returns. It first tests for the presence of\n",
        "    volatility clustering (ARCH effects) and, if detected, fits a GARCH(1,1)\n",
        "    model to quantify the dynamics.\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary of 1D numpy arrays,\n",
        "            which should represent financial returns.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary of reports, one for each series,\n",
        "            containing the ARCH test results and the fitted GARCH model summary.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the analysis results.\n",
        "    analysis_results = {}\n",
        "\n",
        "    for name, series in data_dict.items():\n",
        "        # --- Data Preparation ---\n",
        "        # GARCH models are typically applied to demeaned returns.\n",
        "        clean_series = np.asarray(series)\n",
        "        clean_series = clean_series[~np.isnan(clean_series)]\n",
        "        demeaned_series = clean_series - np.mean(clean_series)\n",
        "        if len(demeaned_series) < 20:\n",
        "            continue\n",
        "\n",
        "        # --- 1. Test for ARCH Effects ---\n",
        "        # Engle's LM test for autoregressive conditional heteroskedasticity.\n",
        "        # A low p-value suggests the presence of ARCH effects.\n",
        "        arch_test_result = het_arch(demeaned_series)\n",
        "        arch_p_value = arch_test_result[1]\n",
        "\n",
        "        # --- 2. Fit GARCH(1,1) Model (if ARCH effects are present) ---\n",
        "        garch_params = None\n",
        "        vol_persistence = None\n",
        "        if arch_p_value < 0.05:\n",
        "            try:\n",
        "                # Specify a GARCH(1,1) model.\n",
        "                garch = arch_model(demeaned_series, vol='Garch', p=1, q=1)\n",
        "                # Fit the model. `update_freq=0` suppresses verbose output.\n",
        "                garch_fit = garch.fit(update_freq=0, disp='off')\n",
        "                # Store the fitted parameters.\n",
        "                garch_params = garch_fit.params.to_dict()\n",
        "                # Calculate volatility persistence.\n",
        "                # Persistence = alpha[1] + beta[1]\n",
        "                vol_persistence = garch_params.get('alpha[1]', 0) + garch_params.get('beta[1]', 0)\n",
        "            except Exception:\n",
        "                # Handle cases where the GARCH model fails to converge.\n",
        "                pass\n",
        "\n",
        "        # --- 3. Assemble Report ---\n",
        "        analysis_results[name] = {\n",
        "            'arch_lm_test_p_value': arch_p_value,\n",
        "            'garch_parameters': garch_params,\n",
        "            'volatility_persistence': vol_persistence\n",
        "        }\n",
        "    return analysis_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 6.3 ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_temporal_analysis(\n",
        "    data_dict: Dict[str, np.ndarray]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive analysis of time series properties.\n",
        "\n",
        "    This master function runs a full suite of temporal analyses, including\n",
        "    autocorrelation and persistence checks, structural break detection, and\n",
        "    volatility clustering analysis.\n",
        "\n",
        "    Args:\n",
        "        data_dict (Dict[str, np.ndarray]): A dictionary where keys are series names\n",
        "            and values are the 1D numpy arrays of data to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive report containing the results from all\n",
        "                        temporal analysis components.\n",
        "    \"\"\"\n",
        "    print(\"========== Starting Task 6.3: Time Series Properties Analysis ==========\")\n",
        "\n",
        "    # --- Step 1: Analyze Autocorrelation and Persistence ---\n",
        "    print(\"  - Step 6.3.1: Analyzing autocorrelation and persistence...\")\n",
        "    dependence_results = analyze_temporal_dependence(data_dict)\n",
        "\n",
        "    # --- Step 2: Detect Structural Breaks ---\n",
        "    print(\"  - Step 6.3.2: Detecting structural breaks...\")\n",
        "    break_results = detect_structural_changes(data_dict)\n",
        "\n",
        "    # --- Step 3: Analyze Volatility Patterns ---\n",
        "    print(\"  - Step 6.3.3: Analyzing volatility clustering (GARCH)...\")\n",
        "    volatility_results = analyze_volatility_patterns(data_dict)\n",
        "\n",
        "    # --- Step 4: Assemble Final Report ---\n",
        "    final_report = {\n",
        "        'temporal_dependence': dependence_results,\n",
        "        'structural_breaks': break_results,\n",
        "        'volatility_patterns': volatility_results\n",
        "    }\n",
        "\n",
        "    print(\"\\n========== Task 6.3: Temporal Analysis Completed Successfully ==========\")\n",
        "    return final_report\n"
      ],
      "metadata": {
        "id": "LRHwPkNQAp19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK VII: ORCHESTRATION AND ROBUSTNESS ANALYSIS\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 7.1.1: MASTER RESEARCH PIPELINE ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def run_full_research_pipeline(\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    analyses_to_run: List[str]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete end-to-end research pipeline for the study.\n",
        "\n",
        "    This master function serves as the single entry point for the entire project.\n",
        "    It executes a sequence of modular, high-rigor tasks, from initial data\n",
        "    validation and preparation to the final empirical estimation and theoretical\n",
        "    simulations. It provides a fully reproducible and auditable workflow for\n",
        "    replicating the paper's findings. This version includes timing instrumentation\n",
        "    to record the duration of each major computational phase for performance analysis.\n",
        "\n",
        "    Args:\n",
        "        raw_macro_df (pd.DataFrame): The raw annual macroeconomic data, indexed by\n",
        "                                     ['country_iso', 'year'].\n",
        "        raw_fx_df (pd.DataFrame): The raw daily foreign exchange data, indexed by\n",
        "                                  ['country_iso', 'date'].\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary that\n",
        "                                        governs all parameters and settings.\n",
        "        analyses_to_run (List[str]): A list specifying which major phases of the\n",
        "            analysis to execute. Supported options: 'data_prep', 'empirical',\n",
        "            'theoretical', 'simulation_validation'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive, nested dictionary containing all data,\n",
        "                        reports, and results generated throughout the pipeline,\n",
        "                        including a performance log with phase timings.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If 'data_prep' is not in `analyses_to_run`, as it is a\n",
        "                    mandatory first step, or if initial input validation fails.\n",
        "    \"\"\"\n",
        "    # Initialize the master dictionary that will store all results from the pipeline.\n",
        "    master_results: Dict[str, Any] = {}\n",
        "    # Initialize a dictionary to store the execution time of each major phase.\n",
        "    phase_timings: Dict[str, float] = {}\n",
        "    # Announce the initiation of the entire pipeline.\n",
        "    print(\"========== MASTER RESEARCH PIPELINE INITIATED ==========\")\n",
        "\n",
        "    # --- Phase I: Data Preparation (Task 1) ---\n",
        "    # This phase is a mandatory prerequisite for all other analyses.\n",
        "    if 'data_prep' in analyses_to_run:\n",
        "        # Record the start time for this phase using a high-resolution clock.\n",
        "        phase_start_time = time.perf_counter()\n",
        "        # Announce the start of Phase I.\n",
        "        print(\"\\n--- Phase I: Data Preparation ---\")\n",
        "\n",
        "        # Execute the entire data preparation pipeline. This single call handles\n",
        "        # initial validation, quality assessment, imputation, outlier treatment,\n",
        "        # and variable transformation. It will raise an error if critical\n",
        "        # validation checks fail, halting the entire process.\n",
        "        analysis_ready_data, data_prep_report = orchestrate_data_preparation_pipeline(\n",
        "            raw_macro_df, raw_fx_df, master_config\n",
        "        )\n",
        "        # Store the comprehensive report from the data preparation phase.\n",
        "        master_results['data_preparation_report'] = data_prep_report\n",
        "        # Store the analysis-ready data, which will be used by all subsequent phases.\n",
        "        master_results['analysis_ready_data'] = analysis_ready_data\n",
        "\n",
        "        # Calculate and store the duration of this phase.\n",
        "        phase_timings['data_preparation'] = time.perf_counter() - phase_start_time\n",
        "        # Announce the successful completion of Phase I and its duration.\n",
        "        print(f\"--- Phase I: Completed Successfully in {phase_timings['data_preparation']:.2f}s ---\")\n",
        "    else:\n",
        "        # If data preparation is not requested, no other analysis can proceed.\n",
        "        raise ValueError(\"'data_prep' must be in `analyses_to_run` as it is a mandatory first step.\")\n",
        "\n",
        "    # --- Phase II: Empirical Analysis (Tasks 2 & 5) ---\n",
        "    # This phase establishes the stylized facts and estimates the trade multiplier model.\n",
        "    if 'empirical' in analyses_to_run:\n",
        "        # Record the start time for the empirical analysis phase.\n",
        "        phase_start_time = time.perf_counter()\n",
        "        # Announce the start of Phase II.\n",
        "        print(\"\\n--- Phase II: Empirical Analysis ---\")\n",
        "\n",
        "        # Execute the full empirical workflow, including FX return analysis and the\n",
        "        # computationally intensive Bayesian estimation of the trade multiplier.\n",
        "        empirical_report = orchestrate_empirical_analysis(\n",
        "            master_results['analysis_ready_data'], master_config\n",
        "        )\n",
        "        # Store the comprehensive results from the empirical analysis.\n",
        "        master_results['empirical_results'] = empirical_report\n",
        "\n",
        "        # Calculate and store the duration of this phase.\n",
        "        phase_timings['empirical_analysis'] = time.perf_counter() - phase_start_time\n",
        "        # Announce the successful completion of Phase II and its duration.\n",
        "        print(f\"--- Phase II: Completed Successfully in {phase_timings['empirical_analysis']:.2f}s ---\")\n",
        "\n",
        "    # --- Phase III: Theoretical Model Analysis (Tasks 3 & 4) ---\n",
        "    # This phase runs the numerical experiments based on the theoretical model.\n",
        "    if 'theoretical' in analyses_to_run:\n",
        "        # Record the start time for the theoretical analysis phase.\n",
        "        phase_start_time = time.perf_counter()\n",
        "        # Announce the start of Phase III.\n",
        "        print(\"\\n--- Phase III: Theoretical Model Analysis ---\")\n",
        "\n",
        "        # First, assemble the toolkit of validated model functions from Task 3.\n",
        "        model_toolkit = implement_theoretical_model()\n",
        "        # Store the toolkit in the results for potential later use or inspection.\n",
        "        master_results['model_toolkit'] = model_toolkit\n",
        "\n",
        "        # Initialize a dictionary to store the results of the theoretical experiments.\n",
        "        theoretical_results: Dict[str, Any] = {}\n",
        "        # Get the dictionary of all scenarios to be run from the master config.\n",
        "        scenarios = master_config['theoretical_simulation']['figure_scenarios']\n",
        "\n",
        "        # Iterate through each defined simulation scenario in the configuration.\n",
        "        for scenario_name, scenario_config in scenarios.items():\n",
        "            # Intelligently determine the type of analysis required for the scenario\n",
        "            # by inspecting the keys in its configuration.\n",
        "            if 'bifurcation_param' in scenario_config:\n",
        "                analysis_type = 'bifurcation'\n",
        "            elif 'basin_plot_settings' in scenario_config:\n",
        "                analysis_type = 'basin_of_attraction'\n",
        "            else:\n",
        "                # If no special keys are present, default to a standard time-series simulation.\n",
        "                analysis_type = 'simulation'\n",
        "\n",
        "            # Announce the specific analysis being run.\n",
        "            print(f\"\\n  - Running theoretical analysis for '{scenario_name}' (type: {analysis_type})...\")\n",
        "\n",
        "            # Dispatch to the appropriate analysis function.\n",
        "            if analysis_type == 'simulation':\n",
        "                # For a standard simulation, call the simulation engine directly.\n",
        "                params = scenario_config['fixed_params']\n",
        "                is_extended = params.get('w_E', 0.0) > 0\n",
        "                ic = master_config['theoretical_simulation']['global_settings']['initial_conditions']\n",
        "                initial_state = (ic['e0'], ic['e_neg1'], ic['delta_y0']) if is_extended else (ic['e0'], ic['delta_y0'])\n",
        "\n",
        "                sim_result = model_toolkit['evolution_engine']['run_simulation'](\n",
        "                    initial_state=initial_state, params=params, core_funcs=model_toolkit['core_dynamics'],\n",
        "                    num_iterations=master_config['theoretical_simulation']['global_settings']['simulation_horizon_daily'],\n",
        "                    transient_period=500,\n",
        "                    seed=42 # Use a fixed seed for reproducibility\n",
        "                )\n",
        "                theoretical_results[scenario_name] = sim_result\n",
        "            else:\n",
        "                # For complex analyses, call the master Task 4 orchestrator.\n",
        "                theoretical_results[scenario_name] = orchestrate_numerical_dynamics_analysis(\n",
        "                    analysis_type=analysis_type,\n",
        "                    scenario_name=scenario_name,\n",
        "                    master_config=master_config,\n",
        "                    model_toolkit=model_toolkit\n",
        "                )\n",
        "\n",
        "        # Store the collected results from all theoretical scenarios.\n",
        "        master_results['theoretical_results'] = theoretical_results\n",
        "\n",
        "        # Calculate and store the total duration of the theoretical analysis phase.\n",
        "        phase_timings['theoretical_analysis'] = time.perf_counter() - phase_start_time\n",
        "        # Announce the successful completion of Phase III and its duration.\n",
        "        print(f\"\\n--- Phase III: Completed Successfully in {phase_timings['theoretical_analysis']:.2f}s ---\")\n",
        "\n",
        "    # --- Phase IV: Statistical Validation of Simulation Output (Task 6) ---\n",
        "    # This phase programmatically checks if the simulation output reproduces the empirical stylized facts.\n",
        "    if 'simulation_validation' in analyses_to_run:\n",
        "        # Record the start time for this phase.\n",
        "        phase_start_time = time.perf_counter()\n",
        "        # Announce the start of Phase IV.\n",
        "        print(\"\\n--- Phase IV: Statistical Validation of Simulation Output ---\")\n",
        "\n",
        "        # Define the key simulation scenario to be validated against empirical data.\n",
        "        validation_scenario = 'fig12'\n",
        "        # Check if the required simulation results were actually generated in Phase III.\n",
        "        if 'theoretical_results' in master_results and validation_scenario in master_results['theoretical_results']:\n",
        "            # Extract the trajectory from the simulation results.\n",
        "            sim_trajectory = master_results['theoretical_results'][validation_scenario]['trajectory']\n",
        "            # The first column of the trajectory is the exchange rate; calculate log-returns.\n",
        "            sim_returns = np.diff(sim_trajectory[:, 0])\n",
        "            # Format the simulated data into the structure expected by the validation toolkit.\n",
        "            sim_data_dict = {f'simulated_fx_returns_{validation_scenario}': sim_returns}\n",
        "\n",
        "            # Run the full statistical validation pipeline (Task 6) on the simulated data.\n",
        "            simulation_validation_report = orchestrate_statistical_validation(\n",
        "                sim_data_dict, master_config\n",
        "            )\n",
        "            # Store the validation report.\n",
        "            master_results['simulation_validation_results'] = simulation_validation_report\n",
        "\n",
        "            # Calculate and store the duration of this phase.\n",
        "            phase_timings['simulation_validation'] = time.perf_counter() - phase_start_time\n",
        "            # Announce the successful completion of Phase IV.\n",
        "            print(f\"--- Phase IV: Completed Successfully in {phase_timings['simulation_validation']:.2f}s ---\")\n",
        "        else:\n",
        "            # If the required simulation was not run, skip this phase and issue a warning.\n",
        "            print(f\"--- Phase IV: SKIPPED - Required simulation results for '{validation_scenario}' not found. ---\")\n",
        "\n",
        "    # --- 5. Final Output ---\n",
        "    # Add the timing report to the master results object for performance analysis.\n",
        "    master_results['performance_log'] = {'phase_timings_seconds': phase_timings}\n",
        "    # Announce the successful completion of the entire pipeline.\n",
        "    print(\"\\n========== MASTER RESEARCH PIPELINE COMPLETED ==========\")\n",
        "\n",
        "    # Return the final, comprehensive results object.\n",
        "    return master_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 7.1.2: PARALLEL PROCESSING AND COMPUTATIONAL OPTIMIZATION\n",
        "# =============================================================================\n",
        "\n",
        "def profile_computational_pipeline(\n",
        "    pipeline_func: Callable[..., Dict[str, Any]],\n",
        "    *args,\n",
        "    **kwargs\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Profiles the computational performance of the main research pipeline.\n",
        "\n",
        "    This function acts as a wrapper around the main pipeline orchestrator. It\n",
        "    executes the pipeline while monitoring key performance indicators such as\n",
        "    execution time (total and per-phase), peak memory usage, and CPU utilization.\n",
        "    It provides a quantitative basis for identifying computational bottlenecks\n",
        "    and optimizing the workflow.\n",
        "\n",
        "    Args:\n",
        "        pipeline_func (Callable[..., Dict[str, Any]]): The master orchestrator\n",
        "            function to be profiled (e.g., `run_full_research_pipeline`).\n",
        "        *args: Positional arguments to be passed to the pipeline function.\n",
        "        **kwargs: Keyword arguments to be passed to the pipeline function.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the detailed performance report\n",
        "                        alongside the original results from the pipeline.\n",
        "    \"\"\"\n",
        "    print(\"========== Starting Task 7.1.2: Computational Performance Profiling ==========\")\n",
        "\n",
        "    # --- 1. Performance Monitoring Setup ---\n",
        "    # Record the start time of the entire process using a high-resolution clock.\n",
        "    start_time = time.perf_counter()\n",
        "    # Get the current OS process for monitoring CPU and memory.\n",
        "    process = psutil.Process(os.getpid())\n",
        "\n",
        "    # --- 2. Execute Pipeline with Memory Profiling ---\n",
        "    # `memory_usage` is a powerful tool that runs the target function and\n",
        "    # records its memory footprint in MiB.\n",
        "    # `max_usage=True` efficiently returns only the peak memory usage.\n",
        "    # `retval=True` ensures that the return value of the profiled function is captured.\n",
        "    mem_usage, pipeline_results = memory_usage(\n",
        "        (pipeline_func, args, kwargs),\n",
        "        max_usage=True,\n",
        "        retval=True,\n",
        "        interval=0.1 # Check memory usage every 100ms\n",
        "    )\n",
        "\n",
        "    # --- 3. Collate Performance Metrics ---\n",
        "    # Record the end time and calculate the total wall-clock duration.\n",
        "    end_time = time.perf_counter()\n",
        "    total_duration_s = end_time - start_time\n",
        "\n",
        "    # Extract the detailed per-phase timings logged by the amended orchestrator.\n",
        "    phase_timings = pipeline_results.get('performance_log', {}).get('phase_timings_seconds', {})\n",
        "\n",
        "    # Programmatically identify the computational bottleneck.\n",
        "    if phase_timings:\n",
        "        # The bottleneck is the phase with the longest execution time.\n",
        "        bottleneck = max(phase_timings, key=phase_timings.get)\n",
        "    else:\n",
        "        # Fallback if per-phase timing is not available.\n",
        "        bottleneck = \"Unknown (per-phase timing not available in results)\"\n",
        "\n",
        "    # Get the final CPU times (user-space and kernel-space).\n",
        "    cpu_times = process.cpu_times()\n",
        "\n",
        "    # --- 4. Assemble Performance Report ---\n",
        "    # Create a comprehensive dictionary summarizing all performance metrics.\n",
        "    performance_report = {\n",
        "        'total_execution_time_seconds': total_duration_s,\n",
        "        'peak_memory_usage_mb': mem_usage,\n",
        "        'cpu_times_seconds': {\n",
        "            'user': cpu_times.user,\n",
        "            'system': cpu_times.system\n",
        "        },\n",
        "        'phase_breakdown_seconds': phase_timings,\n",
        "        'identified_bottleneck': bottleneck\n",
        "    }\n",
        "\n",
        "    print(\"\\n========== Task 7.1.2: Profiling Completed Successfully ==========\")\n",
        "\n",
        "    # Return both the performance report and the original results from the pipeline.\n",
        "    return {\n",
        "        'performance_report': performance_report,\n",
        "        'pipeline_results': pipeline_results\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 7.1.3: RESULT INTEGRATION AND QUALITY ASSURANCE\n",
        "# =============================================================================\n",
        "\n",
        "def integrate_and_validate_results(\n",
        "    master_results: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Integrates and performs quality assurance on the master results object.\n",
        "\n",
        "    This function synthesizes the vast, nested output from the entire pipeline\n",
        "    into a high-level summary. It performs critical cross-component validation,\n",
        "    such as comparing the stylized facts from empirical data against those\n",
        "    generated by the simulation, and compiles key metrics into an executive\n",
        "    summary format.\n",
        "\n",
        "    Args:\n",
        "        master_results (Dict[str, Any]): The comprehensive results dictionary\n",
        "            returned by the `run_full_research_pipeline` orchestrator.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A Quality Assurance report containing a high-level\n",
        "                        summary of findings and cross-validation checks.\n",
        "    \"\"\"\n",
        "    print(\"========== Starting Task 7.1.3: Result Integration and Quality Assurance ==========\")\n",
        "\n",
        "    # Initialize the QA report.\n",
        "    qa_report = {}\n",
        "\n",
        "    # --- 1. Compile Key Data Quality Metrics ---\n",
        "    if 'data_preparation_report' in master_results:\n",
        "        # Example: Calculate average data completeness across countries.\n",
        "        temporal_reports = master_results['data_preparation_report']['temporal_validation']['detailed_report']\n",
        "        avg_completeness = np.mean([\n",
        "            report['annual_data']['completeness_ratio'] for report in temporal_reports.values()\n",
        "        ])\n",
        "        qa_report['data_quality_summary'] = {\n",
        "            'average_annual_data_completeness': avg_completeness\n",
        "        }\n",
        "\n",
        "    # --- 2. Compile Key Empirical Model Performance Metrics ---\n",
        "    if 'empirical_results' in master_results:\n",
        "        # Extract the cross-country performance summary for the trade multiplier model.\n",
        "        perf_summary = master_results['empirical_results']['trade_multiplier_validation']['cross_country_summary']['performance_summary']\n",
        "        qa_report['empirical_model_performance'] = {\n",
        "            'average_r_squared': perf_summary['r_squared'].mean(),\n",
        "            'average_rmse': perf_summary['rmse'].mean()\n",
        "        }\n",
        "\n",
        "    # --- 3. Cross-Component Validation: Empirical vs. Simulated Stylized Facts ---\n",
        "    # This is a crucial step: does the model replicate reality?\n",
        "    if 'simulation_validation_results' in master_results and 'empirical_results' in master_results:\n",
        "        try:\n",
        "            # Extract excess kurtosis (fat tails) from empirical FX returns.\n",
        "            emp_kurtosis = master_results['empirical_results']['fx_rate_properties']['descriptive_statistics']['excess_kurtosis'].mean()\n",
        "\n",
        "            # Extract excess kurtosis from the validated simulation's returns.\n",
        "            sim_report = master_results['simulation_validation_results']['distributional_tests']['qq_and_tail_analysis']\n",
        "            sim_kurtosis = next(iter(sim_report.values()))['excess_kurtosis'] # Get first (and only) item\n",
        "\n",
        "            # Compare the two. A successful model should generate fat tails (kurtosis > 0).\n",
        "            cross_validation_summary = {\n",
        "                'stylized_fact': 'Excess Kurtosis (Fat Tails)',\n",
        "                'empirical_value': emp_kurtosis,\n",
        "                'simulated_value': sim_kurtosis,\n",
        "                'validation_status': 'PASS' if emp_kurtosis > 1 and sim_kurtosis > 1 else 'FAIL'\n",
        "            }\n",
        "            qa_report['simulation_validation_summary'] = cross_validation_summary\n",
        "        except (KeyError, IndexError, StopIteration):\n",
        "            # Handle cases where the nested data might be missing.\n",
        "            qa_report['simulation_validation_summary'] = {'status': 'ERROR', 'message': 'Could not extract data for cross-validation.'}\n",
        "\n",
        "    # --- 4. Assemble Final Report ---\n",
        "    print(\"\\n========== Task 7.1.3: Integration and QA Completed Successfully ==========\")\n",
        "    return qa_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 7.2.1: PARAMETER SENSITIVITY ROBUSTNESS TESTING\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_parameter_sensitivity_analysis(\n",
        "    master_config: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive parameter sensitivity analysis.\n",
        "\n",
        "    This function serves as the master controller for assessing the model's\n",
        "    robustness to parameter uncertainty. It can execute two distinct types of\n",
        "    state-of-the-art sensitivity analysis, driven by a dedicated configuration block:\n",
        "    1.  Local Sensitivity Analysis: Measures the impact of infinitesimal changes\n",
        "        to parameters around a specific baseline point in the parameter space.\n",
        "    2.  Global Sensitivity Analysis (Sobol): Decomposes the variance of the model\n",
        "        output across the entire specified parameter space, quantifying both the\n",
        "        direct and total (including interactions) importance of each parameter.\n",
        "\n",
        "    Args:\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary, which\n",
        "            must contain a `sensitivity_analysis_config` section.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing the results. Keys will be\n",
        "            'local_sensitivity_report' and/or 'global_sensitivity_report',\n",
        "            with pandas DataFrames as values.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the required `sensitivity_analysis_config` is missing from\n",
        "                  the master configuration.\n",
        "        ValueError: If the configuration for a requested analysis is invalid.\n",
        "    \"\"\"\n",
        "    # Announce the start of the task.\n",
        "    print(\"========== Starting Task 7.2.1: Parameter Sensitivity Robustness Testing ==========\")\n",
        "\n",
        "    # --- 1. Validate and Retrieve Configuration ---\n",
        "    # Check for the presence of the main sensitivity analysis configuration block.\n",
        "    if 'sensitivity_analysis_config' not in master_config:\n",
        "        raise KeyError(\"`master_config` must contain a 'sensitivity_analysis_config' section.\")\n",
        "    # Extract the sensitivity configuration.\n",
        "    config = master_config['sensitivity_analysis_config']\n",
        "\n",
        "    # Initialize a dictionary to store the results of the analyses.\n",
        "    results: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    # --- 2. Execute Local Sensitivity Analysis (if configured) ---\n",
        "    # Check if a configuration for local analysis exists.\n",
        "    if 'local' in config:\n",
        "        # Announce the start of the local analysis.\n",
        "        print(\"\\n--- Running Local Sensitivity Analysis ---\")\n",
        "        # Extract the specific configuration for the local analysis.\n",
        "        local_config = config['local']\n",
        "        # Validate that the required keys are present in the configuration.\n",
        "        if not all(k in local_config for k in ['base_scenario', 'params_to_test']):\n",
        "            raise ValueError(\"Local sensitivity config must include 'base_scenario' and 'params_to_test'.\")\n",
        "\n",
        "        # Retrieve the baseline parameter set from the specified figure scenario.\n",
        "        base_params = master_config['theoretical_simulation']['figure_scenarios'][local_config['base_scenario']]['fixed_params']\n",
        "\n",
        "        # Call the dedicated function for local sensitivity analysis.\n",
        "        local_sensitivity_report = analyze_local_parameter_sensitivity(\n",
        "            base_params=base_params,\n",
        "            params_to_test=local_config['params_to_test'],\n",
        "            model_toolkit=model_toolkit,\n",
        "            perturbation_size=local_config.get('perturbation_size', 0.01)\n",
        "        )\n",
        "        # Store the resulting report.\n",
        "        results['local_sensitivity_report'] = local_sensitivity_report\n",
        "        # Announce completion.\n",
        "        print(\"--- Local Sensitivity Analysis Completed ---\")\n",
        "\n",
        "    # --- 3. Execute Global Sensitivity Analysis (if configured) ---\n",
        "    # Check if a configuration for global analysis exists.\n",
        "    if 'global' in config:\n",
        "        # Announce the start of the global analysis.\n",
        "        print(\"\\n--- Running Global Sensitivity Analysis (Sobol) ---\")\n",
        "        # Extract the specific configuration for the global analysis.\n",
        "        global_config = config['global']\n",
        "        # Validate that the required keys are present in the configuration.\n",
        "        if not all(k in global_config for k in ['base_scenario', 'N', 'bounds']):\n",
        "            raise ValueError(\"Global sensitivity config must include 'base_scenario', 'N', and 'bounds'.\")\n",
        "\n",
        "        # Retrieve the baseline parameter set.\n",
        "        base_params = master_config['theoretical_simulation']['figure_scenarios'][global_config['base_scenario']]['fixed_params']\n",
        "\n",
        "        # Call the dedicated function for global sensitivity analysis.\n",
        "        global_sensitivity_report = analyze_global_parameter_sensitivity(\n",
        "            base_params=base_params,\n",
        "            sensitivity_config=global_config,\n",
        "            model_toolkit=model_toolkit\n",
        "        )\n",
        "        # Store the resulting report.\n",
        "        results['global_sensitivity_report'] = global_sensitivity_report\n",
        "        # Announce completion.\n",
        "        print(\"--- Global Sensitivity Analysis Completed ---\")\n",
        "\n",
        "    # Check if any analysis was actually run.\n",
        "    if not results:\n",
        "        print(\"WARNING: No sensitivity analysis was configured or run.\")\n",
        "\n",
        "    # Announce the successful completion of the master task.\n",
        "    print(\"\\n========== Task 7.2.1: Parameter Sensitivity Analysis Completed Successfully ==========\")\n",
        "\n",
        "    # Return the dictionary of reports.\n",
        "    return results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 7.2.2: MODEL SPECIFICATION ROBUSTNESS TESTING\n",
        "# =============================================================================\n",
        "\n",
        "def _test_hp_filter_sensitivity(\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Tests the sensitivity of empirical results to the HP filter's lambda parameter.\n",
        "\n",
        "    This function systematically re-runs the entire empirical analysis pipeline\n",
        "    for different values of the Hodrick-Prescott filter's smoothing parameter (lambda).\n",
        "    It then compares key output metrics (e.g., average R-squared of the trade\n",
        "    multiplier model) across these runs to assess the robustness of the main\n",
        "    empirical findings to this specific methodological choice.\n",
        "\n",
        "    Args:\n",
        "        raw_macro_df (pd.DataFrame): The raw annual macroeconomic data.\n",
        "        raw_fx_df (pd.DataFrame): The raw daily foreign exchange data.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary, which\n",
        "            must contain a `robustness_tests.hp_filter.lambda_values` list.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the key performance metrics for each\n",
        "                      lambda value tested, allowing for easy comparison.\n",
        "    \"\"\"\n",
        "    # --- 1. Retrieve Configuration ---\n",
        "    # Extract the list of alternative lambda values to test from the config.\n",
        "    try:\n",
        "        lambda_values_to_test = master_config['robustness_tests']['hp_filter']['lambda_values']\n",
        "        # Get the baseline lambda to include it in the comparison.\n",
        "        baseline_lambda = master_config['empirical_analysis']['parameters']['hp_filter']['lambda']\n",
        "        # Combine baseline and alternatives, ensuring no duplicates.\n",
        "        all_lambdas = sorted(list(set([baseline_lambda] + lambda_values_to_test)))\n",
        "    except KeyError:\n",
        "        print(\"WARNING: HP filter sensitivity test not configured. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Initialize a list to store the results of each sensitivity run.\n",
        "    sensitivity_results = []\n",
        "\n",
        "    # --- 2. Iterate Through Each Lambda Specification ---\n",
        "    for lambda_val in all_lambdas:\n",
        "        # Announce the start of the specific test run.\n",
        "        print(f\"\\n--- Running robustness test for HP filter lambda = {lambda_val} ---\")\n",
        "\n",
        "        # --- 2.1. Create a Modified Configuration ---\n",
        "        # Create a deep, isolated copy of the master config to avoid side effects.\n",
        "        temp_config = copy.deepcopy(master_config)\n",
        "        # Modify the lambda parameter in the temporary configuration.\n",
        "        temp_config['empirical_analysis']['parameters']['hp_filter']['lambda'] = lambda_val\n",
        "\n",
        "        # --- 2.2. Re-run the Entire Empirical Pipeline ---\n",
        "        # This is computationally intensive but is the only way to ensure a\n",
        "        # rigorous and valid robustness check.\n",
        "        try:\n",
        "            # Step A: Run the full data preparation pipeline with the modified config.\n",
        "            analysis_ready_data, _ = orchestrate_data_preparation_pipeline(\n",
        "                raw_macro_df, raw_fx_df, temp_config\n",
        "            )\n",
        "\n",
        "            # Step B: Run the full empirical analysis pipeline.\n",
        "            empirical_report = orchestrate_empirical_analysis(\n",
        "                analysis_ready_data, temp_config\n",
        "            )\n",
        "\n",
        "            # --- 2.3. Extract and Store Key Result Metrics ---\n",
        "            # Extract the cross-country performance summary DataFrame.\n",
        "            perf_summary = empirical_report['trade_multiplier_validation']['cross_country_summary']['performance_summary']\n",
        "            # Calculate the average R-squared and RMSE across all countries.\n",
        "            avg_r_squared = perf_summary['r_squared'].mean()\n",
        "            avg_rmse = perf_summary['rmse'].mean()\n",
        "\n",
        "            # Append the result for this lambda value.\n",
        "            sensitivity_results.append({\n",
        "                'lambda_value': lambda_val,\n",
        "                'average_r_squared': avg_r_squared,\n",
        "                'average_rmse': avg_rmse,\n",
        "                'status': 'Success'\n",
        "            })\n",
        "        except Exception as e:\n",
        "            # If any part of the pipeline fails for a given lambda, log it and continue.\n",
        "            print(f\"    ERROR: Pipeline run failed for lambda = {lambda_val}. Error: {e}\")\n",
        "            sensitivity_results.append({\n",
        "                'lambda_value': lambda_val,\n",
        "                'average_r_squared': np.nan,\n",
        "                'average_rmse': np.nan,\n",
        "                'status': f'Fail: {e}'\n",
        "            })\n",
        "\n",
        "    # --- 3. Assemble and Return the Final Report ---\n",
        "    # Convert the list of results into a DataFrame for clear reporting.\n",
        "    return pd.DataFrame(sensitivity_results).set_index('lambda_value')\n",
        "\n",
        "\n",
        "def _test_initial_condition_sensitivity(\n",
        "    master_config: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Tests the sensitivity of theoretical model dynamics to initial conditions.\n",
        "\n",
        "    This function checks for path dependence in a key simulation scenario. It runs\n",
        "    the simulation from multiple different starting points and compares summary\n",
        "    statistics of the resulting attractors. For a well-behaved chaotic attractor,\n",
        "    these statistics should be independent of the initial condition (if within\n",
        "    the basin of attraction).\n",
        "\n",
        "    Args:\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary, which\n",
        "            must contain a `robustness_tests.initial_conditions` section.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the attractor properties for each\n",
        "                      initial condition tested.\n",
        "    \"\"\"\n",
        "    # --- 1. Retrieve Configuration ---\n",
        "    try:\n",
        "        # Get the name of the scenario to test (e.g., a chaotic one).\n",
        "        scenario_name = master_config['robustness_tests']['initial_conditions']['scenario_to_test']\n",
        "        # Get the list of alternative initial conditions to test.\n",
        "        alt_ics = master_config['robustness_tests']['initial_conditions']['conditions']\n",
        "        # Get the parameters for the chosen scenario.\n",
        "        params = master_config['theoretical_simulation']['figure_scenarios'][scenario_name]['fixed_params']\n",
        "    except KeyError:\n",
        "        print(\"WARNING: Initial condition sensitivity test not configured. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Initialize a list to store the results of each run.\n",
        "    sensitivity_results = []\n",
        "    # Retrieve the simulation engine from the toolkit.\n",
        "    run_simulation_func = model_toolkit['evolution_engine']['run_simulation']\n",
        "    core_funcs = model_toolkit['core_dynamics']\n",
        "\n",
        "    # --- 2. Iterate Through Each Initial Condition ---\n",
        "    for i, ic in enumerate(alt_ics):\n",
        "        print(f\"\\n--- Running robustness test for initial condition {i+1}: {ic} ---\")\n",
        "\n",
        "        # --- 2.1. Run Simulation ---\n",
        "        # The state must match the model's order (2D or 3D).\n",
        "        is_extended = params.get('w_E', 0.0) > 0\n",
        "        initial_state = (ic[0], ic[0], ic[1]) if is_extended else (ic[0], ic[1])\n",
        "\n",
        "        sim_results = run_simulation_func(\n",
        "            initial_state=initial_state, params=params, core_funcs=core_funcs,\n",
        "            num_iterations=5000, transient_period=2500, seed=42\n",
        "        )\n",
        "\n",
        "        # --- 2.2. Calculate Attractor Statistics ---\n",
        "        # Initialize default values for the metrics.\n",
        "        attractor_mean_e = np.nan\n",
        "        attractor_std_e = np.nan\n",
        "        status = sim_results['status']\n",
        "\n",
        "        # If the simulation completed successfully, calculate statistics.\n",
        "        if status == 'completed':\n",
        "            # Extract the asymptotic exchange rate trajectory.\n",
        "            e_trajectory = sim_results['trajectory'][:, 0]\n",
        "            # Calculate the mean of the attractor.\n",
        "            attractor_mean_e = np.mean(e_trajectory)\n",
        "            # Calculate the standard deviation (amplitude) of the attractor.\n",
        "            attractor_std_e = np.std(e_trajectory)\n",
        "\n",
        "        # --- 2.3. Store Results ---\n",
        "        sensitivity_results.append({\n",
        "            'initial_condition_id': i,\n",
        "            'e0': ic[0],\n",
        "            'dy0': ic[1],\n",
        "            'status': status,\n",
        "            'attractor_mean_e': attractor_mean_e,\n",
        "            'attractor_std_e': attractor_std_e\n",
        "        })\n",
        "\n",
        "    # --- 3. Assemble and Return the Final Report ---\n",
        "    return pd.DataFrame(sensitivity_results).set_index('initial_condition_id')\n",
        "\n",
        "\n",
        "def orchestrate_specification_robustness_tests(\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates a suite of model specification robustness tests.\n",
        "\n",
        "    This function serves as the master controller for Task 7.2.2. It runs\n",
        "    various tests to check how sensitive the project's main conclusions are to\n",
        "    changes in underlying model specifications. Each test involves re-running\n",
        "    a significant portion of the pipeline with a modified assumption and\n",
        "    comparing the results to the baseline.\n",
        "\n",
        "    Args:\n",
        "        raw_macro_df (pd.DataFrame): The raw annual macroeconomic data.\n",
        "        raw_fx_df (pd.DataFrame): The raw daily foreign exchange data.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary where each key represents a\n",
        "            robustness test and each value is a DataFrame summarizing the\n",
        "            results of that test.\n",
        "    \"\"\"\n",
        "    # Announce the start of the task.\n",
        "    print(\"========== Starting Task 7.2.2: Model Specification Robustness Testing ==========\")\n",
        "\n",
        "    # Initialize a dictionary to store the results of all robustness tests.\n",
        "    robustness_reports = {}\n",
        "\n",
        "    # --- 1. HP Filter Lambda Sensitivity Test ---\n",
        "    # This test checks if the main empirical result is robust to the choice of the HP filter smoothing parameter.\n",
        "    hp_sensitivity_report = _test_hp_filter_sensitivity(\n",
        "        raw_macro_df, raw_fx_df, master_config\n",
        "    )\n",
        "    # Store the report.\n",
        "    robustness_reports['hp_filter_sensitivity'] = hp_sensitivity_report\n",
        "\n",
        "    # --- 2. Initial Condition Sensitivity Test ---\n",
        "    # This test checks for path dependence in the theoretical model's dynamics.\n",
        "    ic_sensitivity_report = _test_initial_condition_sensitivity(\n",
        "        master_config, model_toolkit\n",
        "    )\n",
        "    # Store the report.\n",
        "    robustness_reports['initial_condition_sensitivity'] = ic_sensitivity_report\n",
        "\n",
        "    # Announce the successful completion of the master task.\n",
        "    print(\"\\n========== Task 7.2.2: Specification Robustness Testing Completed Successfully ==========\")\n",
        "\n",
        "    # Return the dictionary of reports.\n",
        "    return robustness_reports\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SUB-TASK 7.2.3: DATA AND SAMPLE ROBUSTNESS ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def _test_sample_period_robustness(\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Tests the sensitivity of empirical results to the sample time period.\n",
        "\n",
        "    This function assesses the temporal stability of the main empirical findings.\n",
        "    It splits the full data sample into distinct sub-periods as defined in the\n",
        "    configuration, re-runs the entire empirical analysis pipeline on each subset,\n",
        "    and compares key output metrics. This is a critical test to ensure the\n",
        "    results are not driven by a specific historical era.\n",
        "\n",
        "    Args:\n",
        "        raw_macro_df (pd.DataFrame): The raw annual macroeconomic data.\n",
        "        raw_fx_df (pd.DataFrame): The raw daily foreign exchange data.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary, which\n",
        "            must contain a `robustness_tests.sample_period.periods` dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing key performance and parameter\n",
        "                      metrics for each sub-period tested.\n",
        "    \"\"\"\n",
        "    # --- 1. Retrieve Configuration ---\n",
        "    try:\n",
        "        # Extract the dictionary defining the sub-periods to test.\n",
        "        periods_to_test = master_config['robustness_tests']['sample_period']['periods']\n",
        "    except KeyError:\n",
        "        # If the configuration is missing, skip the test and return an empty DataFrame.\n",
        "        print(\"WARNING: Sample period robustness test not configured. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Initialize a list to store the results of each sub-period run.\n",
        "    sensitivity_results = []\n",
        "\n",
        "    # --- 2. Iterate Through Each Sub-Period Specification ---\n",
        "    for period_name, (start_year, end_year) in periods_to_test.items():\n",
        "        # Announce the start of the specific test run.\n",
        "        print(f\"\\n--- Running robustness test for sample period '{period_name}' ({start_year}-{end_year}) ---\")\n",
        "\n",
        "        # --- 2.1. Create Sub-Sampled DataFrames ---\n",
        "        # Filter the macro data to the specified year range.\n",
        "        year_idx = raw_macro_df.index.get_level_values('year')\n",
        "        macro_df_subset = raw_macro_df[(year_idx >= start_year) & (year_idx <= end_year)].copy()\n",
        "\n",
        "        # Filter the FX data to the corresponding date range.\n",
        "        date_idx = raw_fx_df.index.get_level_values('date')\n",
        "        fx_df_subset = raw_fx_df[(date_idx.year >= start_year) & (date_idx.year <= end_year)].copy()\n",
        "\n",
        "        # Check if the resulting subsets have sufficient data to proceed.\n",
        "        if macro_df_subset.empty or fx_df_subset.empty:\n",
        "            print(f\"    WARNING: No data available for period '{period_name}'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # --- 2.2. Re-run the Empirical Pipeline on the Subset ---\n",
        "        try:\n",
        "            # Execute the entire end-to-end pipeline, but only the data prep and empirical phases.\n",
        "            results = run_full_research_pipeline(\n",
        "                raw_macro_df=macro_df_subset,\n",
        "                raw_fx_df=fx_df_subset,\n",
        "                master_config=master_config,\n",
        "                analyses_to_run=['data_prep', 'empirical']\n",
        "            )\n",
        "\n",
        "            # --- 2.3. Extract and Store Key Result Metrics ---\n",
        "            # Extract the cross-country performance summary.\n",
        "            perf_summary = results['empirical_results']['trade_multiplier_validation']['cross_country_summary']['performance_summary']\n",
        "            # Extract the cross-country parameter summary.\n",
        "            param_summary = results['empirical_results']['trade_multiplier_validation']['cross_country_summary']['parameter_summary']\n",
        "\n",
        "            # Append the key results for this sub-period.\n",
        "            sensitivity_results.append({\n",
        "                'period_name': period_name,\n",
        "                'average_r_squared': perf_summary['r_squared'].mean(),\n",
        "                'average_rmse': perf_summary['rmse'].mean(),\n",
        "                'average_pi_t_mean': param_summary['avg_pi_t_mean'].mean(),\n",
        "                'status': 'Success'\n",
        "            })\n",
        "        except Exception as e:\n",
        "            # If the pipeline fails on the subset, log it comprehensively.\n",
        "            print(f\"    ERROR: Pipeline run failed for period '{period_name}'. Error: {e}\")\n",
        "            sensitivity_results.append({\n",
        "                'period_name': period_name,\n",
        "                'average_r_squared': np.nan,\n",
        "                'average_rmse': np.nan,\n",
        "                'average_pi_t_mean': np.nan,\n",
        "                'status': f'Fail: {e}'\n",
        "            })\n",
        "\n",
        "    # --- 3. Assemble and Return the Final Report ---\n",
        "    return pd.DataFrame(sensitivity_results).set_index('period_name')\n",
        "\n",
        "\n",
        "def _test_country_leave_one_out_robustness(\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a leave-one-out cross-validation at the country level.\n",
        "\n",
        "    This function assesses whether the overall empirical results are disproportionately\n",
        "    driven by any single country in the sample. It iteratively removes one\n",
        "    country, re-runs the entire empirical analysis on the remaining data, and\n",
        "    records the change in the key performance metric.\n",
        "\n",
        "    Args:\n",
        "        raw_macro_df (pd.DataFrame): The raw annual macroeconomic data.\n",
        "        raw_fx_df (pd.DataFrame): The raw daily foreign exchange data.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the impact on the average R-squared\n",
        "                      when each country is excluded.\n",
        "    \"\"\"\n",
        "    # --- 1. Run Baseline Analysis on Full Sample ---\n",
        "    print(\"\\n--- Running baseline analysis on full sample for LOO comparison ---\")\n",
        "    baseline_results = run_full_research_pipeline(\n",
        "        raw_macro_df, raw_fx_df, master_config, ['data_prep', 'empirical']\n",
        "    )\n",
        "    baseline_r2 = baseline_results['empirical_results']['trade_multiplier_validation']['cross_country_summary']['performance_summary']['r_squared'].mean()\n",
        "    print(f\"Baseline average R-squared: {baseline_r2:.4f}\")\n",
        "\n",
        "    # --- 2. Iterate Through Each Country to Leave Out ---\n",
        "    # Get the list of all unique countries in the dataset.\n",
        "    all_countries = raw_macro_df.index.get_level_values('country_iso').unique()\n",
        "    sensitivity_results = []\n",
        "\n",
        "    for country_to_leave_out in all_countries:\n",
        "        print(f\"\\n--- Running LOO robustness test, leaving out: {country_to_leave_out} ---\")\n",
        "\n",
        "        # --- 2.1. Create Leave-One-Out Data Subsets ---\n",
        "        macro_df_subset = raw_macro_df.drop(country_to_leave_out, level='country_iso')\n",
        "        fx_df_subset = raw_fx_df.drop(country_to_leave_out, level='country_iso')\n",
        "\n",
        "        # --- 2.2. Re-run the Empirical Pipeline on the Subset ---\n",
        "        try:\n",
        "            results = run_full_research_pipeline(\n",
        "                raw_macro_df=macro_df_subset,\n",
        "                raw_fx_df=fx_df_subset,\n",
        "                master_config=master_config,\n",
        "                analyses_to_run=['data_prep', 'empirical']\n",
        "            )\n",
        "            # --- 2.3. Extract and Store the Key Result Metric ---\n",
        "            perf_summary = results['empirical_results']['trade_multiplier_validation']['cross_country_summary']['performance_summary']\n",
        "            loo_r2 = perf_summary['r_squared'].mean()\n",
        "\n",
        "            sensitivity_results.append({\n",
        "                'left_out_country': country_to_leave_out,\n",
        "                'loo_average_r_squared': loo_r2,\n",
        "                'change_from_baseline': loo_r2 - baseline_r2,\n",
        "                'status': 'Success'\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"    ERROR: Pipeline run failed when leaving out {country_to_leave_out}. Error: {e}\")\n",
        "            sensitivity_results.append({\n",
        "                'left_out_country': country_to_leave_out,\n",
        "                'loo_average_r_squared': np.nan,\n",
        "                'change_from_baseline': np.nan,\n",
        "                'status': f'Fail: {e}'\n",
        "            })\n",
        "\n",
        "    # --- 3. Assemble and Return the Final Report ---\n",
        "    return pd.DataFrame(sensitivity_results).set_index('left_out_country')\n",
        "\n",
        "\n",
        "def orchestrate_data_sample_robustness_tests(\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates a suite of data and sample robustness tests.\n",
        "\n",
        "    This function serves as the master controller for Task 7.2.3. It runs\n",
        "    various tests to check how sensitive the project's main conclusions are to\n",
        "    the specific sample of data used for the analysis, such as the time period\n",
        "    or the inclusion/exclusion of specific countries.\n",
        "\n",
        "    Args:\n",
        "        raw_macro_df (pd.DataFrame): The raw annual macroeconomic data.\n",
        "        raw_fx_df (pd.DataFrame): The raw daily foreign exchange data.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary where each key represents a\n",
        "            robustness test and each value is a DataFrame summarizing the\n",
        "            results of that test.\n",
        "    \"\"\"\n",
        "    # Announce the start of the task.\n",
        "    print(\"========== Starting Task 7.2.3: Data and Sample Robustness Analysis ==========\")\n",
        "\n",
        "    # Initialize a dictionary to store the results of all robustness tests.\n",
        "    robustness_reports = {}\n",
        "\n",
        "    # --- 1. Sample Period Robustness Test ---\n",
        "    # This test checks if the main empirical result is stable across different historical periods.\n",
        "    sample_period_report = _test_sample_period_robustness(\n",
        "        raw_macro_df, raw_fx_df, master_config\n",
        "    )\n",
        "    # Store the report.\n",
        "    robustness_reports['sample_period_robustness'] = sample_period_report\n",
        "\n",
        "    # --- 2. Leave-One-Out Country Robustness Test ---\n",
        "    # This test checks if the results are driven by a single influential country.\n",
        "    loo_country_report = _test_country_leave_one_out_robustness(\n",
        "        raw_macro_df, raw_fx_df, master_config\n",
        "    )\n",
        "    # Store the report.\n",
        "    robustness_reports['country_leave_one_out_robustness'] = loo_country_report\n",
        "\n",
        "    # Announce the successful completion of the master task.\n",
        "    print(\"\\n========== Task 7.2.3: Data and Sample Robustness Testing Completed Successfully ==========\")\n",
        "\n",
        "    # Return the dictionary of reports.\n",
        "    return robustness_reports\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 7.2: COMPREHENSIVE ROBUSTNESS ANALYSIS ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def orchestrate_robustness_analysis(\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    model_toolkit: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive, multi-faceted robustness analysis framework.\n",
        "\n",
        "    This master function serves as the single entry point for Task 7.2, the final\n",
        "    validation stage of the research pipeline. It systematically assesses the\n",
        "    robustness of the model's findings across three critical dimensions, driven\n",
        "    by a dedicated `robustness_tests` section in the master configuration:\n",
        "\n",
        "    1.  **Parameter Sensitivity (Task 7.2.1):** Quantifies how results change in\n",
        "        response to variations in model parameters.\n",
        "    2.  **Specification Robustness (Task 7.2.2):** Tests how results change when\n",
        "        core model assumptions or methodological choices are altered.\n",
        "    3.  **Data & Sample Robustness (Task 7.2.3):** Examines whether results hold\n",
        "        across different subsets of the data (e.g., time periods, countries).\n",
        "\n",
        "    Args:\n",
        "        raw_macro_df (pd.DataFrame): The raw annual macroeconomic data.\n",
        "        raw_fx_df (pd.DataFrame): The raw daily foreign exchange data.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        model_toolkit (Dict[str, Any]): The dictionary of implemented model functions.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing the detailed reports\n",
        "                        from all executed robustness analyses.\n",
        "    \"\"\"\n",
        "    # Announce the start of the final, top-level task.\n",
        "    print(\"========== Starting Task 7.2: Comprehensive Robustness Analysis Framework ==========\")\n",
        "\n",
        "    # --- 1. Configuration Validation ---\n",
        "    # Check for the presence of the main robustness testing configuration block.\n",
        "    if 'robustness_tests' not in master_config or not master_config['robustness_tests']:\n",
        "        # If the configuration is missing or empty, issue a warning and exit gracefully.\n",
        "        print(\"WARNING: `robustness_tests` configuration not found or is empty in master_config. Skipping all robustness tests.\")\n",
        "        return {}\n",
        "\n",
        "    # Extract the specific configuration for robustness tests.\n",
        "    config = master_config['robustness_tests']\n",
        "\n",
        "    # Initialize a dictionary to store the results of all robustness tests.\n",
        "    robustness_reports: Dict[str, Any] = {}\n",
        "\n",
        "    # --- 2. Dispatch to Sub-Task Orchestrators ---\n",
        "    # The execution of each block of tests is conditional on its presence in the config.\n",
        "\n",
        "    # Execute Parameter Sensitivity tests.\n",
        "    if 'parameter_sensitivity' in config:\n",
        "        # Call the dedicated orchestrator for parameter sensitivity.\n",
        "        # Note: This was renamed from its original plan to avoid conflict.\n",
        "        # The config key remains the same for user convenience.\n",
        "        report = orchestrate_parameter_sensitivity_analysis(\n",
        "            master_config=master_config,\n",
        "            model_toolkit=model_toolkit\n",
        "        )\n",
        "        # Store the results under a clear key.\n",
        "        robustness_reports['parameter_sensitivity_report'] = report\n",
        "\n",
        "    # Execute Model Specification tests.\n",
        "    if 'specification_robustness' in config:\n",
        "        # Call the dedicated orchestrator for specification robustness.\n",
        "        report = orchestrate_specification_robustness_tests(\n",
        "            raw_macro_df=raw_macro_df,\n",
        "            raw_fx_df=raw_fx_df,\n",
        "            master_config=master_config,\n",
        "            model_toolkit=model_toolkit\n",
        "        )\n",
        "        # Store the results.\n",
        "        robustness_reports['specification_robustness_report'] = report\n",
        "\n",
        "    # Execute Data and Sample tests.\n",
        "    if 'data_sample_robustness' in config:\n",
        "        # Call the dedicated orchestrator for data and sample robustness.\n",
        "        report = orchestrate_data_sample_robustness_tests(\n",
        "            raw_macro_df=raw_macro_df,\n",
        "            raw_fx_df=raw_fx_df,\n",
        "            master_config=master_config\n",
        "        )\n",
        "        # Store the results.\n",
        "        robustness_reports['data_sample_robustness_report'] = report\n",
        "\n",
        "    # --- 3. Finalize and Return ---\n",
        "    # Announce the successful completion of the master task.\n",
        "    print(\"\\n========== Task 7.2: Comprehensive Robustness Analysis Completed Successfully ==========\")\n",
        "\n",
        "    # Return the aggregated dictionary of all robustness reports.\n",
        "    return robustness_reports\n"
      ],
      "metadata": {
        "id": "UY1QkRbE2Tpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Master Orchestrator Function\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL TOP-LEVEL MASTER ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def execute_digital_twin_replication(\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_fx_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    analyses_to_run: List[str],\n",
        "    output_filepath: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete, end-to-end digital twin research pipeline.\n",
        "\n",
        "    This function is the definitive top-level entry point for the entire project.\n",
        "    It orchestrates the full sequence of tasks, from data ingestion and validation\n",
        "    to empirical analysis, theoretical simulation, and final robustness checks.\n",
        "    The entire process is profiled for computational performance, and the final\n",
        "    results are integrated into a quality-assured, comprehensive report.\n",
        "\n",
        "    The workflow is as follows:\n",
        "    1.  The main research pipeline (`run_full_research_pipeline`) is executed\n",
        "        within a performance profiler.\n",
        "    2.  The results from the main pipeline are unpacked.\n",
        "    3.  A high-level Quality Assurance report is generated by integrating and\n",
        "        cross-validating key results.\n",
        "    4.  A full suite of robustness analyses is performed.\n",
        "    5.  All generated artifacts—data, reports, and analyses—are aggregated into\n",
        "        a single master dictionary and saved to disk.\n",
        "\n",
        "    Args:\n",
        "        raw_macro_df (pd.DataFrame): The raw annual macroeconomic data.\n",
        "        raw_fx_df (pd.DataFrame): The raw daily foreign exchange data.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        analyses_to_run (List[str]): A list specifying which major phases of the\n",
        "            analysis to execute (e.g., ['data_prep', 'empirical', 'theoretical']).\n",
        "        output_filepath (str): The path to save the final, comprehensive\n",
        "                               results dictionary to.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The final, all-encompassing master results dictionary.\n",
        "    \"\"\"\n",
        "    # Announce the initiation of the final, top-level orchestrator.\n",
        "    print(\"========================================================\")\n",
        "    print(\"=== EXECUTING FULL DIGITAL TWIN REPLICATION PIPELINE ===\")\n",
        "    print(\"========================================================\")\n",
        "\n",
        "    # --- Step 1: Run the Main Pipeline within the Performance Profiler ---\n",
        "    # The `profile_computational_pipeline` function acts as a wrapper, executing\n",
        "    # the main research pipeline while monitoring its performance.\n",
        "    profiled_run_results = profile_computational_pipeline(\n",
        "        pipeline_func=run_full_research_pipeline,\n",
        "        raw_macro_df=raw_macro_df,\n",
        "        raw_fx_df=raw_fx_df,\n",
        "        master_config=master_config,\n",
        "        analyses_to_run=analyses_to_run\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Unpack the Profiled Results ---\n",
        "    # Separate the performance report from the main research results.\n",
        "    performance_report = profiled_run_results['performance_report']\n",
        "    pipeline_results = profiled_run_results['pipeline_results']\n",
        "\n",
        "    # --- Step 3: Integrate and Perform Quality Assurance on Results ---\n",
        "    # This function synthesizes the detailed pipeline results into a high-level\n",
        "    # summary and performs critical cross-validation checks.\n",
        "    print(\"\\n--- Final Phase: Integrating Results and Quality Assurance ---\")\n",
        "    qa_report = integrate_and_validate_results(pipeline_results)\n",
        "    print(\"--- Integration and QA Completed ---\")\n",
        "\n",
        "    # --- Step 4: Run Comprehensive Robustness Analysis ---\n",
        "    # This phase executes the computationally intensive robustness checks.\n",
        "    print(\"\\n--- Final Phase: Comprehensive Robustness Analysis ---\")\n",
        "    # The `model_toolkit` is required for these tests and was generated\n",
        "    # during the main pipeline run.\n",
        "    model_toolkit = pipeline_results.get('model_toolkit')\n",
        "    if model_toolkit:\n",
        "        robustness_report = orchestrate_robustness_analysis(\n",
        "            raw_macro_df=raw_macro_df,\n",
        "            raw_fx_df=raw_fx_df,\n",
        "            master_config=master_config,\n",
        "            model_toolkit=model_toolkit\n",
        "        )\n",
        "    else:\n",
        "        # If the theoretical model was not run, robustness tests cannot be performed.\n",
        "        print(\"WARNING: `model_toolkit` not found in pipeline results. Skipping robustness analysis.\")\n",
        "        robustness_report = {}\n",
        "    print(\"--- Robustness Analysis Completed ---\")\n",
        "\n",
        "    # --- Step 5: Assemble the Final, All-Encompassing Report ---\n",
        "    # Combine all generated artifacts into a single master dictionary.\n",
        "    final_master_report = {\n",
        "        'pipeline_run_results': pipeline_results,\n",
        "        'performance_report': performance_report,\n",
        "        'quality_assurance_report': qa_report,\n",
        "        'robustness_analysis_report': robustness_report\n",
        "    }\n",
        "\n",
        "    # --- Step 6: Save Results to Disk ---\n",
        "    # For large, complex projects, saving the final results object is critical\n",
        "    # for later analysis, visualization, and auditability.\n",
        "    print(f\"\\n--- Saving final master report to: {output_filepath} ---\")\n",
        "    try:\n",
        "        # `joblib.dump` is often more efficient for large numpy arrays than pickle.\n",
        "        joblib.dump(final_master_report, output_filepath)\n",
        "        print(\"--- Save successful ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"--- ERROR: Failed to save results. Error: {e} ---\")\n",
        "\n",
        "    # Announce the successful completion of the entire project.\n",
        "    print(\"\\n========================================================\")\n",
        "    print(\"======= DIGITAL TWIN REPLICATION PIPELINE COMPLETE =======\")\n",
        "    print(\"========================================================\")\n",
        "\n",
        "    # Return the final master report object.\n",
        "    return final_master_report\n"
      ],
      "metadata": {
        "id": "37UdpCk8NI15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}